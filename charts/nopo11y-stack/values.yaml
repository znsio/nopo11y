## kube-prometheus-stack
kube-prometheus-stack:
  enabled: true
  # Default values for kube-prometheus-stack.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  ## Provide a name in place of kube-prometheus-stack for `app:` labels
  ##
  nameOverride: ""

  ## Override the deployment namespace
  ##
  namespaceOverride: ""

  ## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6
  ##
  kubeTargetVersionOverride: ""

  ## Allow kubeVersion to be overridden while creating the ingress
  ##
  kubeVersionOverride: ""

  ## Provide a name to substitute for the full names of resources
  ##
  fullnameOverride: ""

  ## Labels to apply to all resources
  ##
  commonLabels: {}
  # scmhash: abc123
  # myLabel: aakkmd

  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: true

  ## custom Rules to override "for" and "severity" in defaultRules
  ##
  customRules: {}
    # AlertmanagerFailedReload:
    #   for: 3m
    # AlertmanagerMembersInconsistent:
    #   for: 5m
    #   severity: "warning"

  ## Create default rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: true
      configReloaders: true
      general: true
      k8sContainerCpuUsageSecondsTotal: true
      k8sContainerMemoryCache: true
      k8sContainerMemoryRss: true
      k8sContainerMemorySwap: true
      k8sContainerResource: true
      k8sContainerMemoryWorkingSetBytes: true
      k8sPodOwner: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: true
      kubelet: true
      kubeProxy: true
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: true
      kubeSchedulerRecording: true
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true
      windows: true

    ## Reduce app namespace alert scope
    appNamespacesTarget: ".*"

    ## Set keep_firing_for for all alerts
    keepFiringFor: ""

    ## Labels for default rules
    labels: {}
    ## Annotations for default rules
    annotations: {}

    ## Additional labels for PrometheusRule alerts
    additionalRuleLabels: {}

    ## Additional annotations for PrometheusRule alerts
    additionalRuleAnnotations: {}

    ## Additional labels for specific PrometheusRule alert groups
    additionalRuleGroupLabels:
      alertmanager: {}
      etcd: {}
      configReloaders: {}
      general: {}
      k8sContainerCpuUsageSecondsTotal: {}
      k8sContainerMemoryCache: {}
      k8sContainerMemoryRss: {}
      k8sContainerMemorySwap: {}
      k8sContainerResource: {}
      k8sPodOwner: {}
      kubeApiserverAvailability: {}
      kubeApiserverBurnrate: {}
      kubeApiserverHistogram: {}
      kubeApiserverSlos: {}
      kubeControllerManager: {}
      kubelet: {}
      kubeProxy: {}
      kubePrometheusGeneral: {}
      kubePrometheusNodeRecording: {}
      kubernetesApps: {}
      kubernetesResources: {}
      kubernetesStorage: {}
      kubernetesSystem: {}
      kubeSchedulerAlerting: {}
      kubeSchedulerRecording: {}
      kubeStateMetrics: {}
      network: {}
      node: {}
      nodeExporterAlerting: {}
      nodeExporterRecording: {}
      prometheus: {}
      prometheusOperator: {}

    ## Additional annotations for specific PrometheusRule alerts groups
    additionalRuleGroupAnnotations:
      alertmanager: {}
      etcd: {}
      configReloaders: {}
      general: {}
      k8sContainerCpuUsageSecondsTotal: {}
      k8sContainerMemoryCache: {}
      k8sContainerMemoryRss: {}
      k8sContainerMemorySwap: {}
      k8sContainerResource: {}
      k8sPodOwner: {}
      kubeApiserverAvailability: {}
      kubeApiserverBurnrate: {}
      kubeApiserverHistogram: {}
      kubeApiserverSlos: {}
      kubeControllerManager: {}
      kubelet: {}
      kubeProxy: {}
      kubePrometheusGeneral: {}
      kubePrometheusNodeRecording: {}
      kubernetesApps: {}
      kubernetesResources: {}
      kubernetesStorage: {}
      kubernetesSystem: {}
      kubeSchedulerAlerting: {}
      kubeSchedulerRecording: {}
      kubeStateMetrics: {}
      network: {}
      node: {}
      nodeExporterAlerting: {}
      nodeExporterRecording: {}
      prometheus: {}
      prometheusOperator: {}

    additionalAggregationLabels: []

    ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
    runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

    node:
      fsSelector: 'fstype!=""'
      # fsSelector: 'fstype=~"ext[234]|btrfs|xfs|zfs"'

    ## Disabled PrometheusRule alerts
    disabled: {}
    # KubeAPIDown: true
    # NodeRAIDDegraded: true

  ## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.
  ##
  # additionalPrometheusRules: []
  #  - name: my-rule-file
  #    groups:
  #      - name: my_group
  #        rules:
  #        - record: my_record
  #          expr: 100 * my_record

  ## Provide custom recording or alerting rules to be deployed into the cluster.
  ##
  additionalPrometheusRulesMap: {}
  #  rule-name:
  #    groups:
  #    - name: my_group
  #      rules:
  #      - record: my_record
  #        expr: 100 * my_record

  ##
  global:
    rbac:
      create: true

      ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
      ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
      createAggregateClusterRoles: false
      pspEnabled: false
      pspAnnotations: {}
        ## Specify pod annotations
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
        ##
        # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
        # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
        # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

    ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
    ##
    imageRegistry: ""

    ## Reference to one or more secrets to be used when pulling images
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    imagePullSecrets: []
    # - name: "image-pull-secret"
    # or
    # - "image-pull-secret"

  windowsMonitoring:
    ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
    enabled: false

  ## Configuration for prometheus-windows-exporter
  ## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
  ##
  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: true
        jobLabel: jobLabel

    releaseLabel: true

    ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
    ##
    podLabels:
      jobLabel: windows-exporter

    ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
    ##
    config: |-
      collectors:
        enabled: '[defaults],memory,container'

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: true

    ## Annotations for Alertmanager
    ##
    annotations: {}

    ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
    ##
    apiVersion: v2

    ## @param alertmanager.enableFeatures Enable access to Alertmanager disabled features.
    ##
    enableFeatures: []

    ## Service account for Alertmanager to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: true

    ## Configure pod disruption budgets for Alertmanager
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: ""

    ## Alertmanager configuration directives
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    config:
      global:
        resolve_timeout: 5m
      inhibit_rules:
        - source_matchers:
            - 'severity = critical'
          target_matchers:
            - 'severity =~ warning|info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'severity = warning'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
            - 'alertname'
        - source_matchers:
            - 'alertname = InfoInhibitor'
          target_matchers:
            - 'severity = info'
          equal:
            - 'namespace'
        - target_matchers:
            - 'alertname = InfoInhibitor'
      route:
        group_by: ['namespace']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
        - receiver: 'null'
          matchers:
            - alertname = "Watchdog"
      receivers:
      - name: 'null'
      templates:
      - '/etc/alertmanager/config/*.tmpl'

    ## Alertmanager configuration directives (as string type, preferred over the config hash map)
    ## stringConfig will be used only, if tplConfig is true
    ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
    ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
    ##
    stringConfig: ""

    ## Pass the Alertmanager configuration directives through Helm's templating
    ## engine. If the Alertmanager configuration contains Alertmanager templates,
    ## they'll need to be properly escaped so that they are not interpreted by
    ## Helm
    ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
    ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string
    ##      https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    tplConfig: false

    ## Alertmanager template files to format alerts
    ## By default, templateFiles are placed in /etc/alertmanager/config/ and if
    ## they have a .tmpl file suffix will be loaded. See config.templates above
    ## to change, add other suffixes. If adding other suffixes, be sure to update
    ## config.templates above to include those suffixes.
    ## ref: https://prometheus.io/docs/alerting/notifications/
    ##      https://prometheus.io/docs/alerting/notification_examples/
    ##
    templateFiles: {}
    #
    ## An example template:
    #   template_1.tmpl: |-
    #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
    #
    #       {{ define "slack.myorg.text" }}
    #       {{- $root := . -}}
    #       {{ range .Alerts }}
    #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
    #         *Cluster:* {{ template "cluster" $root }}
    #         *Description:* {{ .Annotations.description }}
    #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
    #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
    #         *Details:*
    #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
    #           {{ end }}
    #       {{ end }}
    #       {{ end }}

    ingress:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}

      labels: {}

      ## Override ingress to a different defined port on the service
      # servicePort: 8081
      ## Override ingress to a different service then the default, this is useful if you need to
      ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)
      # serviceName: kube-prometheus-stack-alertmanager-0

      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts: []
        # - alertmanager.domain.com

      ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for Alertmanager Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: alertmanager-general-tls
      #   hosts:
      #   - alertmanager.example.com

    # -- BETA: Configure the gateway routes for the chart here.
    # More routes can be added by adding a dictionary key like the 'main' route.
    # Be aware that this is an early beta of this feature,
    # kube-prometheus-stack does not guarantee this works and is subject to change.
    # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk
    # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)
    route:
      main:
        # -- Enables or disables the route
        enabled: false

        # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2
        apiVersion: gateway.networking.k8s.io/v1
        # -- Set the route kind
        # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute
        kind: HTTPRoute

        annotations: {}
        labels: {}

        hostnames: []
        # - my-filter.example.com
        parentRefs: []
        # - name: acme-gw

        matches:
          - path:
              type: PathPrefix
              value: /

        ## Filters define the filters that are applied to requests that match this rule.
        filters: []

        ## Additional custom rules that can be added to the route
        additionalRules: []

    ## Configuration for Alertmanager secret
    ##
    secret:
      annotations: {}

    ## Configuration for creating an Ingress that will map to each Alertmanager replica service
    ## alertmanager.servicePerReplica must be enabled
    ##
    ingressPerReplica:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}
      labels: {}

      ## Final form of the hostname for each per replica ingress is
      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
      ##
      ## Prefix for the per replica ingress that will have `-$replicaNumber`
      ## appended to the end
      hostPrefix: ""
      ## Domain that will be used for the per replica ingress
      hostDomain: ""

      ## Paths to use for ingress rules
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## Secret name containing the TLS certificate for alertmanager per replica ingress
      ## Secret must be manually created in the namespace
      tlsSecretName: ""

      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
      ##
      tlsSecretPerReplica:
        enabled: false
        ## Final form of the secret for each per replica ingress is
        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
        ##
        prefix: "alertmanager"

    ## Configuration for Alertmanager service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

      ## Port for Alertmanager Service to listen on
      ##
      port: 9093
      ## To be used with a proxy extraContainer port
      ##
      targetPort: 9093
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30903
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##

      ## Additional ports to open for Alertmanager service
      ##
      additionalPorts: []
      # - name: oauth-proxy
      #   port: 8081
      #   targetPort: 8081
      # - name: oauth-metrics
      #   port: 8082
      #   targetPort: 8082

      externalIPs: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## If you want to make sure that connections from a particular client are passed to the same Pod each time
      ## Accepts 'ClientIP' or 'None'
      ##
      sessionAffinity: None

      ## If you want to modify the ClientIP sessionAffinity timeout
      ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
      ##
      sessionAffinityConfig:
        clientIP:
          timeoutSeconds: 10800

      ## Service type
      ##
      type: ClusterIP

    ## Configuration for creating a separate Service for each statefulset Alertmanager replica
    ##
    servicePerReplica:
      enabled: false
      annotations: {}

      ## Port for Alertmanager Service per replica to listen on
      ##
      port: 9093

      ## To be used with a proxy extraContainer port
      targetPort: 9093

      ## Port to expose on each node
      ## Only used if servicePerReplica.type is 'NodePort'
      ##
      nodePort: 30904

      ## Loadbalancer source IP ranges
      ## Only used if servicePerReplica.type is "LoadBalancer"
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

    ## Configuration for creating a ServiceMonitor for AlertManager
    ##
    serviceMonitor:
      ## If true, a ServiceMonitor will be created for the AlertManager service.
      ##
      selfMonitor: true

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## Additional labels
      ##
      additionalLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## enableHttp2: Whether to enable HTTP2.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
      enableHttp2: true

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      bearerTokenFile:

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
    ##
    alertmanagerSpec:
      ## Statefulset's persistent volume claim retention policy
      ## whenDeleted and whenScaled determine whether
      ## statefulset's PVCs are deleted (true) or retained (false)
      ## on scaling down and deleting statefulset, respectively.
      ## Requires Kubernetes version 1.27.0+.
      ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
      persistentVolumeClaimRetentionPolicy: {}
      #  whenDeleted: Retain
      #  whenScaled: Retain

      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
      ##
      podMetadata: {}

      ## Image of Alertmanager
      ##
      image:
        registry: quay.io
        repository: prometheus/alertmanager
        tag: v0.27.0
        sha: ""

      ## If true then the user will be responsible to provide a secret with alertmanager configuration
      ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
      ##
      useExistingSecret: false

      ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
      ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
      ##
      secrets: []

      ## If false then the user will opt out of automounting API credentials.
      ##
      automountServiceAccountToken: true

      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
      ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
      ##
      configMaps: []

      ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
      ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
      ##
      # configSecret:

      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec
      web: {}

      ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
      ##
      alertmanagerConfigSelector: {}
      ## Example which selects all alertmanagerConfig resources
      ## with label "alertconfig" with values any of "example-config" or "example-config-2"
      # alertmanagerConfigSelector:
      #   matchExpressions:
      #     - key: alertconfig
      #       operator: In
      #       values:
      #         - example-config
      #         - example-config-2
      #
      ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
      # alertmanagerConfigSelector:
      #   matchLabels:
      #     role: example-config

      ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
      ##
      alertmanagerConfigNamespaceSelector: {}
      ## Example which selects all namespaces
      ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
      # alertmanagerConfigNamespaceSelector:
      #   matchExpressions:
      #     - key: alertmanagerconfig
      #       operator: In
      #       values:
      #         - example-namespace
      #         - example-namespace-2

      ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
      # alertmanagerConfigNamespaceSelector:
      #   matchLabels:
      #     alertmanagerconfig: enabled

      ## AlermanagerConfig to be used as top level configuration
      ##
      alertmanagerConfiguration: {}
      ## Example with select a global alertmanagerconfig
      # alertmanagerConfiguration:
      #   name: global-alertmanager-Configuration

      ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:
      ##
      alertmanagerConfigMatcherStrategy: {}
      ## Example with use OnNamespace strategy
      # alertmanagerConfigMatcherStrategy:
      #   type: OnNamespace

      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: logfmt

      ## Log level for Alertmanager to be configured with.
      ##
      logLevel: info

      ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1

      ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 120h

      ## Storage is the definition of how storage will be used by the Alertmanager instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storage: {}
      # volumeClaimTemplate:
      #   spec:
      #     storageClassName: gluster
      #     accessModes: ["ReadWriteOnce"]
      #     resources:
      #       requests:
      #         storage: 50Gi
      #   selector: {}


      ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
      ##
      externalUrl:

      ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
      ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
      ##
      routePrefix: /

      ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
      ##
      paused: false

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Define resources requests and limits for single Pods.
      ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      ##
      podAntiAffinity: "soft"

      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname

      ## Assign custom affinity rules to the alertmanager instance
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2

      ## If specified, the pod's tolerations.
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"

      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: alertmanager

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault

      ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
      ## Note this is only for the Alertmanager UI, not the gossip communication.
      ##
      listenLocal: false

      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
      ##
      containers: []
      # containers:
      # - name: oauth-proxy
      #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
      #   args:
      #   - --upstream=http://127.0.0.1:9093
      #   - --http-address=0.0.0.0:8081
      #   - --metrics-address=0.0.0.0:8082
      #   - ...
      #   ports:
      #   - containerPort: 8081
      #     name: oauth-proxy
      #     protocol: TCP
      #   - containerPort: 8082
      #     name: oauth-metrics
      #     protocol: TCP
      #   resources: {}

      # Additional volumes on the output StatefulSet definition.
      volumes: []

      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []

      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []

      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""

      ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
      ##
      additionalPeers: []

      ## PortName to use for Alert Manager.
      ##
      portName: "http-web"

      ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
      ##
      clusterAdvertiseAddress: false

      ## clusterGossipInterval determines interval between gossip attempts.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterGossipInterval: ""

      ## clusterPeerTimeout determines timeout for cluster peering.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterPeerTimeout: ""

      ## clusterPushpullInterval determines interval between pushpull attempts.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterPushpullInterval: ""

      ## clusterLabel defines the identifier that uniquely identifies the Alertmanager cluster.
      clusterLabel: ""

      ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
      ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
      forceEnableClusterMode: false

      ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
      ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
      minReadySeconds: 0

      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}

      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating inside alertmanagerSpec.
      ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)
      additionalConfigString: ""

    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true
    namespaceOverride: ""

    ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
    ##
    forceDeployDatasources: false

    ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
    ##
    forceDeployDashboards: false

    ## Deploy default dashboards
    ##
    defaultDashboardsEnabled: true

    ## Timezone for the default dashboards
    ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg
    ##
    defaultDashboardsTimezone: utc

    ## Editable flag for the default dashboards
    ##
    defaultDashboardsEditable: true

    adminPassword: prom-operator

    grafana.ini:
      dashboards:
        default_home_dashboard_path: /tmp/dashboards/nopo11y-home.json

    rbac:
      ## If true, Grafana PSPs will be created
      ##
      pspEnabled: false

    ingress:
      ## If true, Grafana Ingress will be created
      ##
      enabled: false

      ## IngressClassName for Grafana Ingress.
      ## Should be provided if Ingress is enable.
      ##
      # ingressClassName: nginx

      ## Annotations for Grafana Ingress
      ##
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"

      ## Labels to be added to the Ingress
      ##
      labels: {}

      ## Hostnames.
      ## Must be provided if Ingress is enable.
      ##
      # hosts:
      #   - grafana.domain.com
      hosts: []

      ## Path for grafana ingress
      path: /

      ## TLS configuration for grafana Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: grafana-general-tls
      #   hosts:
      #   - grafana.example.com

    # # To make Grafana persistent (Using Statefulset)
    # #
    # persistence:
    #   enabled: true
    #   type: sts
    #   storageClassName: "storageClassName"
    #   accessModes:
    #     - ReadWriteOnce
    #   size: 20Gi
    #   finalizers:
    #     - kubernetes.io/pvc-protection

    serviceAccount:
      create: true
      autoMount: true

    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        # Allow discovery in all namespaces for dashboards
        searchNamespace: ALL

        # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels
        enableNewTablePanelSyntax: false

        ## Annotations for Grafana dashboard configmaps
        ##
        annotations: {}
        multicluster:
          global:
            enabled: false
          etcd:
            enabled: false
        provider:
          allowUiUpdates: false
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        isDefaultDatasource: true

        name: Prometheus
        uid: prometheus

        ## URL of prometheus datasource
        ##
        # url: http://prometheus-stack-prometheus:9090/

        ## Prometheus request timeout in seconds
        # timeout: 30

        # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
        # defaultDatasourceScrapeInterval: 15s

        ## Annotations for Grafana datasource configmaps
        ##
        annotations: {}

        ## Set method for HTTP to send query to datasource
        httpMethod: POST

        ## Create datasource for each Pod of Prometheus StatefulSet;
        ## this uses headless service `prometheus-operated` which is
        ## created by Prometheus Operator
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286
        createPrometheusReplicasDatasources: false
        label: grafana_datasource
        labelValue: "1"

        ## Field with internal link pointing to existing data source in Grafana.
        ## Can be provisioned via additionalDataSources
        exemplarTraceIdDestinations: {}
          # datasourceUid: Jaeger
          # traceIdLabelName: trace_id
        alertmanager:
          enabled: true
          name: Alertmanager
          uid: alertmanager
          handleGrafanaManagedAlerts: false
          implementation: prometheus

    extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   configMap: certs-configmap
    #   readOnly: true

    deleteDatasources: []
    # - name: example-datasource
    #   orgId: 1

    ## Configure additional grafana datasources (passed through tpl)
    ## ref: http://docs.grafana.org/administration/provisioning/#datasources
    additionalDataSources: []
    # - name: prometheus-sample
    #   access: proxy
    #   basicAuth: true
    #   secureJsonData:
    #       basicAuthPassword: pass
    #   basicAuthUser: daco
    #   editable: false
    #   jsonData:
    #       tlsSkipVerify: true
    #   orgId: 1
    #   type: prometheus
    #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
    #   version: 1

    # Flag to mark provisioned data sources for deletion if they are no longer configured.
    # It takes no effect if data sources are already listed in the deleteDatasources section.
    # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file
    prune: false

    ## Passed to grafana subchart and used by servicemonitor below
    ##
    service:
      portName: http-web
      ipFamilies: []
      ipFamilyPolicy: ""

    serviceMonitor:
      # If true, a ServiceMonitor CRD is created for a prometheus operator
      # https://github.com/coreos/prometheus-operator
      #
      enabled: true

      # Path to use for scraping metrics. Might be different if server.root_url is set
      # in grafana.ini
      path: "/metrics"

      #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)

      # labels for the ServiceMonitor
      labels: {}

      # Scrape interval. If not set, the Prometheus default scrape interval is used.
      #
      interval: ""
      scheme: http
      tlsConfig: {}
      scrapeTimeout: 30s

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

  ## Flag to disable all the kubernetes component scrapers
  ##
  kubernetesServiceMonitors:
    enabled: true

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: true
    tlsConfig:
      serverName: kubernetes
      insecureSkipVerify: false
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      jobLabel: component
      selector:
        matchLabels:
          component: apiserver
          provider: kubernetes

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings:
        # Drop excessively noisy apiserver buckets.
        - action: drop
          regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
          sourceLabels:
            - __name__
            - le
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels:
      #     - __meta_kubernetes_namespace
      #     - __meta_kubernetes_service_name
      #     - __meta_kubernetes_endpoint_port_name
      #   action: keep
      #   regex: default;kubernetes;https
      # - targetLabel: __address__
      #   replacement: kubernetes.default.svc:443

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: true
    namespace: kube-system

    serviceMonitor:
      ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.
      ##
      attachMetadata:
        node: false

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## If true, Prometheus use (respect) labels provided by exporter.
      ##
      honorLabels: true

      ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
      ##
      honorTimestamps: true

      ## If true, defines whether Prometheus tracks staleness of the metrics that have an explicit timestamp present in scraped data. Has no effect if `honorTimestamps` is false.
      ## We recommend enabling this if you want the best possible accuracy for container_ metrics scraped from cadvisor.
      ## For more details see: https://github.com/prometheus-community/helm-charts/pull/5063#issuecomment-2545374849
      trackTimestampsStaleness: true

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## Enable scraping the kubelet over https. For requirements to enable this see
      ## https://github.com/prometheus-operator/prometheus-operator/issues/926
      ##
      https: true

      ## Skip TLS certificate validation when scraping.
      ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed
      ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs
      ##
      insecureSkipVerify: true

      ## Enable scraping /metrics/probes from kubelet's service
      ##
      probes: true

      ## Enable scraping /metrics/resource from kubelet's service
      ## This is disabled by default because container metrics are already exposed by cAdvisor
      ##
      resource: false
      # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
      resourcePath: "/metrics/resource/v1alpha1"
      ## Configure the scrape interval for resource metrics. This is configured to the default Kubelet cAdvisor
      ## minimum housekeeping interval in order to avoid missing samples. Note, this value is ignored
      ## if kubelet.serviceMonitor.interval is not empty.
      resourceInterval: 10s

      ## Enable scraping /metrics/cadvisor from kubelet's service
      ##
      cAdvisor: true
      ## Configure the scrape interval for cAdvisor. This is configured to the default Kubelet cAdvisor
      ## minimum housekeeping interval in order to avoid missing samples. Note, this value is ignored
      ## if kubelet.serviceMonitor.interval is not empty.
      cAdvisorInterval: 10s
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      cAdvisorMetricRelabelings:
        # Drop less useful container CPU metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
        # Drop less useful container / always zero filesystem metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
        # Drop less useful / always zero container memory metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_memory_(mapped_file|swap)'
        # Drop less useful container process metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(file_descriptors|tasks_state|threads_max)'
        # Drop container spec metrics that overlap with kube-state-metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_spec.*'
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: '.+;'
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      probesMetricRelabelings: []
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      ## metrics_path is required to match upstream rules and charts
      cAdvisorRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      probesRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      resourceRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      ## metrics_path is required to match upstream rules and charts
      relabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true

    ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeControllerManager.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
      ## of default port in Kubernetes 1.22.
      ##
      port: null
      targetPort: null
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   component: kube-controller-manager

    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: kube-controller-manager

      ## Enable scraping kube-controller-manager over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
      ## If null or unset, the value is determined dynamically based on target Kubernetes version.
      ##
      https: null

      # Skip TLS certificate validation when scraping
      insecureSkipVerify: null

      # Name of the server to use when validating TLS certificate
      serverName: null

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: true
    service:
      enabled: true
      port: 9153
      targetPort: 9153

      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   k8s-app: kube-dns
    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-dns

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kubeDns. Use either this or coreDns
  ##
  kubeDns:
    enabled: false
    service:
      dnsmasq:
        port: 10054
        targetPort: 10054
      skydns:
        port: 10055
        targetPort: 10055
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   k8s-app: kube-dns
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-dns

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      dnsmasqMetricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      dnsmasqRelabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: true

    ## If your etcd is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      port: 2381
      targetPort: 2381
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   component: etcd

    ## Configure secure access to the etcd cluster by loading a secret into prometheus and
    ## specifying security configuration below. For example, with a secret named etcd-client-cert
    ##
    ## serviceMonitor:
    ##   scheme: https
    ##   insecureSkipVerify: false
    ##   serverName: localhost
    ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    ##
    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
      scheme: http
      insecureSkipVerify: false
      serverName: ""
      caFile: ""
      certFile: ""
      keyFile: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: etcd

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: true

    ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeScheduler.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
      ## of default port in Kubernetes 1.23.
      ##
      port: null
      targetPort: null
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   component: kube-scheduler

    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
      ## Enable scraping kube-scheduler over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
      ## If null or unset, the value is determined dynamically based on target Kubernetes version.
      ##
      https: null

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: kube-scheduler

      ## Skip TLS certificate validation when scraping
      insecureSkipVerify: null

      ## Name of the server to use when validating TLS certificate
      serverName: null

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: true

    ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    service:
      enabled: true
      port: 10249
      targetPort: 10249
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   k8s-app: kube-proxy

    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-proxy

      ## Enable scraping kube-proxy over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
      ##
      https: false

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    namespaceOverride: ""
    rbac:
      create: true
    releaseLabel: true
    prometheus:
      monitor:
        enabled: true

        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""

        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0

        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0

        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0

        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0

        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0

        ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
        ##
        scrapeTimeout: ""

        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""

        # Keep labels from scraped data, overriding server-side labels
        ##
        honorLabels: true

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

    selfMonitor:
      enabled: false

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true
    operatingSystems:
      linux:
        enabled: true
      aix:
        enabled: true
      darwin:
        enabled: true

    ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
    ##
    forceDeployDashboards: false

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    namespaceOverride: ""
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    releaseLabel: true
    extraArgs:
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    service:
      portName: http-metrics
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      labels:
        jobLabel: node-exporter

    prometheus:
      monitor:
        enabled: true

        jobLabel: jobLabel

        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""

        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0

        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0

        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0

        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0

        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0

        ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
        ##
        scrapeTimeout: ""

        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - sourceLabels: [__name__]
        #   separator: ;
        #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
        #   replacement: $1
        #   action: drop

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

        ## Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above.
        ##
        # attachMetadata:
        #   node: false

    rbac:
      ## If true, create PSPs for node-exporter
      ##
      pspEnabled: false

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true

    ## Use '{{ template "kube-prometheus-stack.fullname" . }}-operator' by default
    fullnameOverride: ""

    ## Number of old replicasets to retain ##
    ## The default value is 10, 0 will garbage-collect old replicasets ##
    revisionHistoryLimit: 10

    ## Strategy of the deployment
    ##
    strategy: {}

    ## Prometheus-Operator v0.39.0 and later support TLS natively.
    ##
    tls:
      enabled: true
      # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
      tlsMinVersion: VersionTLS13
      # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
      internalPort: 10250

    ## Liveness probe for the prometheusOperator deployment
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 0
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## Readiness probe for the prometheusOperator deployment
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 0
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

    ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
    ## rules from making their way into prometheus and potentially preventing the container from starting
    admissionWebhooks:
      ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
      ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
      failurePolicy: ""
      ## The default timeoutSeconds is 10 and the maximum value is 30.
      timeoutSeconds: 10
      enabled: true
      ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
      ## If unspecified, system trust roots on the apiserver are used.
      caBundle: ""
      ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
      ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
      ## certs ahead of time if you wish.
      ##
      annotations: {}
      #   argocd.argoproj.io/hook: PreSync
      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded

      namespaceSelector: {}
      objectSelector: {}

      mutatingWebhookConfiguration:
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync

      validatingWebhookConfiguration:
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync

      deployment:
        enabled: false

        ## Number of replicas
        ##
        replicas: 1

        ## Strategy of the deployment
        ##
        strategy: {}

        # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
        podDisruptionBudget: {}
          # maxUnavailable: 1
          # minAvailable: 1

        ## Number of old replicasets to retain ##
        ## The default value is 10, 0 will garbage-collect old replicasets ##
        revisionHistoryLimit: 10

        ## Prometheus-Operator v0.39.0 and later support TLS natively.
        ##
        tls:
          enabled: true
          # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
          tlsMinVersion: VersionTLS13
          # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
          internalPort: 10250

        ## Service account for Prometheus Operator Webhook to use.
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
        ##
        serviceAccount:
          annotations: {}
          automountServiceAccountToken: false
          create: true
          name: ""

        ## Configuration for Prometheus operator Webhook service
        ##
        service:
          annotations: {}
          labels: {}
          clusterIP: ""
          ipDualStack:
            enabled: false
            ipFamilies: ["IPv6", "IPv4"]
            ipFamilyPolicy: "PreferDualStack"

          ## Port to expose on each node
          ## Only used if service.type is 'NodePort'
          ##
          nodePort: 31080

          nodePortTls: 31443

          ## Additional ports to open for Prometheus operator Webhook service
          ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
          ##
          additionalPorts: []

          ## Loadbalancer IP
          ## Only use if service.type is "LoadBalancer"
          ##
          loadBalancerIP: ""
          loadBalancerSourceRanges: []

          ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
          ##
          externalTrafficPolicy: Cluster

          ## Service type
          ## NodePort, ClusterIP, LoadBalancer
          ##
          type: ClusterIP

          ## List of IP addresses at which the Prometheus server service is available
          ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
          ##
          externalIPs: []

        # ## Labels to add to the operator webhook deployment
        # ##
        labels: {}

        ## Annotations to add to the operator webhook deployment
        ##
        annotations: {}

        ## Labels to add to the operator webhook pod
        ##
        podLabels: {}

        ## Annotations to add to the operator webhook pod
        ##
        podAnnotations: {}

        ## Assign a PriorityClassName to pods if set
        # priorityClassName: ""

        ## Define Log Format
        # Use logfmt (default) or json logging
        # logFormat: logfmt

        ## Decrease log verbosity to errors only
        # logLevel: error

        ## Prometheus-operator webhook image
        ##
        image:
          registry: quay.io
          repository: prometheus-operator/admission-webhook
          # if not set appVersion field from Chart.yaml is used
          tag: ""
          sha: ""
          pullPolicy: IfNotPresent

        ## Define Log Format
        # Use logfmt (default) or json logging
        # logFormat: logfmt

        ## Decrease log verbosity to errors only
        # logLevel: error


        ## Liveness probe
        ##
        livenessProbe:
          enabled: true
          failureThreshold: 3
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1

        ## Readiness probe
        ##
        readinessProbe:
          enabled: true
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1

        ## Resource limits & requests
        ##
        resources: {}
        # limits:
        #   cpu: 200m
        #   memory: 200Mi
        # requests:
        #   cpu: 100m
        #   memory: 100Mi

        # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
        # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
        ##
        hostNetwork: false

        ## Define which Nodes the Pods are scheduled on.
        ## ref: https://kubernetes.io/docs/user-guide/node-selection/
        ##
        nodeSelector: {}

        ## Tolerations for use with node taints
        ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
        ##
        tolerations: []
        # - key: "key"
        #   operator: "Equal"
        #   value: "value"
        #   effect: "NoSchedule"

        ## Assign custom affinity rules to the prometheus operator
        ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
        ##
        affinity: {}
          # nodeAffinity:
          #   requiredDuringSchedulingIgnoredDuringExecution:
          #     nodeSelectorTerms:
          #     - matchExpressions:
          #       - key: kubernetes.io/e2e-az-name
          #         operator: In
          #         values:
          #         - e2e-az1
        #         - e2e-az2
        dnsConfig: {}
          # nameservers:
          #   - 1.2.3.4
          # searches:
          #   - ns1.svc.cluster-domain.example
          #   - my.dns.search.suffix
          # options:
          #   - name: ndots
          #     value: "2"
          #   - name: edns0
        securityContext:
          fsGroup: 65534
          runAsGroup: 65534
          runAsNonRoot: true
          runAsUser: 65534
          seccompProfile:
            type: RuntimeDefault

        ## Container-specific security context configuration
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        ##
        containerSecurityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL

        ## If false then the user will opt out of automounting API credentials.
        ##
        automountServiceAccountToken: true

      patch:
        enabled: true
        image:
          registry: registry.k8s.io
          repository: ingress-nginx/kube-webhook-certgen
          tag: v20221220-controller-v1.5.1-58-g787ea74b6
          sha: ""
          pullPolicy: IfNotPresent
        resources: {}
        ## Provide a priority class name to the webhook patching job
        ##
        priorityClassName: ""
        ttlSecondsAfterFinished: 60
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync
        #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
        podAnnotations: {}
        nodeSelector: {}
        affinity: {}
        tolerations: []

        ## SecurityContext holds pod-level security attributes and common container settings.
        ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        ##
        securityContext:
          runAsGroup: 2000
          runAsNonRoot: true
          runAsUser: 2000
          seccompProfile:
            type: RuntimeDefault
        ## Service account for Prometheus Operator Webhook Job Patch to use.
        ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
        ##
        serviceAccount:
          create: true
          annotations: {}
          automountServiceAccountToken: true

      # Security context for create job container
      createSecretJob:
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL

        # Security context for patch job container
      patchWebhookJob:
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL

      # Use certmanager to generate webhook certs
      certManager:
        enabled: false
        # self-signed root certificate
        rootCert:
          duration: ""  # default to be 5y
        admissionCert:
          duration: ""  # default to be 1y
        # issuerRef:
        #   name: "issuer"
        #   kind: "ClusterIssuer"

    ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
    ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
    ##
    namespaces: {}
      # releaseNamespace: true
      # additional:
      # - kube-system

    ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
    ##
    denyNamespaces: []

    ## Filter namespaces to look for prometheus-operator custom resources
    ##
    alertmanagerInstanceNamespaces: []
    alertmanagerConfigNamespaces: []
    prometheusInstanceNamespaces: []
    thanosRulerInstanceNamespaces: []

    ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
    ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
    ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
    ##
    # clusterDomain: "cluster.local"

    networkPolicy:
      ## Enable creation of NetworkPolicy resources.
      ##
      enabled: false

      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes

      # cilium:
      #   egress:

      ## match labels used in selector
      # matchLabels: {}

    ## Service account for Prometheus Operator to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      automountServiceAccountToken: true
      annotations: {}

    ## Configuration for Prometheus operator service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
      nodePort: 30080

      nodePortTls: 30443

    ## Additional ports to open for Prometheus operator service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
    ##
      additionalPorts: []

    ## Loadbalancer IP
    ## Only use if service.type is "LoadBalancer"
    ##
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

    ## Service type
    ## NodePort, ClusterIP, LoadBalancer
    ##
      type: ClusterIP

      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##
      externalIPs: []

    # ## Labels to add to the operator deployment
    # ##
    labels: {}

    ## Annotations to add to the operator deployment
    ##
    annotations: {}

    ## Labels to add to the operator pod
    ##
    podLabels: {}

    ## Annotations to add to the operator pod
    ##
    podAnnotations: {}

    ## Assign a PriorityClassName to pods if set
    # priorityClassName: ""

    ## Define Log Format
    # Use logfmt (default) or json logging
    # logFormat: logfmt

    ## Decrease log verbosity to errors only
    # logLevel: error

    kubeletService:
      ## If true, the operator will create and maintain a service for scraping kubelets
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
      ##
      enabled: true
      namespace: kube-system
      selector: ""
      ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
      name: ""

    ## Create Endpoints objects for kubelet targets.
    kubeletEndpointsEnabled: true
    ## Create EndpointSlice objects for kubelet targets.
    kubeletEndpointSliceEnabled: false

    ## Extra arguments to pass to prometheusOperator
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/operator.md
    extraArgs: []
    #  - --labels="cluster=talos-cluster"

    ## Create a servicemonitor for the operator
    ##
    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus operator
      ##
      selfMonitor: true

      ## Labels for ServiceMonitor
      additionalLabels: {}

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
      scrapeTimeout: ""

      ## Metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      #   relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

    ## Resource limits & requests
    ##
    resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 200Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi

    ## Operator Environment
    ##  env:
    ##    VARIABLE: value
    env:
      GOGC: "30"

    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
    ##
    hostNetwork: false

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Tolerations for use with node taints
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    ## Assign custom affinity rules to the prometheus operator
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    ##
    affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2
    dnsConfig: {}
      # nameservers:
      #   - 1.2.3.4
      # searches:
      #   - ns1.svc.cluster-domain.example
      #   - my.dns.search.suffix
      # options:
      #   - name: ndots
      #     value: "2"
    #   - name: edns0
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
      seccompProfile:
        type: RuntimeDefault

    ## Container-specific security context configuration
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ##
    containerSecurityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL

    # Enable vertical pod autoscaler support for prometheus-operator
    verticalPodAutoscaler:
      enabled: false

      # Recommender responsible for generating recommendation for the object.
      # List should be empty (then the default recommender will generate the recommendation)
      # or contain exactly one recommender.
      # recommenders:
      # - name: custom-recommender-performance

      # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      controlledResources: []
      # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.
      # controlledValues: RequestsAndLimits

      # Define the max allowed resources for the pod
      maxAllowed: {}
      # cpu: 200m
      # memory: 100Mi
      # Define the min allowed resources for the pod
      minAllowed: {}
      # cpu: 200m
      # memory: 100Mi

      updatePolicy:
        # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
        # minReplicas: 1
        # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
        # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
        updateMode: Auto

    ## Prometheus-operator image
    ##
    image:
      registry: quay.io
      repository: prometheus-operator/prometheus-operator
      # if not set appVersion field from Chart.yaml is used
      tag: ""
      sha: ""
      pullPolicy: IfNotPresent

    ## Prometheus image to use for prometheuses managed by the operator
    ##
    # prometheusDefaultBaseImage: prometheus/prometheus

    ## Prometheus image registry to use for prometheuses managed by the operator
    ##
    # prometheusDefaultBaseImageRegistry: quay.io

    ## Alertmanager image to use for alertmanagers managed by the operator
    ##
    # alertmanagerDefaultBaseImage: prometheus/alertmanager

    ## Alertmanager image registry to use for alertmanagers managed by the operator
    ##
    # alertmanagerDefaultBaseImageRegistry: quay.io

    ## Prometheus-config-reloader
    ##
    prometheusConfigReloader:
      image:
        registry: quay.io
        repository: prometheus-operator/prometheus-config-reloader
        # if not set appVersion field from Chart.yaml is used
        tag: ""
        sha: ""

      # add prometheus config reloader liveness and readiness probe. Default: false
      enableProbe: false

      # resource config for prometheusConfigReloader
      resources: {}
        # requests:
        #   cpu: 200m
        #   memory: 50Mi
        # limits:
        #   cpu: 200m
        #   memory: 50Mi

    ## Thanos side-car image when configured
    ##
    thanosImage:
      registry: quay.io
      repository: thanos/thanos
      tag: v0.37.2
      sha: ""

    ## Set a Label Selector to filter watched prometheus and prometheusAgent
    ##
    prometheusInstanceSelector: ""

    ## Set a Label Selector to filter watched alertmanager
    ##
    alertmanagerInstanceSelector: ""

    ## Set a Label Selector to filter watched thanosRuler
    thanosRulerInstanceSelector: ""

    ## Set a Field Selector to filter watched secrets
    ##
    secretFieldSelector: "type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1"

    ## If false then the user will opt out of automounting API credentials.
    ##
    automountServiceAccountToken: true

    ## Additional volumes
    ##
    extraVolumes: []

    ## Additional volume mounts
    ##
    extraVolumeMounts: []

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true

    ## Toggle prometheus into agent mode
    ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md
    ##
    agentMode: false

    ## Annotations for Prometheus
    ##
    annotations: {}

    ## Configure network policy for the prometheus
    networkPolicy:
      enabled: false

      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes

      # cilium:
      #   endpointSelector:
      #   egress:
      #   ingress:

      # egress:
      # - {}
      # ingress:
      # - {}
      # podSelector:
      #   matchLabels:
      #     app: prometheus

    ## Service account for Prometheuses to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: true

    # Service for thanos service discovery on sidecar
    # Enable this can make Thanos Query can use
    # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
    # Thanos sidecar on prometheus nodes
    # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
    thanosService:
      enabled: false
      annotations: {}
      labels: {}

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

      ## Service dual stack
      ##
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

      ## gRPC port config
      portName: grpc
      port: 10901
      targetPort: "grpc"

      ## HTTP port config (for metrics)
      httpPortName: http
      httpPort: 10902
      targetHttpPort: "http"

      ## ClusterIP to assign
      # Default is to make this a headless service ("None")
      clusterIP: "None"

      ## Port to expose on each node, if service type is NodePort
      ##
      nodePort: 30901
      httpNodePort: 30902

    # ServiceMonitor to scrape Sidecar metrics
    # Needs thanosService to be enabled as well
    thanosServiceMonitor:
      enabled: false
      interval: ""

      ## Additional labels
      ##
      additionalLabels: {}

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      bearerTokenFile:

      ## Metric relabel configs to apply to samples before ingestion.
      metricRelabelings: []

      ## relabel configs to apply to samples before ingestion.
      relabelings: []

    # Service for external access to sidecar
    # Enabling this creates a service to expose thanos-sidecar outside the cluster.
    thanosServiceExternal:
      enabled: false
      annotations: {}
      labels: {}
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## gRPC port config
      portName: grpc
      port: 10901
      targetPort: "grpc"

      ## HTTP port config (for metrics)
      httpPortName: http
      httpPort: 10902
      targetHttpPort: "http"

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: LoadBalancer

      ## Port to expose on each node
      ##
      nodePort: 30901
      httpNodePort: 30902

    ## Configuration for Prometheus service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

      ## Port for Prometheus Service to listen on
      ##
      port: 9090

      ## To be used with a proxy extraContainer port
      targetPort: 9090

      ## Port for Prometheus Reloader to listen on
      ##
      reloaderWebPort: 8080

      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##
      externalIPs: []

      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30090

      ## Loadbalancer IP
      ## Only use if service.type is "LoadBalancer"
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

      ## Additional ports to open for Prometheus service
      ##
      additionalPorts: []
      # additionalPorts:
      # - name: oauth-proxy
      #   port: 8081
      #   targetPort: 8081
      # - name: oauth-metrics
      #   port: 8082
      #   targetPort: 8082

      ## Consider that all endpoints are considered "ready" even if the Pods themselves are not
      ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec
      publishNotReadyAddresses: false

      ## If you want to make sure that connections from a particular client are passed to the same Pod each time
      ## Accepts 'ClientIP' or 'None'
      ##
      sessionAffinity: None

      ## If you want to modify the ClientIP sessionAffinity timeout
      ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
      ##
      sessionAffinityConfig:
        clientIP:
          timeoutSeconds: 10800

    ## Configuration for creating a separate Service for each statefulset Prometheus replica
    ##
    servicePerReplica:
      enabled: false
      annotations: {}

      ## Port for Prometheus Service per replica to listen on
      ##
      port: 9090

      ## To be used with a proxy extraContainer port
      targetPort: 9090

      ## Port to expose on each node
      ## Only used if servicePerReplica.type is 'NodePort'
      ##
      nodePort: 30091

      ## Loadbalancer source IP ranges
      ## Only used if servicePerReplica.type is "LoadBalancer"
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

      ## Service dual stack
      ##
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

    ## Configure pod disruption budgets for Prometheus
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: ""

    # Ingress exposes thanos sidecar outside the cluster
    thanosIngress:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}
      labels: {}
      servicePort: 10901

      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30901

      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts: []
        # - thanos-gateway.domain.com

      ## Paths to use for ingress rules
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for Thanos Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: thanos-gateway-tls
      #   hosts:
      #   - thanos-gateway.domain.com
      #

    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

    ingress:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}
      labels: {}

      ## Redirect ingress to an additional defined port on the service
      # servicePort: 8081

      ## Hostnames.
      ## Must be provided if Ingress is enabled.
      ##
      # hosts:
      #   - prometheus.domain.com
      hosts: []

      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for Prometheus Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
        # - secretName: prometheus-general-tls
        #   hosts:
        #     - prometheus.example.com

    # -- BETA: Configure the gateway routes for the chart here.
    # More routes can be added by adding a dictionary key like the 'main' route.
    # Be aware that this is an early beta of this feature,
    # kube-prometheus-stack does not guarantee this works and is subject to change.
    # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk
    # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)
    route:
      main:
        # -- Enables or disables the route
        enabled: false

        # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2
        apiVersion: gateway.networking.k8s.io/v1
        # -- Set the route kind
        # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute
        kind: HTTPRoute

        annotations: {}
        labels: {}

        hostnames: []
        # - my-filter.example.com
        parentRefs: []
        # - name: acme-gw

        matches:
        - path:
            type: PathPrefix
            value: /

        ## Filters define the filters that are applied to requests that match this rule.
        filters: []

        ## Additional custom rules that can be added to the route
        additionalRules: []

    ## Configuration for creating an Ingress that will map to each Prometheus replica service
    ## prometheus.servicePerReplica must be enabled
    ##
    ingressPerReplica:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}
      labels: {}

      ## Final form of the hostname for each per replica ingress is
      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
      ##
      ## Prefix for the per replica ingress that will have `-$replicaNumber`
      ## appended to the end
      hostPrefix: ""
      ## Domain that will be used for the per replica ingress
      hostDomain: ""

      ## Paths to use for ingress rules
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## Secret name containing the TLS certificate for Prometheus per replica ingress
      ## Secret must be manually created in the namespace
      tlsSecretName: ""

      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
      ##
      tlsSecretPerReplica:
        enabled: false
        ## Final form of the secret for each per replica ingress is
        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
        ##
        prefix: "prometheus"

    ## Configure additional options for default pod security policy for Prometheus
    ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
    podSecurityPolicy:
      allowedCapabilities: []
      allowedHostPaths: []
      volumes: []

    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus
      ##
      selfMonitor: true

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## Additional labels
      ##
      additionalLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      bearerTokenFile:

      ## Metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      #   relabel configs to apply to samples before ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics

    ## Settings affecting prometheusSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
    ##
    prometheusSpec:
      ## Statefulset's persistent volume claim retention policy
      ## whenDeleted and whenScaled determine whether
      ## statefulset's PVCs are deleted (true) or retained (false)
      ## on scaling down and deleting statefulset, respectively.
      ## Requires Kubernetes version 1.27.0+.
      ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
      persistentVolumeClaimRetentionPolicy: {}
      #  whenDeleted: Retain
      #  whenScaled: Retain

      ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
      ##
      ## AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in the pod,
      ## If the field isn’t set, the operator mounts the service account token by default.
      ## Warning: be aware that by default, Prometheus requires the service account token for Kubernetes service discovery,
      ## It is possible to use strategic merge patch to project the service account token into the ‘prometheus’ container.
      automountServiceAccountToken: true

      disableCompaction: false
      ## APIServerConfig
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig
      ##
      apiserverConfig: {}

      ## Allows setting additional arguments for the Prometheus container
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus
      additionalArgs: []

      ## Interval between consecutive scrapes.
      ## Defaults to 30s.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
      ##
      scrapeInterval: ""

      ## Number of seconds to wait for target to respond before erroring
      ##
      scrapeTimeout: ""

      ## List of scrape classes to expose to scraping objects such as
      ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.
      ##
      scrapeClasses: []
      # - name: istio-mtls
      #   default: false
      #   tlsConfig:
      #     caFile: /etc/prometheus/secrets/istio.default/root-cert.pem
      #     certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem

      ## Interval between consecutive evaluations.
      ##
      evaluationInterval: ""

      ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
      ##
      listenLocal: false

      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
      ## This is disabled by default.
      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
      ##
      enableAdminAPI: false

      ## Sets version of Prometheus overriding the Prometheus version as derived
      ## from the image tag. Useful in cases where the tag does not follow semver v2.
      version: ""

      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig
      web: {}

      ## Exemplars related settings that are runtime reloadable.
      ## It requires to enable the exemplar storage feature to be effective.
      exemplars: ""
        ## Maximum number of exemplars stored in memory for all series.
        ## If not set, Prometheus uses its default value.
        ## A value of zero or less than zero disables the storage.
        # maxSize: 100000

      # EnableFeatures API enables access to Prometheus disabled features.
      # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/
      enableFeatures: []
      # - exemplar-storage

      ## Image of Prometheus.
      ##
      image:
        registry: quay.io
        repository: prometheus/prometheus
        tag: v3.0.1
        sha: ""

      ## Tolerations for use with node taints
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      #  - key: "key"
      #    operator: "Equal"
      #    value: "value"
      #    effect: "NoSchedule"

      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: prometheus

      ## Alertmanagers to which alerts will be sent
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints
      ##
      ## Default configuration will connect to the alertmanager deployed as part of this release
      ##
      alertingEndpoints: []
      # - name: ""
      #   namespace: ""
      #   port: http
      #   scheme: http
      #   pathPrefix: ""
      #   tlsConfig: {}
      #   bearerTokenFile: ""
      #   apiVersion: v2

      ## External labels to add to any time series or alerts when communicating with external systems
      ##
      externalLabels: {}

      ## enable --web.enable-remote-write-receiver flag on prometheus-server
      ##
      enableRemoteWriteReceiver: false

      ## Name of the external label used to denote replica name
      ##
      replicaExternalLabelName: ""

      ## If true, the Operator won't add the external label used to denote replica name
      ##
      replicaExternalLabelNameClear: false

      ## Name of the external label used to denote Prometheus instance name
      ##
      prometheusExternalLabelName: ""

      ## If true, the Operator won't add the external label used to denote Prometheus instance name
      ##
      prometheusExternalLabelNameClear: false

      ## External URL at which Prometheus will be reachable.
      ##
      externalUrl: ""

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
      ## with the new list of secrets.
      ##
      secrets: []

      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
      ##
      configMaps: []

      ## QuerySpec defines the query command line flags when starting Prometheus.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec
      ##
      query: {}

      ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.
      ruleNamespaceSelector: {}
      ## Example which selects PrometheusRules in namespaces with label "prometheus" set to "somelabel"
      # ruleNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ##
      ruleSelectorNilUsesHelmValues: true

      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector: {}
      ## Example which select all PrometheusRules resources
      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
      # ruleSelector:
      #   matchExpressions:
      #     - key: prometheus
      #       operator: In
      #       values:
      #         - example-rules
      #         - example-rules-2
      #
      ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
      # ruleSelector:
      #   matchLabels:
      #     role: example-rules

      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the servicemonitors created
      ##
      serviceMonitorSelectorNilUsesHelmValues: true

      ## ServiceMonitors to be selected for target discovery.
      ## If {}, select all ServiceMonitors
      ##
      serviceMonitorSelector: {}
      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
      # serviceMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## Namespaces to be selected for ServiceMonitor discovery.
      ##
      serviceMonitorNamespaceSelector: {}
      ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
      # serviceMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the podmonitors created
      ##
      podMonitorSelectorNilUsesHelmValues: true

      ## PodMonitors to be selected for target discovery.
      ## If {}, select all PodMonitors
      ##
      podMonitorSelector: {}
      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
      # podMonitorSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.
      podMonitorNamespaceSelector: {}
      ## Example which selects PodMonitor in namespaces with label "prometheus" set to "somelabel"
      # podMonitorNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the probes created
      ##
      probeSelectorNilUsesHelmValues: true

      ## Probes to be selected for target discovery.
      ## If {}, select all Probes
      ##
      probeSelector: {}
      ## Example which selects Probes with label "prometheus" set to "somelabel"
      # probeSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If nil, select own namespace. Namespaces to be selected for Probe discovery.
      probeNamespaceSelector: {}
      ## Example which selects Probe in namespaces with label "prometheus" set to "somelabel"
      # probeNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the scrapeConfigs created
      ##
      ## If null and scrapeConfigSelector is also null, exclude field from the prometheusSpec
      ## (keeping downward compatibility with older versions of CRD)
      ##
      scrapeConfigSelectorNilUsesHelmValues: true

      ## scrapeConfigs to be selected for target discovery.
      ## If {}, select all scrapeConfigs
      ##
      scrapeConfigSelector: {}
      ## Example which selects scrapeConfigs with label "prometheus" set to "somelabel"
      # scrapeConfigSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.
      ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)
      scrapeConfigNamespaceSelector: {}
      ## Example which selects scrapeConfig in namespaces with label "prometheus" set to "somelabel"
      # scrapeConfigNamespaceSelector:
      #   matchLabels:
      #     prometheus: somelabel

      ## How long to retain metrics
      ##
      retention: 10d

      ## Maximum size of metrics
      ##
      retentionSize: ""

      ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration
      ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb
      tsdb:
        outOfOrderTimeWindow: 0s

      ## Enable compression of the write-ahead log using Snappy.
      ##
      walCompression: true

      ## If true, the Operator won't process any Prometheus configuration changes
      ##
      paused: false

      ## Number of replicas of each shard to deploy for a Prometheus deployment.
      ## Number of replicas multiplied by shards is the total number of Pods created.
      ##
      replicas: 1

      ## EXPERIMENTAL: Number of shards to distribute targets onto.
      ## Number of replicas multiplied by shards is the total number of Pods created.
      ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
      ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
      ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
      ## Sharding is done on the content of the `__address__` target meta-label.
      ##
      shards: 1

      ## Log level for Prometheus be configured in
      ##
      logLevel: info

      ## Log format for Prometheus be configured in
      ##
      logFormat: logfmt

      ## Prefix used to register routes, overriding externalUrl route.
      ## Useful for proxies that rewrite URLs.
      ##
      routePrefix: /

      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the prometheus pods.
      ##
      podMetadata: {}
      # labels:
      #   app: prometheus
      #   k8s-app: prometheus

      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      podAntiAffinity: "soft"

      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname

      ## Assign custom affinity rules to the prometheus instance
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2

      ## The remote_read spec configuration for Prometheus.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec
      remoteRead: []
      # - url: http://remote1/read
      ## additionalRemoteRead is appended to remoteRead
      additionalRemoteRead: []

      ## The remote_write spec configuration for Prometheus.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec
      remoteWrite: []
      # - url: http://remote1/push
      ## additionalRemoteWrite is appended to remoteWrite
      additionalRemoteWrite: []

      ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
      remoteWriteDashboards: false

      ## Resource limits & requests
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Prometheus StorageSpec for persistent data
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storageSpec: {}
      ## Using PersistentVolumeClaim
      ##
      #  volumeClaimTemplate:
      #    spec:
      #      storageClassName: gluster
      #      accessModes: ["ReadWriteOnce"]
      #      resources:
      #        requests:
      #          storage: 50Gi
      #    selector: {}

      ## Using tmpfs volume
      ##
      #  emptyDir:
      #    medium: Memory

      # Additional volumes on the output StatefulSet definition.
      volumes: []

      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []

      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
      ## as specified in the official Prometheus documentation:
      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
      ## scrape configs are going to break Prometheus after the upgrade.
      ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
      ##
      ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
      ##
      additionalScrapeConfigs: []
      # - job_name: kube-etcd
      #   kubernetes_sd_configs:
      #     - role: node
      #   scheme: https
      #   tls_config:
      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
      #   relabel_configs:
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)
      #   - source_labels: [__address__]
      #     action: replace
      #     targetLabel: __address__
      #     regex: ([^:;]+):(\d+)
      #     replacement: ${1}:2379
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: keep
      #     regex: .*mst.*
      #   - source_labels: [__meta_kubernetes_node_name]
      #     action: replace
      #     targetLabel: node
      #     regex: (.*)
      #     replacement: ${1}
      #   metric_relabel_configs:
      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
      #     action: labeldrop
      #
      ## If scrape config contains a repetitive section, you may want to use a template.
      ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
      # additionalScrapeConfigs: |
      #  - job_name: "node-exporter"
      #    gce_sd_configs:
      #    {{range $zone := .Values.gcp_zones}}
      #    - project: "project1"
      #      zone: "{{$zone}}"
      #      port: 9100
      #    {{end}}
      #    relabel_configs:
      #    ...


      ## If additional scrape configurations are already deployed in a single secret file you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalScrapeConfigs
      additionalScrapeConfigsSecret: {}
        # enabled: false
        # name:
        # key:

      ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
      ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
      additionalPrometheusSecretsAnnotations: {}

      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertManagerConfigs: []
      # - consul_sd_configs:
      #   - server: consul.dev.test:8500
      #     scheme: http
      #     datacenter: dev
      #     tag_separator: ','
      #     services:
      #       - metrics-prometheus-alertmanager

      ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertManagerConfigs
      additionalAlertManagerConfigsSecret: {}
        # name:
        # key:
        # optional: false

      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
      ## configs are going to break Prometheus after the upgrade.
      ##
      additionalAlertRelabelConfigs: []
      # - separator: ;
      #   regex: prometheus_replica
      #   replacement: $1
      #   action: labeldrop

      ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertRelabelConfigs
      additionalAlertRelabelConfigsSecret: {}
        # name:
        # key:

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault

      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""

      ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
      ## This section is experimental, it may change significantly without deprecation notice in any release.
      ## This is experimental and may change significantly without backward compatibility in any release.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec
      ##
      thanos: {}
        # secretProviderClass:
        #   provider: gcp
        #   parameters:
        #     secrets: |
        #       - resourceName: "projects/$PROJECT_ID/secrets/testsecret/versions/latest"
        #         fileName: "objstore.yaml"
        ## ObjectStorageConfig configures object storage in Thanos.
        # objectStorageConfig:
        #   # use existing secret, if configured, objectStorageConfig.secret will not be used
        #   existingSecret: {}
        #     # name: ""
        #     # key: ""
        #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,
        #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set
        #   # https://thanos.io/tip/thanos/storage.md/#s3
        #   secret: {}
        #     # type: S3
        #     # config:
        #     #   bucket: ""
        #     #   endpoint: ""
        #     #   region: ""
        #     #   access_key: ""
        #     #   secret_key: ""

      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
      ## if using proxy extraContainer update targetPort with proxy container port
      containers: []
      # containers:
      # - name: oauth-proxy
      #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
      #   args:
      #   - --upstream=http://127.0.0.1:9090
      #   - --http-address=0.0.0.0:8081
      #   - --metrics-address=0.0.0.0:8082
      #   - ...
      #   ports:
      #   - containerPort: 8081
      #     name: oauth-proxy
      #     protocol: TCP
      #   - containerPort: 8082
      #     name: oauth-metrics
      #     protocol: TCP
      #   resources: {}

      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []

      ## PortName to use for Prometheus.
      ##
      portName: "http-web"

      ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files
      ## on the file system of the Prometheus container e.g. bearer token files.
      arbitraryFSAccessThroughSMs: false

      ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor
      ## or PodMonitor to true, this overrides honor_labels to false.
      overrideHonorLabels: false

      ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.
      overrideHonorTimestamps: false

      ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,
      ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,
      ## and servicemonitors will be installed in the default service namespace.
      ## Defaults to false.
      ignoreNamespaceSelectors: false

      ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
      ## The label value will always be the namespace of the object that is being created.
      ## Disabled by default
      enforcedNamespaceLabel: ""

      ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.
      ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair
      ## Deprecated, use `excludedFromEnforcement` instead
      prometheusRulesExcludedFromEnforce: []

      ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects
      ## to be excluded from enforcing a namespace label of origin.
      ## Works only if enforcedNamespaceLabel set to true.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference
      excludedFromEnforcement: []

      ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,
      ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such
      ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions
      ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
      queryLogFile: false

      # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.
      # Set to 'false' to disable global sample_limit. or set to a number to override the default value.
      sampleLimit: false

      # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.
      # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets
      # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.
      enforcedKeepDroppedTargets: 0

      ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
      ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
      ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
      enforcedSampleLimit: false

      ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set
      ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall
      ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except
      ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.
      enforcedTargetLimit: false


      ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present
      ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
      ## 2.27.0 and newer.
      enforcedLabelLimit: false

      ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number
      ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
      ## 2.27.0 and newer.
      enforcedLabelNameLengthLimit: false

      ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this
      ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus
      ## versions 2.27.0 and newer.
      enforcedLabelValueLengthLimit: false

      ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental
      ## in Prometheus so it may change in any upcoming release.
      allowOverlappingBlocks: false

      ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
      ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
      minReadySeconds: 0

      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
      # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.
      # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.
      hostNetwork: false

      # HostAlias holds the mapping between IP and hostnames that will be injected
      # as an entry in the pod’s hosts file.
      hostAliases: []
      #  - ip: 10.10.0.100
      #    hostnames:
      #      - a1.app.local
      #      - b1.app.local

      ## TracingConfig configures tracing in Prometheus.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig
      tracingConfig: {}

      ## Defines the service discovery role used to discover targets from ServiceMonitor objects and Alertmanager endpoints.
      ## If set, the value should be either “Endpoints” or “EndpointSlice”. If unset, the operator assumes the “Endpoints” role.
      serviceDiscoveryRole: ""

      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}

      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating inside alertmanagerSpec.
      ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)
      additionalConfigString: ""

      ## Defines the maximum time that the `prometheus` container's startup probe
      ## will wait before being considered failed. The startup probe will return
      ## success after the WAL replay is complete. If set, the value should be
      ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15
      ## minutes).
      maximumStartupDurationSeconds: 0

    additionalRulesForClusterRole: []
    #  - apiGroups: [ "" ]
    #    resources:
    #      - nodes/proxy
    #    verbs: [ "get", "list", "watch" ]

    additionalServiceMonitors: []
    ## Name of the ServiceMonitor to create
    ##
    # - name: ""

      ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
      ## the chart
      ##
      # additionalLabels: {}

      ## Service label for use in assembling a job name of the form <label value>-<port>
      ## If no label is specified, the service name is used.
      ##
      # jobLabel: ""

      ## labels to transfer from the kubernetes service to the target
      ##
      # targetLabels: []

      ## labels to transfer from the kubernetes pods to the target
      ##
      # podTargetLabels: []

      ## Label selector for services to which this ServiceMonitor applies
      ##
      # selector: {}
        ## Example which selects all services to be monitored
        ## with label "monitoredby" with values any of "example-service-1" or "example-service-2"
        # matchExpressions:
        #   - key: "monitoredby"
        #     operator: In
        #     values:
        #       - example-service-1
        #       - example-service-2

        ## label selector for services
        ##
        # matchLabels: {}

      ## Namespaces from which services are selected
      ##
      # namespaceSelector:
        ## Match any namespace
        ##
        # any: false

        ## Explicit list of namespace names to select
        ##
        # matchNames: []

      ## Endpoints of the selected service to be monitored
      ##
      # endpoints: []
        ## Name of the endpoint's service port
        ## Mutually exclusive with targetPort
        # - port: ""

        ## Name or number of the endpoint's target port
        ## Mutually exclusive with port
        # - targetPort: ""

        ## File containing bearer token to be used when scraping targets
        ##
        #   bearerTokenFile: ""

        ## Interval at which metrics should be scraped
        ##
        #   interval: 30s

        ## HTTP path to scrape for metrics
        ##
        #   path: /metrics

        ## HTTP scheme to use for scraping
        ##
        #   scheme: http

        ## TLS configuration to use when scraping the endpoint
        ##
        #   tlsConfig:

            ## Path to the CA file
            ##
            # caFile: ""

            ## Path to client certificate file
            ##
            # certFile: ""

            ## Skip certificate verification
            ##
            # insecureSkipVerify: false

            ## Path to client key file
            ##
            # keyFile: ""

            ## Server name used to verify host name
            ##
            # serverName: ""

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      # metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      # relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

    additionalPodMonitors: []
    ## Name of the PodMonitor to create
    ##
    # - name: ""

      ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
      ## the chart
      ##
      # additionalLabels: {}

      ## Pod label for use in assembling a job name of the form <label value>-<port>
      ## If no label is specified, the pod endpoint name is used.
      ##
      # jobLabel: ""

      ## Label selector for pods to which this PodMonitor applies
      ##
      # selector: {}
        ## Example which selects all Pods to be monitored
        ## with label "monitoredby" with values any of "example-pod-1" or "example-pod-2"
        # matchExpressions:
        #   - key: "monitoredby"
        #     operator: In
        #     values:
        #       - example-pod-1
        #       - example-pod-2

        ## label selector for pods
        ##
        # matchLabels: {}

      ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
      ##
      # podTargetLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      # sampleLimit: 0

      ## Namespaces from which pods are selected
      ##
      # namespaceSelector:
        ## Match any namespace
        ##
        # any: false

        ## Explicit list of namespace names to select
        ##
        # matchNames: []

      ## Endpoints of the selected pods to be monitored
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint
      ##
      # podMetricsEndpoints: []

  ## Configuration for thanosRuler
  ## ref: https://thanos.io/tip/components/rule.md/
  ##
  thanosRuler:

    ## Deploy thanosRuler
    ##
    enabled: false

    ## Annotations for ThanosRuler
    ##
    annotations: {}

    ## Service account for ThanosRuler to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}

    ## Configure pod disruption budgets for ThanosRuler
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
    ##
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      maxUnavailable: ""

    ingress:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}

      labels: {}

      ## Hosts must be provided if Ingress is enabled.
      ##
      hosts: []
        # - thanosruler.domain.com

      ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## TLS configuration for ThanosRuler Ingress
      ## Secret must be manually created in the namespace
      ##
      tls: []
      # - secretName: thanosruler-general-tls
      #   hosts:
      #   - thanosruler.example.com

    # -- BETA: Configure the gateway routes for the chart here.
    # More routes can be added by adding a dictionary key like the 'main' route.
    # Be aware that this is an early beta of this feature,
    # kube-prometheus-stack does not guarantee this works and is subject to change.
    # Being BETA this can/will change in the future without notice, do not use unless you want to take that risk
    # [[ref]](https://gateway-api.sigs.k8s.io/references/spec/#gateway.networking.k8s.io%2fv1alpha2)
    route:
      main:
        # -- Enables or disables the route
        enabled: false

        # -- Set the route apiVersion, e.g. gateway.networking.k8s.io/v1 or gateway.networking.k8s.io/v1alpha2
        apiVersion: gateway.networking.k8s.io/v1
        # -- Set the route kind
        # Valid options are GRPCRoute, HTTPRoute, TCPRoute, TLSRoute, UDPRoute
        kind: HTTPRoute

        annotations: {}
        labels: {}

        hostnames: []
        # - my-filter.example.com
        parentRefs: []
        # - name: acme-gw

        matches:
          - path:
              type: PathPrefix
              value: /

        ## Filters define the filters that are applied to requests that match this rule.
        filters: []

        ## Additional custom rules that can be added to the route
        additionalRules: []

    ## Configuration for ThanosRuler service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

      ## Port for ThanosRuler Service to listen on
      ##
      port: 10902
      ## To be used with a proxy extraContainer port
      ##
      targetPort: 10902
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30905
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##

      ## Additional ports to open for ThanosRuler service
      additionalPorts: []

      externalIPs: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

    ## Configuration for creating a ServiceMonitor for the ThanosRuler service
    ##
    serviceMonitor:
      ## If true, create a serviceMonitor for thanosRuler
      ##
      selfMonitor: true

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## Additional labels
      ##
      additionalLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
      tlsConfig: {}

      bearerTokenFile:

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics

    ## Settings affecting thanosRulerpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec
    ##
    thanosRulerSpec:
      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.
      ##
      podMetadata: {}

      ## Image of ThanosRuler
      ##
      image:
        registry: quay.io
        repository: thanos/thanos
        tag: v0.37.2
        sha: ""

      ## Namespaces to be selected for PrometheusRules discovery.
      ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage
      ##
      ruleNamespaceSelector: {}

      ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ##
      ruleSelectorNilUsesHelmValues: true

      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector: {}
      ## Example which select all PrometheusRules resources
      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
      # ruleSelector:
      #   matchExpressions:
      #     - key: prometheus
      #       operator: In
      #       values:
      #         - example-rules
      #         - example-rules-2
      #
      ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
      # ruleSelector:
      #   matchLabels:
      #     role: example-rules

      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: logfmt

      ## Log level for ThanosRuler to be configured with.
      ##
      logLevel: info

      ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1

      ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 24h

      ## Interval between consecutive evaluations.
      ##
      evaluationInterval: ""

      ## Storage is the definition of how storage will be used by the ThanosRuler instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storage: {}
      # volumeClaimTemplate:
      #   spec:
      #     storageClassName: gluster
      #     accessModes: ["ReadWriteOnce"]
      #     resources:
      #       requests:
      #         storage: 50Gi
      #   selector: {}

      ## AlertmanagerConfig define configuration for connecting to alertmanager.
      ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.
      alertmanagersConfig:
        # use existing secret, if configured, alertmanagersConfig.secret will not be used
        existingSecret: {}
          # name: ""
          # key: ""
        # will render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set
        # https://thanos.io/tip/components/rule.md/#alertmanager
        secret: {}
          # alertmanagers:
          # - api_version: v2
          #   http_config:
          #     basic_auth:
          #       username: some_user
          #       password: some_pass
          #   static_configs:
          #     - alertmanager.thanos.io
          #   scheme: http
          #   timeout: 10s

      ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.
      ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.
      # alertmanagersUrl:

      ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false
      ##
      externalPrefix:

      ## If true, http://{{ template "kube-prometheus-stack.thanosRuler.name" . }}.{{ template "kube-prometheus-stack.namespace" . }}:{{ .Values.thanosRuler.service.port }}
      ## will be used as value for externalPrefix
      externalPrefixNilUsesHelmValues: true

      ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
      ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
      ##
      routePrefix: /

      ## ObjectStorageConfig configures object storage in Thanos
      objectStorageConfig:
        # use existing secret, if configured, objectStorageConfig.secret will not be used
        existingSecret: {}
          # name: ""
          # key: ""
        # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set
        # https://thanos.io/tip/thanos/storage.md/#s3
        secret: {}
          # type: S3
          # config:
          #   bucket: ""
          #   endpoint: ""
          #   region: ""
          #   access_key: ""
          #   secret_key: ""

      ## Labels by name to drop before sending to alertmanager
      ## Maps to the --alert.label-drop flag of thanos ruler.
      alertDropLabels: []

      ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.
      ## Maps to the --query flag of thanos ruler.
      queryEndpoints: []

      ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
      ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
      queryConfig:
        # use existing secret, if configured, queryConfig.secret will not be used
        existingSecret: {}
          # name: ""
          # key: ""
        # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set
        # https://thanos.io/tip/components/rule.md/#query-api
        secret: {}
          # - http_config:
          #     basic_auth:
          #       username: some_user
          #       password: some_pass
          #   static_configs:
          #     - URL
          #   scheme: http
          #   timeout: 10s

      ## Labels configure the external label pairs to ThanosRuler. A default replica
      ## label `thanos_ruler_replica` will be always added as a label with the value
      ## of the pod's name and it will be dropped in the alerts.
      labels: {}

      ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
      ##
      paused: false

      ## Allows setting additional arguments for the ThanosRuler container
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler
      ##
      additionalArgs: []
        # - name: remote-write.config
        #   value: |-
        #     "remote_write":
        #     - "name": "receiver-0"
        #       "remote_timeout": "30s"
        #       "url": "http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive"

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Define resources requests and limits for single Pods.
      ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      ##
      podAntiAffinity: "soft"

      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname

      ## Assign custom affinity rules to the thanosRuler instance
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2

      ## If specified, the pod's tolerations.
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"

      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: thanos-ruler

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault

      ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.
      ## Note this is only for the ThanosRuler UI, not the gossip communication.
      ##
      listenLocal: false

      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.
      ##
      containers: []

      # Additional volumes on the output StatefulSet definition.
      volumes: []

      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []

      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []

      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""

      ## PortName to use for ThanosRuler.
      ##
      portName: "web"

      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerwebspec
      web: {}

      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}

      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating
      additionalConfigString: ""

    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  ##
  cleanPrometheusOperatorObjectNames: false

  ## Extra manifests to deploy as an array
  extraManifests: []
    # - apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #   labels:
    #     name: prometheus-extra
    #   data:
    #     extra-data: "value"



# Thanos
thanos:
  enabled: true
  # Copyright Broadcom, Inc. All Rights Reserved.
  # SPDX-License-Identifier: APACHE-2.0

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## @param global.defaultStorageClass Global default StorageClass for Persistent Volume(s)
  ## @param global.storageClass DEPRECATED: use global.defaultStorageClass instead
  ##
  global:
    imageRegistry: ""
    ## e.g:
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    defaultStorageClass: ""
    storageClass: ""
    ## Security parameters
    ##
    security:
      ## @param global.security.allowInsecureImages Allows skipping image verification
      allowInsecureImages: false
    ## Compatibility adaptations for Kubernetes platforms
    ##
    compatibility:
      ## Compatibility adaptations for Openshift
      ##
      openshift:
        ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
        ##
        adaptSecurityContext: auto
  ## @section Common parameters

  ## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)
  ##
  kubeVersion: ""
  ## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
  ##
  nameOverride: ""
  ## @param fullnameOverride String to fully override common.names.fullname template
  ##
  fullnameOverride: ""
  ## @param commonLabels Add labels to all the deployed resources
  ##
  commonLabels: {}
  ## @param commonAnnotations Add annotations to all the deployed resources
  ##
  commonAnnotations: {}
  ## @param clusterDomain Kubernetes Cluster Domain
  ##
  clusterDomain: cluster.local
  ## @param extraDeploy Array of extra objects to deploy with the release
  ##
  extraDeploy: []
  ## @section Thanos common parameters

  ## Bitnami Thanos image
  ## ref: https://hub.docker.com/r/bitnami/thanos/tags/
  ## @param image.registry [default: REGISTRY_NAME] Thanos image registry
  ## @param image.repository [default: REPOSITORY_NAME/thanos] Thanos image repository
  ## @skip image.tag Thanos image tag (immutable tags are recommended)
  ## @param image.digest Thanos image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy Thanos image pull policy
  ## @param image.pullSecrets Specify docker-registry secret names as an array
  ##
  image:
    registry: docker.io
    repository: bitnami/thanos
    tag: 0.37.2-debian-12-r0
    digest: ""
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## @param objstoreConfig The [objstore configuration](https://thanos.io/tip/thanos/storage.md/)
  ## Specify content for objstore.yml
  ##
  objstoreConfig: ""
  ## @param indexCacheConfig The [index cache configuration](https://thanos.io/tip/components/store.md/)
  ## Specify content for index-cache.yml
  ##
  indexCacheConfig: ""
  ## @param bucketCacheConfig The [bucket cache configuration](https://thanos.io/tip/components/store.md/)
  ## Specify content for bucket-cache.yml
  ##
  bucketCacheConfig: ""
  ## @param existingObjstoreSecret Secret with Objstore Configuration
  ## Note: This will override objstoreConfig
  ##
  existingObjstoreSecret: ""
  ## @param existingObjstoreSecretItems Optional item list for specifying a custom Secret key. If so, path should be objstore.yml
  ##
  existingObjstoreSecretItems: []
  ## @param httpConfig The [https and basic auth configuration](https://thanos.io/tip/operating/https.md/)
  ## If provided, overrides settings under https.* and auth.*
  httpConfig: ""
  ## @param existingHttpConfigSecret Secret containing the HTTPS and Basic auth configuration
  ##
  existingHttpConfigSecret: ""
  ## HTTPS configuration (Experimental)
  ## Ref: https://thanos.io/tip/operating/https.md/
  ##
  https:
    ## @param https.enabled Set to true to enable HTTPS. Requires a secret containing the certificate and key.
    ##
    enabled: false
    ## @param https.autoGenerated Create self-signed TLS certificates.
    ##
    autoGenerated: false
    ## @param https.existingSecret Existing secret containing your own server key and certificate
    ##
    existingSecret: ""
    ## @param https.certFilename
    ##
    certFilename: "tls.crt"
    ## @param https.keyFilename
    ##
    keyFilename: "tls.key"
    ## @param https.caFilename
    ##
    caFilename: "ca.crt"
    ## @param https.key TLS Key for Thanos HTTPS - ignored if existingSecret is provided
    ## @param https.cert TLS Certificate for Thanos HTTPS - ignored if existingSecret is provided
    ## @param https.ca (Optional, used for client) CA Certificate for Thanos HTTPS - ignored if existingSecret is provided
    ##
    key: ""
    cert: ""
    ca: ""
    ## @param https.clientAuthType Server policy for client authentication using certificates. Maps to ClientAuth Policies.
    ## For more detail on clientAuth options: https://golang.org/pkg/crypto/tls/#ClientAuthType
    clientAuthType: ""
    ## @param https.extraTlsServerConfig Extra tls_server_config options
    ## For more detail on possible options: https://thanos.io/tip/operating/https.md
    extraTlsServerConfig: {}
  ## Thanos Basic authentication (Experimental)
  ##
  auth:
    ## @param auth.basicAuthUsers Object containing <user>:<passwords> key-value pairs for each user that will have access via basic authentication
    ## Note: Passwords will be later encrypted using bcrypt
    basicAuthUsers: {}
  ## @section Thanos Query parameters
  query:
    ## @param query.enabled Set to true to enable Thanos Query component
    ##
    enabled: true
    ## @param query.logLevel Thanos Query log level
    ##
    logLevel: info
    ## @param query.logFormat Thanos Query log format
    ##
    logFormat: logfmt
    ## @param query.replicaLabel Replica indicator(s) along which data is de-duplicated
    ##
    replicaLabel: [replica]
    ## Dynamically configure store APIs using DNS discovery
    ## @param query.dnsDiscovery.enabled Enable store APIs discovery via DNS
    ## @param query.dnsDiscovery.sidecarsService Sidecars service name to discover them using DNS discovery
    ## @param query.dnsDiscovery.sidecarsNamespace Sidecars namespace to discover them using DNS discovery
    ##
    dnsDiscovery:
      enabled: true
      sidecarsService: ""
      sidecarsNamespace: ""
    ## @param query.stores Statically configure store APIs to connect with Thanos Query
    ##
    stores: []
    ## @param query.sdConfig Query Service Discovery Configuration
    ## Specify content for servicediscovery.yml
    ##
    sdConfig: ""
    ## @param query.existingSDConfigmap Name of existing ConfigMap with Ruler configuration
    ## NOTE: This will override query.sdConfig
    ##
    existingSDConfigmap: ""
    ## @param query.extraEnvVars Extra environment variables for Thanos Query container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param query.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Query nodes
    ##
    extraEnvVarsCM: ""
    ## @param query.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Query nodes
    ##
    extraEnvVarsSecret: ""
    ## @param query.extraFlags Extra Flags to passed to Thanos Query
    ##
    extraFlags: []
    ## @param query.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param query.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param query.replicaCount Number of Thanos Query replicas to deploy
    ##
    replicaCount: 1
    ## @param query.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param query.updateStrategy.type Update strategy type for Thanos Query replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param query.containerPorts.http HTTP container port
    ## @param query.containerPorts.grpc HTTP container port
    ##
    containerPorts:
      http: 10902
      grpc: 10901
    ## K8s Pod Security Context for Thanos Query pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param query.podSecurityContext.enabled Enable security context for the Thanos Query pods
    ## @param query.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param query.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param query.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param query.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Query pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Query containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param query.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param query.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param query.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param query.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param query.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param query.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param query.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param query.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param query.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param query.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Query containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param query.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if query.resources is set (query.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param query.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Query containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param query.livenessProbe.enabled Enable livenessProbe on Thanos Query containers
    ## @param query.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param query.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param query.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param query.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param query.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param query.readinessProbe.enabled Enable readinessProbe on Thanos Query containers
    ## @param query.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param query.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param query.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param query.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param query.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param query.startupProbe.enabled Enable startupProbe on Thanos Query containers
    ## @param query.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param query.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param query.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param query.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param query.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param query.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param query.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param query.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param query.initContainers Add additional init containers to the Thanos Query pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param query.sidecars Extra containers running as sidecars to Thanos Query pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param query.extraVolumes Extra volumes to add to Thanos Query
    ##
    extraVolumes: []
    ## @param query.extraVolumeMounts Extra volume mounts to add to the query container
    ##
    extraVolumeMounts: []
    ## @param query.podAffinityPreset Thanos Query pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param query.podAntiAffinityPreset Thanos Query pod anti-affinity preset. Ignored if `query.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## @param query.podAntiAffinityPresetTopologyKey Thanos Query pod anti-affinity topologyKey. Ignored if `query.affinity` is set.
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPresetTopologyKey: ""
    ## Thanos Query node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param query.nodeAffinityPreset.type Thanos Query node affinity preset type. Ignored if `query.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param query.nodeAffinityPreset.key Thanos Query node label key to match Ignored if `query.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param query.nodeAffinityPreset.values Thanos Query node label values to match. Ignored if `query.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param query.affinity Thanos Query affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: query.podAffinityPreset, query.podAntiAffinityPreset, and query.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param query.nodeSelector Thanos Query node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param query.tolerations Thanos Query tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param query.podLabels Thanos Query pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param query.podAnnotations Annotations for Thanos Query pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param query.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param query.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param query.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param query.lifecycleHooks for the Thanos Query container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param query.priorityClassName Thanos Query priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param query.schedulerName Name of the k8s scheduler (other than default) for Thanos Query pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param query.topologySpreadConstraints Topology Spread Constraints for Thanos Query pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## Thanos Query GRPC parameters
    ## ref: https://github.com/thanos-io/thanos/blob/master/docs/components/query.md#flags
    ##
    grpc:
      ## GRPC server side
      ##
      server:
        ## TLS configuration
        ## @param query.grpc.server.tls.enabled Enable TLS encryption in the GRPC server
        ## @param query.grpc.server.tls.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates
        ## @param query.grpc.server.tls.cert TLS Certificate for GRPC server - ignored if existingSecret is provided
        ## @param query.grpc.server.tls.key TLS Key for GRPC server - ignored if existingSecret is provided
        ## @param query.grpc.server.tls.ca TLS CA to verify clients against - ignored if existingSecret is provided
        ## @param query.grpc.server.tls.clientAuthEnabled Enable TLS client verification against provided CA
        ## @param query.grpc.server.tls.existingSecret Existing secret containing your own TLS certificates
        ## e.g:
        ## existingSecret:
        ##   name: foo
        ##   keyMapping:
        ##     ca-cert: ca.pem
        ##     tls-cert: cert.pem
        ##     tls-key: key.pem
        ##
        tls:
          enabled: false
          autoGenerated: false
          cert: ""
          key: ""
          ca: ""
          clientAuthEnabled: true
          existingSecret: {}
      ## GRPC client side
      ##
      client:
        ## @param query.grpc.client.serverName Server name to verify the hostname on the returned GRPC certificates
        ##
        serverName: ""
        ## TLS configuration
        ## @param query.grpc.client.tls.enabled Enable TLS encryption in the GRPC server
        ## @param query.grpc.client.tls.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates
        ## @param query.grpc.client.tls.cert TLS Certificate for GRPC server - ignored if existingSecret is provided
        ## @param query.grpc.client.tls.key TLS Key for GRPC server - ignored if existingSecret is provided
        ## @param query.grpc.client.tls.ca TLS CA to verify clients against - ignored if existingSecret is provided
        ## @param query.grpc.client.tls.existingSecret Existing secret containing your own TLS certificates
        ## e.g:
        ## existingSecret:
        ##   name: foo
        ##   keyMapping:
        ##     ca-cert: ca.pem
        ##     tls-cert: cert.pem
        ##     tls-key: key.pem
        ##
        tls:
          enabled: false
          autoGenerated: false
          cert: ""
          key: ""
          ca: ""
          existingSecret: {}
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param query.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param query.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param query.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param query.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param query.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param query.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param query.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param query.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param query.service.ports.http Thanos Query service HTTP port
      ##
      ports:
        http: 9090
      ## @param query.service.nodePorts.http Specify the Thanos Query HTTP nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
      ## @param query.service.clusterIP Thanos Query service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param query.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param query.service.loadBalancerSourceRanges Address that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param query.service.externalTrafficPolicy Thanos Query service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param query.service.labels Labels for Thanos Query service
      ##
      labels: {}
      ## @param query.service.annotations Annotations for Thanos Query service
      ##
      annotations: {}
      ## @param query.service.extraPorts Extra ports to expose in the Thanos Query service
      ##
      extraPorts: []
      ## @param query.service.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
      ## @param query.service.additionalHeadless Additional Headless service
      ##
      additionalHeadless: false
      ## Headless service properties
      ##
      headless:
        ## @param query.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
    ## Service GRPC parameters
    ##
    serviceGrpc:
      ## @param query.serviceGrpc.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param query.serviceGrpc.ports.grpc Thanos Query service GRPC port
      ##
      ports:
        grpc: 10901
      ## @param query.serviceGrpc.nodePorts.grpc Specify the Thanos Query GRPC nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        grpc: ""
      ## @param query.serviceGrpc.clusterIP Thanos Query service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param query.serviceGrpc.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param query.serviceGrpc.loadBalancerSourceRanges Address that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param query.serviceGrpc.externalTrafficPolicy Thanos Query service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param query.serviceGrpc.labels Labels for Thanos Query service GRPC
      ##
      labels: {}
      ## @param query.serviceGrpc.annotations Annotations for Thanos Query service
      ##
      annotations: {}
      ## @param query.serviceGrpc.extraPorts Extra ports to expose in the Thanos Query service
      ##
      extraPorts: []
      ## @param query.serviceGrpc.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
      ## @param query.serviceGrpc.additionalHeadless Additional Headless service
      ##
      additionalHeadless: false
      ## Headless service properties
      ##
      headless:
        ## @param query.serviceGrpc.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
    ## Autoscaling parameters
    ## @param query.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the deployment
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param query.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param query.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param query.serviceAccount.annotations Annotations for Thanos Query Service Account
    ## @param query.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## RBAC configuration
    ##
    rbac:
      ## @param query.rbac.create Create a ClusterRole and ClusterRoleBinding for the Thanos Query Service Account
      ##
      create: false
      ## @param query.rbac.rules Custom RBAC rules to set
      ## e.g:
      ## rules:
      ##   - apiGroups:
      ##       - ""
      ##     resources:
      ##       - pods
      ##     verbs:
      ##       - get
      ##       - list
      ##
      rules: []
    ## @param query.pspEnabled Whether to create a PodSecurityPolicy for Thanos Query
    ## WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later
    ##
    pspEnabled: false
    ## Thanos Query Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param query.autoscaling.enabled Enable autoscaling for Thanos Query
    ## @param query.autoscaling.minReplicas Minimum number of Thanos Query replicas
    ## @param query.autoscaling.maxReplicas Maximum number of Thanos Query replicas
    ## @param query.autoscaling.targetCPU Target CPU utilization percentage
    ## @param query.autoscaling.targetMemory Target Memory utilization percentage
    ## @param query.autoscaling.targetPodMetrics [array] List of custom pod metrics name and averageValue to evaluate for scaling
    ## e.g.
    ## targetPodMetrics:
    ## - name: thanos_query_range_query_duration
    ##   averageValue: 2
    ## - name: thanos_query_promql_duration_metric
    ##   averageValue: 0.5
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
      targetPodMetrics: []
    ## Thanos Query Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param query.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Query
    ## @param query.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param query.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Configure the ingress resource that allows you to access Thanos Query
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param query.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param query.ingress.hostname Default host for the ingress resource
      ##
      hostname: thanos.local
      ## @param query.ingress.secretName Custom secretName for the ingress resource
      ## If query.ingress.secretName is not set, the secret will be named as follows: query.ingress.hostname-tls
      secretName: ""
      ## @param query.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param query.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param query.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##
      extraHosts: []
      ## @param query.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos.local
      ##   secretName: thanos.local-tls
      ##
      extraTls: []
      ## @param query.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param query.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param query.ingress.tls Enable TLS configuration for the hostname defined at `query.ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.query.ingress.hostname }}`
      ## You can:
      ##   - Use the `query.ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `query.ingress.selfSigned=true`
      ##
      tls: false
      ## @param query.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param query.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param query.ingress.path Ingress path
      ##
      path: /
      ## @param query.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
      ## Create an ingress object for the GRPC service. This requires an HTTP/2
      ## capable Ingress controller (eg. traefik using AWS NLB). Example annotations
      ## - ingress.kubernetes.io/protocol: h2c
      ## - service.beta.kubernetes.io/aws-load-balancer-type: nlb
      ## - service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
      ## For more information see https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/
      ## and also the documentation for your ingress controller.
      ##
      ## The options that are accepted are identical to the HTTP one listed above
      ##
      grpc:
        ## @param query.ingress.grpc.enabled Enable ingress controller resource (GRPC)
        ##
        enabled: false
        ## @param query.ingress.grpc.hostname Default host for the ingress resource (GRPC)
        ##
        hostname: thanos-grpc.local
        ## @param query.ingress.grpc.secretName Custom secretName for the ingress resource (GRPC)
        ## If query.ingress.grpc.secretName is not set, the secret will be named as follows: query.ingress.grpc.hostname-tls
        secretName: ""
        ## @param query.ingress.grpc.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
        ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
        ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
        ##
        ingressClassName: ""
        ## @param query.ingress.grpc.annotations Additional annotations for the Ingress resource (GRPC). To enable certificate autogeneration, place here your cert-manager annotations.
        ## For a full list of possible ingress annotations, please see
        ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
        ## Use this parameter to set the required annotations for cert-manager, see
        ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
        ##
        ## Examples:
        ## kubernetes.io/ingress.class: nginx
        ## cert-manager.io/cluster-issuer: cluster-issuer-name
        ##
        annotations: {}
        ## @param query.ingress.grpc.extraHosts The list of additional hostnames to be covered with this ingress record.
        ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
        ## extraHosts:
        ## - name: thanos-grpc.local
        ##   path: /
        ##
        extraHosts: []
        ## @param query.ingress.grpc.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
        ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
        ## extraTls:
        ## - hosts:
        ##     - thanos-grpc.local
        ##   secretName: thanos-grpc.local-tls
        ##
        extraTls: []
        ## @param query.ingress.grpc.secrets If you're providing your own certificates, please use this to add the certificates as secrets
        ## key and certificate should start with -----BEGIN CERTIFICATE----- or
        ## -----BEGIN RSA PRIVATE KEY-----
        ##
        ## name should line up with a tlsSecret set further up
        ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
        ##
        ## It is also possible to create and manage the certificates outside of this helm chart
        ## Please see README.md for more information
        ## e.g:
        ## - name: thanos-grpc.local-tls
        ##   key:
        ##   certificate:
        ##
        secrets: []
        ## @param query.ingress.grpc.extraRules Additional rules to be covered with this ingress record
        ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
        ## e.g:
        ## extraRules:
        ## - host: example.local
        ##     http:
        ##       path: /
        ##       backend:
        ##         service:
        ##           name: example-svc
        ##           port:
        ##             name: http
        ##
        extraRules: []
        ## @param query.ingress.grpc.tls Enable TLS configuration for the hostname defined at `query.ingress.grpc.hostname` parameter
        ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.query.ingress.grpc.hostname }}`
        ## You can:
        ##   - Use the `query.ingress.grpc.secrets` parameter to create this TLS secret
        ##   - Rely on cert-manager to create it by setting the corresponding annotations
        ##   - Rely on Helm to create self-signed certificates by setting `query.ingress.grpc.selfSigned=true`
        ##
        tls: false
        ## @param query.ingress.grpc.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
        ##
        selfSigned: false
        ## @param query.ingress.grpc.apiVersion Override API Version (automatically detected if not set)
        ##
        apiVersion: ""
        ## @param query.ingress.grpc.path Ingress Path
        ##
        path: /
        ## @param query.ingress.grpc.pathType Ingress Path type
        ##
        pathType: ImplementationSpecific
  ## @section Thanos Query Frontend parameters
  queryFrontend:
    ## @param queryFrontend.enabled Enable/disable Thanos Query Frontend component
    ##
    enabled: true
    ## @param queryFrontend.logLevel Thanos Query Frontend log level
    ##
    logLevel: info
    ## @param queryFrontend.logFormat Thanos Query Frontend log format
    ##
    logFormat: logfmt
    ## @param queryFrontend.config Thanos Query Frontend configuration
    ## Specify content for config.yml
    ##
    config: ""
    ## @param queryFrontend.existingConfigmap Name of existing ConfigMap with Thanos Query Frontend configuration
    ## NOTE: This will override queryFrontend.config
    ##
    existingConfigmap: ""
    ## @param queryFrontend.extraEnvVars Extra environment variables for Thanos Query Frontend container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param queryFrontend.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Query Frontend nodes
    ##
    extraEnvVarsCM: ""
    ## @param queryFrontend.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Query Frontend nodes
    ##
    extraEnvVarsSecret: ""
    ## @param queryFrontend.extraFlags Extra Flags to passed to Thanos Query Frontend
    ##
    extraFlags: []
    ## @param queryFrontend.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param queryFrontend.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param queryFrontend.replicaCount Number of Thanos Query Frontend replicas to deploy
    ##
    replicaCount: 1
    ## @param queryFrontend.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param queryFrontend.updateStrategy.type Update strategy type for Thanos Query Frontend replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param queryFrontend.containerPorts.http HTTP container port
    ##
    containerPorts:
      http: 9090
    ## K8s Pod Security Context for Thanos Query Frontend pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param queryFrontend.podSecurityContext.enabled Enable security context for the Thanos Query Frontend pods
    ## @param queryFrontend.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param queryFrontend.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param queryFrontend.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param queryFrontend.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Query Frontend pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Query Frontend containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param queryFrontend.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param queryFrontend.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param queryFrontend.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param queryFrontend.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param queryFrontend.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param queryFrontend.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param queryFrontend.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param queryFrontend.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param queryFrontend.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param queryFrontend.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Query Frontend containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param queryFrontend.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if queryFrontend.resources is set (queryFrontend.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param queryFrontend.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Query Frontend containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param queryFrontend.livenessProbe.enabled Enable livenessProbe on Thanos Query Frontend containers
    ## @param queryFrontend.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param queryFrontend.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param queryFrontend.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param queryFrontend.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param queryFrontend.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param queryFrontend.readinessProbe.enabled Enable readinessProbe on Thanos Query Frontend containers
    ## @param queryFrontend.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param queryFrontend.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param queryFrontend.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param queryFrontend.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param queryFrontend.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param queryFrontend.startupProbe.enabled Enable startupProbe on Thanos Query Frontend containers
    ## @param queryFrontend.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param queryFrontend.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param queryFrontend.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param queryFrontend.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param queryFrontend.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param queryFrontend.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param queryFrontend.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param queryFrontend.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param queryFrontend.initContainers Add additional init containers to the Thanos Query Frontend pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param queryFrontend.sidecars Extra containers running as sidecars to Thanos Query Frontend pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param queryFrontend.extraVolumes Extra volumes to add to Thanos Query Frontend
    ##
    extraVolumes: []
    ## @param queryFrontend.extraVolumeMounts Extra volume mounts to add to the query-frontend container
    ##
    extraVolumeMounts: []
    ## @param queryFrontend.podAffinityPreset Thanos Query Frontend pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param queryFrontend.podAntiAffinityPreset Thanos Query Frontend pod anti-affinity preset. Ignored if `queryFrontend.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Query Frontend node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param queryFrontend.nodeAffinityPreset.type Thanos Query Frontend node affinity preset type. Ignored if `queryFrontend.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param queryFrontend.nodeAffinityPreset.key Thanos Query Frontend node label key to match. Ignored if `queryFrontend.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param queryFrontend.nodeAffinityPreset.values Thanos Query Frontend node label values to match. Ignored if `queryFrontend.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param queryFrontend.affinity Thanos Query Frontend affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: queryFrontend.podAffinityPreset, queryFrontend.podAntiAffinityPreset, and queryFrontend.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param queryFrontend.nodeSelector Thanos Query Frontend node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param queryFrontend.tolerations Thanos Query Frontend tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param queryFrontend.podLabels Thanos Query Frontend pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param queryFrontend.podAnnotations Annotations for Thanos Query Frontend pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param queryFrontend.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param queryFrontend.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param queryFrontend.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param queryFrontend.lifecycleHooks for the Thanos Query Frontend container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param queryFrontend.priorityClassName Thanos Query Frontend priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param queryFrontend.schedulerName Name of the k8s scheduler (other than default) for Thanos Query Frontend pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param queryFrontend.topologySpreadConstraints Topology Spread Constraints for Thanos Query Frontend pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param queryFrontend.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param queryFrontend.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param queryFrontend.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param queryFrontend.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param queryFrontend.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param queryFrontend.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param queryFrontend.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param queryFrontend.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param queryFrontend.service.ports.http Thanos Query Frontend service HTTP port
      ##
      ports:
        http: 9090
      ## @param queryFrontend.service.nodePorts.http Specify the Thanos Query Frontend HTTP nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
      ## @param queryFrontend.service.clusterIP Thanos Query Frontend service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param queryFrontend.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param queryFrontend.service.loadBalancerSourceRanges Address that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param queryFrontend.service.externalTrafficPolicy Thanos Query Frontend service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param queryFrontend.service.annotations Annotations for Thanos Query Frontend service
      ##
      annotations: {}
      ## @param queryFrontend.service.labels Labels for Thanos Query Frontend service
      ##
      labels: {}
      ## @param queryFrontend.service.extraPorts Extra ports to expose in the Thanos Query Frontend service
      ##
      extraPorts: []
      ## @param queryFrontend.service.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
    ## @param queryFrontend.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the deployment
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param queryFrontend.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param queryFrontend.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param queryFrontend.serviceAccount.annotations Annotations for Thanos Query Frontend Service Account
    ## @param queryFrontend.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## RBAC configuration
    ##
    rbac:
      ## @param queryFrontend.rbac.create Create a ClusterRole and ClusterRoleBinding for the Thanos Query Frontend Service Account
      ##
      create: false
      ## @param queryFrontend.rbac.rules Custom RBAC rules to set
      ## e.g:
      ## rules:
      ##   - apiGroups:
      ##       - ""
      ##     resources:
      ##       - pods
      ##     verbs:
      ##       - get
      ##       - list
      ##
      rules: []
    ## @param queryFrontend.pspEnabled Whether to create a PodSecurityPolicy for Thanos Query Frontend
    ## WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later
    ##
    pspEnabled: false
    ## Thanos Query Frontend Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param queryFrontend.autoscaling.enabled Enable autoscaling for Thanos Query Frontend
    ## @param queryFrontend.autoscaling.minReplicas Minimum number of Thanos Query Frontend replicas
    ## @param queryFrontend.autoscaling.maxReplicas Maximum number of Thanos Query Frontend replicas
    ## @param queryFrontend.autoscaling.targetCPU Target CPU utilization percentage
    ## @param queryFrontend.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
    ## Thanos Query Frontend Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param queryFrontend.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Query Frontend
    ## @param queryFrontend.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param queryFrontend.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Configure the ingress resource that allows you to access Thanos Query Frontend
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param queryFrontend.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param queryFrontend.ingress.hostname Default host for the ingress resource
      ##
      hostname: thanos.local
      ## @param queryFrontend.ingress.overrideAlertQueryURL Automatically use query-frontend's ingress hostname as --alert.queryURL for both Query and Ruler.
      ## This is used in order for the expression url on alerts/rules to be correctly rendered on UI as Frontend's hostname, instead of http://localhost:10902
      ##
      overrideAlertQueryURL: true
      ## @param queryFrontend.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param queryFrontend.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param queryFrontend.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##
      extraHosts: []
      ## @param queryFrontend.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos.local
      ##   secretName: thanos.local-tls
      ##
      extraTls: []
      ## @param queryFrontend.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param queryFrontend.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param queryFrontend.ingress.tls Enable TLS configuration for the hostname defined at `queryFrontend.ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.queryFrontend.ingress.hostname }}`
      ## You can:
      ##   - Use the `queryFrontend.ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `queryFrontend.ingress.selfSigned=true`
      ##
      tls: false
      ## @param queryFrontend.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param queryFrontend.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param queryFrontend.ingress.path Ingress path
      ##
      path: /
      ## @param queryFrontend.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
  ## @section Thanos Bucket Web parameters
  bucketweb:
    ## @param bucketweb.enabled Enable/disable Thanos Bucket Web component
    ##
    enabled: false
    ## @param bucketweb.logLevel Thanos Bucket Web log level
    ##
    logLevel: info
    ## @param bucketweb.logFormat Thanos Bucket Web log format
    ##
    logFormat: logfmt
    ## @param bucketweb.refresh Refresh interval to download metadata from remote storage
    ##
    refresh: 30m
    ## @param bucketweb.timeout Timeout to download metadata from remote storage
    ##
    timeout: 5m
    ## @param bucketweb.extraEnvVars Extra environment variables for Thanos Bucket Web container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param bucketweb.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Bucket Web nodes
    ##
    extraEnvVarsCM: ""
    ## @param bucketweb.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Bucket Web nodes
    ##
    extraEnvVarsSecret: ""
    ## @param bucketweb.extraFlags Extra Flags to passed to Thanos Bucket Web
    ##
    extraFlags: []
    ## @param bucketweb.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param bucketweb.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param bucketweb.replicaCount Number of Thanos Bucket Web replicas to deploy
    ##
    replicaCount: 1
    ## @param bucketweb.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param bucketweb.updateStrategy.type Update strategy type for Thanos Bucket Web replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param bucketweb.containerPorts.http HTTP container port
    ##
    containerPorts:
      http: 8080
    ## K8s Pod Security Context for Thanos Bucket Web pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param bucketweb.podSecurityContext.enabled Enable security context for the Thanos Bucket Web pods
    ## @param bucketweb.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param bucketweb.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param bucketweb.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param bucketweb.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Bucket Web pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Bucket Web containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param bucketweb.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param bucketweb.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param bucketweb.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param bucketweb.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param bucketweb.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param bucketweb.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param bucketweb.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param bucketweb.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param bucketweb.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param bucketweb.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Bucket Web containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param bucketweb.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if bucketweb.resources is set (bucketweb.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param bucketweb.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Bucket Web containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param bucketweb.livenessProbe.enabled Enable livenessProbe on Thanos Bucket Web containers
    ## @param bucketweb.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param bucketweb.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param bucketweb.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param bucketweb.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param bucketweb.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param bucketweb.readinessProbe.enabled Enable readinessProbe on Thanos Bucket Web containers
    ## @param bucketweb.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param bucketweb.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param bucketweb.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param bucketweb.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param bucketweb.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param bucketweb.startupProbe.enabled Enable startupProbe on Thanos Bucket Web containers
    ## @param bucketweb.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param bucketweb.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param bucketweb.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param bucketweb.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param bucketweb.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param bucketweb.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param bucketweb.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param bucketweb.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param bucketweb.initContainers Add additional init containers to the Thanos Bucket Web pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param bucketweb.sidecars Extra containers running as sidecars to Thanos Bucket Web pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param bucketweb.extraVolumes Extra volumes to add to Bucket Web
    ##
    extraVolumes: []
    ## @param bucketweb.extraVolumeMounts Extra volume mounts to add to the bucketweb container
    ##
    extraVolumeMounts: []
    ## @param bucketweb.podAffinityPreset Thanos Bucket Web pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param bucketweb.podAntiAffinityPreset Thanos Bucket Web pod anti-affinity preset. Ignored if `bucketweb.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Bucket Web node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param bucketweb.nodeAffinityPreset.type Thanos Bucket Web node affinity preset type. Ignored if `bucketweb.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param bucketweb.nodeAffinityPreset.key Thanos Bucket Web node label key to match. Ignored if `bucketweb.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param bucketweb.nodeAffinityPreset.values Thanos Bucket Web node label values to match. Ignored if `bucketweb.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param bucketweb.affinity Thanos Bucket Web affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: bucketweb.podAffinityPreset, bucketweb.podAntiAffinityPreset, and bucketweb.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param bucketweb.nodeSelector Thanos Bucket Web node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param bucketweb.tolerations Thanos Bucket Web tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param bucketweb.podLabels Thanos Bucket Web pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param bucketweb.podAnnotations Annotations for Thanos Bucket Web pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param bucketweb.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param bucketweb.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param bucketweb.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param bucketweb.lifecycleHooks for the Thanos Bucket Web container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param bucketweb.priorityClassName Thanos Bucket Web priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param bucketweb.schedulerName Name of the k8s scheduler (other than default) for Thanos Bucket Web pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param bucketweb.topologySpreadConstraints Topology Spread Constraints for Thanos Bucket Web pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param bucketweb.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param bucketweb.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param bucketweb.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param bucketweb.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param bucketweb.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param bucketweb.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param bucketweb.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param bucketweb.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param bucketweb.service.ports.http Thanos Bucket Web service HTTP port
      ##
      ports:
        http: 8080
      ## @param bucketweb.service.nodePorts.http Specify the Thanos Bucket Web HTTP nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
      ## @param bucketweb.service.clusterIP Thanos Bucket Web service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param bucketweb.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param bucketweb.service.loadBalancerSourceRanges Address that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ##
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param bucketweb.service.externalTrafficPolicy Thanos Bucket Web service externalTrafficPolicy
      ##
      externalTrafficPolicy: Cluster
      ## @param bucketweb.service.labels Extra labels for Thanos Bucket Web service
      ##
      labels: {}
      ## @param bucketweb.service.annotations Annotations for Thanos Bucket Web service
      ##
      annotations: {}
      ## @param bucketweb.service.extraPorts Extra ports to expose in the Thanos Bucket Web service
      ##
      extraPorts: []
      ## @param bucketweb.service.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
    ## @param bucketweb.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the deployment
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param bucketweb.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param bucketweb.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param bucketweb.serviceAccount.annotations Annotations for Thanos Bucket Web Service Account
    ## @param bucketweb.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## Thanos Bucket Web Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param bucketweb.autoscaling.enabled Enable autoscaling for Thanos Bucket Web
    ## @param bucketweb.autoscaling.minReplicas Minimum number of Thanos Bucket Web replicas
    ## @param bucketweb.autoscaling.maxReplicas Maximum number of Thanos Bucket Web replicas
    ## @param bucketweb.autoscaling.targetCPU Target CPU utilization percentage
    ## @param bucketweb.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
    ## Thanos Bucket Web Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param bucketweb.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Bucket Web
    ## @param bucketweb.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param bucketweb.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Configure the ingress resource that allows you to access Thanos Bucketweb
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param bucketweb.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param bucketweb.ingress.hostname Default host for the ingress resource
      ##
      hostname: thanos-bucketweb.local
      ## @param bucketweb.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param bucketweb.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param bucketweb.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos-bucketweb.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##
      extraHosts: []
      ## @param bucketweb.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos-bucketweb.local
      ##   secretName: thanos-bucketweb.local-tls
      ##
      extraTls: []
      ## @param bucketweb.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos-bucketweb.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param bucketweb.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param bucketweb.ingress.tls Enable TLS configuration for the hostname defined at `bucketweb.ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.bucketweb.ingress.hostname }}`
      ## You can:
      ##   - Use the `bucketweb.ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `bucketweb.ingress.selfSigned=true`
      ##
      tls: false
      ## @param bucketweb.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param bucketweb.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param bucketweb.ingress.path Ingress path
      ##
      path: /
      ## @param bucketweb.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
  ## @section Thanos Compactor parameters
  compactor:
    ## @param compactor.enabled Enable/disable Thanos Compactor component
    ##
    enabled: false
    ## @param compactor.logLevel Thanos Compactor log level
    ##
    logLevel: info
    ## @param compactor.logFormat Thanos Compactor log format
    ##
    logFormat: logfmt
    ## @param compactor.dataDir Thanos Compactor data directory
    ##
    dataDir: /data
    ## Resolution and Retention flags
    ## @param compactor.retentionResolutionRaw Resolution and Retention flag
    ## @param compactor.retentionResolution5m Resolution and Retention flag
    ## @param compactor.retentionResolution1h Resolution and Retention flag
    ##
    retentionResolutionRaw: 30d
    retentionResolution5m: 30d
    retentionResolution1h: 10y
    ## @param compactor.concurrency Number of goroutines to use when compacting groups.
    concurrency: 1
    ## @param compactor.consistencyDelay Minimum age of fresh (non-compacted) blocks before they are being processed
    ##
    consistencyDelay: 30m
    ## @param compactor.extraEnvVars Extra environment variables for Thanos Compactor container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param compactor.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Compactor nodes
    ##
    extraEnvVarsCM: ""
    ## @param compactor.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Compactor nodes
    ##
    extraEnvVarsSecret: ""
    ## @param compactor.extraFlags Extra Flags to passed to Thanos Compactor
    ##
    extraFlags: []
    ## @param compactor.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param compactor.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param compactor.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## K8s CronJob configuration
    ## ref: https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/
    ## @param compactor.cronJob.enabled Run compactor as a CronJob rather than a Deployment
    ## @param compactor.cronJob.schedule The schedule in Cron format, see <https://en.wikipedia.org/wiki/Cron>
    ## @param compactor.cronJob.timeZone The time zone name for the given schedule, see <https://en.wikipedia.org/wiki/List_of_tz_database_time_zones>
    ## @param compactor.cronJob.concurrencyPolicy Specifies how to treat concurrent executions of a Job
    ## @param compactor.cronJob.startingDeadlineSeconds Optional deadline in seconds for starting the job if it misses scheduled time for any reason
    ## @param compactor.cronJob.suspend This flag tells the controller to suspend subsequent executions
    ## @param compactor.cronJob.successfulJobsHistoryLimit The number of successful finished jobs to retain
    ## @param compactor.cronJob.failedJobsHistoryLimit The number of failed finished jobs to retain
    ## @param compactor.cronJob.backoffLimit The number of retries before marking this job failed
    ## @param compactor.cronJob.ttlSecondsAfterFinished The maximum retention before removing the job
    ##
    cronJob:
      enabled: false
      schedule: "0 */6 * * *"
      timeZone: ""
      startingDeadlineSeconds: ""
      concurrencyPolicy: Forbid
      suspend: ""
      successfulJobsHistoryLimit: ""
      failedJobsHistoryLimit: ""
      backoffLimit: ""
      ttlSecondsAfterFinished: ""
    ## @param compactor.restartPolicy Compactor container restart policy.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy
    ##
    restartPolicy: ""
    ## @param compactor.updateStrategy.type Update strategy type for Thanos Compactor replicas
    ##
    updateStrategy:
      type: Recreate
    ## @param compactor.containerPorts.http HTTP container port
    ##
    containerPorts:
      http: 10902
    ## K8s Pod Security Context for Thanos Compactor pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param compactor.podSecurityContext.enabled Enable security context for the Thanos Compactor pods
    ## @param compactor.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param compactor.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param compactor.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param compactor.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Compactor pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Compactor containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param compactor.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param compactor.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param compactor.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param compactor.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param compactor.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param compactor.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param compactor.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param compactor.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param compactor.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param compactor.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Compactor containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param compactor.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if compactor.resources is set (compactor.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param compactor.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Compactor containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param compactor.livenessProbe.enabled Enable livenessProbe on Thanos Compactor containers
    ## @param compactor.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param compactor.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param compactor.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param compactor.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param compactor.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param compactor.readinessProbe.enabled Enable readinessProbe on Thanos Compactor containers
    ## @param compactor.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param compactor.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param compactor.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param compactor.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param compactor.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param compactor.startupProbe.enabled Enable startupProbe on Thanos Compactor containers
    ## @param compactor.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param compactor.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param compactor.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param compactor.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param compactor.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param compactor.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param compactor.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param compactor.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param compactor.initContainers Add additional init containers to the Thanos Compactor pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param compactor.sidecars Extra containers running as sidecars to Thanos Compactor pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param compactor.extraVolumes Extra volumes to add to Thanos Compactor
    ##
    extraVolumes: []
    ## @param compactor.extraVolumeMounts Extra volume mounts to add to the compactor container
    ##
    extraVolumeMounts: []
    ## @param compactor.podAffinityPreset Thanos Compactor pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param compactor.podAntiAffinityPreset Thanos Compactor pod anti-affinity preset. Ignored if `compactor.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Compactor node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param compactor.nodeAffinityPreset.type Thanos Compactor node affinity preset type. Ignored if `compactor.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param compactor.nodeAffinityPreset.key Thanos Compactor node label key to match. Ignored if `compactor.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param compactor.nodeAffinityPreset.values Thanos Compactor node label values to match. Ignored if `compactor.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param compactor.affinity Thanos Compactor affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: compactor.podAffinityPreset, compactor.podAntiAffinityPreset, and compactor.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param compactor.nodeSelector Thanos Compactor node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param compactor.tolerations Thanos Compactor tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param compactor.podLabels Thanos Compactor pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param compactor.podAnnotations Annotations for Thanos Compactor pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param compactor.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param compactor.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param compactor.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param compactor.lifecycleHooks for the Thanos Compactor container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param compactor.priorityClassName Thanos Compactor priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param compactor.schedulerName Name of the k8s scheduler (other than default) for Thanos Compactor pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param compactor.topologySpreadConstraints Topology Spread Constraints for Thanos Compactor pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param compactor.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param compactor.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param compactor.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param compactor.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param compactor.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param compactor.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param compactor.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param compactor.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param compactor.service.ports.http Thanos Compactor service HTTP port
      ##
      ports:
        http: 9090
      ## @param compactor.service.nodePorts.http Specify the Thanos Compactor HTTP nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
      ## @param compactor.service.clusterIP Thanos Compactor service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param compactor.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param compactor.service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param compactor.service.externalTrafficPolicy Thanos Compactor service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param compactor.service.labels Labels for Thanos Compactor service
      ##
      labels: {}
      ## @param compactor.service.annotations Annotations for Thanos Compactor service
      ##
      annotations: {}
      ## @param compactor.service.extraPorts Extra ports to expose in the Thanos Compactor service
      ##
      extraPorts: []
      ## @param compactor.service.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
    ## @param compactor.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the deployment
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param compactor.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param compactor.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param compactor.serviceAccount.annotations Annotations for Thanos Compactor Service Account
    ## @param compactor.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## Configure the ingress resource that allows you to access Thanos Query Frontend
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param compactor.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param compactor.ingress.hostname Default host for the ingress resource
      ##
      hostname: thanos-compactor.local
      ## @param compactor.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param compactor.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param compactor.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##
      extraHosts: []
      ## @param compactor.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos.local
      ##   secretName: thanos.local-tls
      ##
      extraTls: []
      ## @param compactor.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param compactor.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param compactor.ingress.tls Enable TLS configuration for the hostname defined at `compactor.ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.compactor.ingress.hostname }}`
      ## You can:
      ##   - Use the `compactor.ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `compactor.ingress.selfSigned=true`
      ##
      tls: false
      ## @param compactor.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param compactor.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param compactor.ingress.path Ingress path
      ##
      path: /
      ## @param compactor.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
    ## Persistence parameters
    ##
    persistence:
      ## @param compactor.persistence.enabled Enable data persistence using PVC(s) on Thanos Compactor pods
      ##
      enabled: true
      ## @param compactor.persistence.ephemeral Use ephemeral volume for data persistence using PVC(s) on Thanos Compactor pods
      ##
      ephemeral: false
      ## @param compactor.persistence.defaultEmptyDir Defaults to emptyDir if persistence is disabled.
      ##
      defaultEmptyDir: true
      ## @param compactor.persistence.storageClass Specify the `storageClass` used to provision the volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ## set, choosing the default provisioner.
      ##
      storageClass: ""
      ## @param compactor.persistence.accessModes PVC Access Modes for data volume
      ##
      accessModes:
        - ReadWriteOnce
      ## @param compactor.persistence.size PVC Storage Request for data volume
      ##
      size: 8Gi
      ## @param compactor.persistence.labels Labels for the PVC
      ##
      labels: {}
      ## @param compactor.persistence.annotations Annotations for the PVC
      ##
      annotations: {}
      ## @param compactor.persistence.existingClaim Name of an existing PVC to use
      ## If defined, PVC must be created manually before volume will be bound
      ##
      existingClaim: ""
  ## @section Thanos Store Gateway parameters
  storegateway:
    ## @param storegateway.enabled Enable/disable Thanos Store Gateway component
    ##
    enabled: false
    ## @param storegateway.logLevel Thanos Store Gateway log level
    ##
    logLevel: info
    ## @param storegateway.logFormat Thanos Store Gateway log format
    ##
    logFormat: logfmt
    ## @param storegateway.useEndpointGroup Specify whether to use `endpoint-group` when querying the Store API of HA Store Gateway replicas
    ## NOTE: This will take effect in the querier configuration
    ##
    useEndpointGroup: false
    ## @param storegateway.config Thanos Store Gateway configuration
    ## Specify content for config.yml
    ##
    config: ""
    ## @param storegateway.existingConfigmap Name of existing ConfigMap with Thanos Store Gateway configuration
    ## NOTE: This will override storegateway.config
    ##
    existingConfigmap: ""
    ## Thanos Store Gateway GRPC parameters
    ## ref: https://github.com/thanos-io/thanos/blob/master/docs/components/store.md#flags
    ##
    grpc:
      ## GRPC server side
      ##
      server:
        ## TLS configuration
        ## @param storegateway.grpc.server.tls.enabled Enable TLS encryption in the GRPC server
        ## @param storegateway.grpc.server.tls.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates
        ## @param storegateway.grpc.server.tls.cert TLS Certificate for GRPC server - ignored if existingSecret is provided
        ## @param storegateway.grpc.server.tls.key TLS Key for GRPC server - ignored if existingSecret is provided
        ## @param storegateway.grpc.server.tls.ca TLS CA to verify clients against - ignored if existingSecret is provided
        ## @param storegateway.grpc.server.tls.clientAuthEnabled Enable TLS client verification against provided CA
        ## @param storegateway.grpc.server.tls.existingSecret Existing secret containing your own TLS certificates
        ## e.g:
        ## existingSecret:
        ##   name: foo
        ##   keyMapping:
        ##     ca-cert: ca.pem
        ##     tls-cert: cert.pem
        ##     tls-key: key.pem
        ##
        tls:
          enabled: false
          autoGenerated: false
          cert: ""
          key: ""
          ca: ""
          clientAuthEnabled: true
          existingSecret: {}
    ## @param storegateway.extraEnvVars Extra environment variables for Thanos Store Gateway container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param storegateway.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Store Gateway nodes
    ##
    extraEnvVarsCM: ""
    ## @param storegateway.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Store Gateway nodes
    ##
    extraEnvVarsSecret: ""
    ## @param storegateway.extraFlags Extra Flags to passed to Thanos Store Gateway
    ##
    extraFlags: []
    ## @param storegateway.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param storegateway.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param storegateway.replicaCount Number of Thanos Store Gateway replicas to deploy
    ##
    replicaCount: 1
    ## @param storegateway.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param storegateway.updateStrategy.type Update strategy type for Thanos Store Gateway replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param storegateway.podManagementPolicy Statefulset Pod management policy: OrderedReady (default) or Parallel
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: OrderedReady
    ## @param storegateway.containerPorts.http HTTP container port
    ## @param storegateway.containerPorts.grpc GRPC container port
    ##
    containerPorts:
      http: 10902
      grpc: 10901
    ## K8s Pod Security Context for Thanos Store Gateway pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param storegateway.podSecurityContext.enabled Enable security context for the Thanos Store Gateway pods
    ## @param storegateway.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param storegateway.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param storegateway.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param storegateway.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Store Gateway pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Store Gateway containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param storegateway.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param storegateway.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param storegateway.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param storegateway.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param storegateway.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param storegateway.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param storegateway.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param storegateway.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param storegateway.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param storegateway.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Store Gateway containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param storegateway.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if storegateway.resources is set (storegateway.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param storegateway.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Store Gateway containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param storegateway.livenessProbe.enabled Enable livenessProbe on Thanos Store Gateway containers
    ## @param storegateway.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param storegateway.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param storegateway.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param storegateway.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param storegateway.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param storegateway.readinessProbe.enabled Enable readinessProbe on Thanos Store Gateway containers
    ## @param storegateway.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param storegateway.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param storegateway.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param storegateway.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param storegateway.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param storegateway.startupProbe.enabled Enable startupProbe on Thanos Store Gateway containers
    ## @param storegateway.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param storegateway.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param storegateway.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param storegateway.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param storegateway.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param storegateway.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param storegateway.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param storegateway.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param storegateway.initContainers Add additional init containers to the Thanos Store Gateway pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param storegateway.sidecars Extra containers running as sidecars to Thanos Store Gateway pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param storegateway.extraVolumes Extra volumes to add to Thanos Store Gateway
    ##
    extraVolumes: []
    ## @param storegateway.extraVolumeMounts Extra volume mounts to add to the storegateway container
    ##
    extraVolumeMounts: []
    ## @param storegateway.podAffinityPreset Thanos Store Gateway pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param storegateway.podAntiAffinityPreset Thanos Store Gateway pod anti-affinity preset. Ignored if `storegateway.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Store Gateway node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param storegateway.nodeAffinityPreset.type Thanos Store Gateway node affinity preset type. Ignored if `storegateway.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param storegateway.nodeAffinityPreset.key Thanos Store Gateway node label key to match. Ignored if `storegateway.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param storegateway.nodeAffinityPreset.values Thanos Store Gateway node label values to match. Ignored if `storegateway.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param storegateway.affinity Thanos Store Gateway affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: storegateway.podAffinityPreset, storegateway.podAntiAffinityPreset, and storegateway.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param storegateway.nodeSelector Thanos Store Gateway node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param storegateway.tolerations Thanos Store Gateway tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param storegateway.podLabels Thanos Store Gateway pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param storegateway.podAnnotations Annotations for Thanos Store Gateway pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param storegateway.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param storegateway.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param storegateway.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param storegateway.lifecycleHooks for the Thanos Store Gateway container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param storegateway.priorityClassName Thanos Store Gateway priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param storegateway.topologySpreadConstraints Topology Spread Constraints for Thanos Store Gateway pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param storegateway.schedulerName Name of the k8s scheduler (other than default) for Thanos Store Gateway pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param storegateway.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param storegateway.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param storegateway.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param storegateway.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param storegateway.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param storegateway.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param storegateway.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param storegateway.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param storegateway.service.ports.http Thanos Store Gateway service HTTP port
      ## @param storegateway.service.ports.grpc Thanos Store Gateway service GRPC port
      ##
      ports:
        http: 9090
        grpc: 10901
      ## @param storegateway.service.nodePorts.http Specify the Thanos Store Gateway HTTP nodePort value for the LoadBalancer and NodePort service types
      ## @param storegateway.service.nodePorts.grpc Specify the Thanos Store Gateway GRPC nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param storegateway.service.clusterIP Thanos Store Gateway service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param storegateway.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param storegateway.service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param storegateway.service.externalTrafficPolicy Thanos Store Gateway service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param storegateway.service.labels Extra labels for Thanos Store Gateway service
      ##
      labels: {}
      ## @param storegateway.service.annotations Annotations for Thanos Store Gateway service
      ##
      annotations: {}
      ## @param storegateway.service.extraPorts Extra ports to expose in the Thanos Store Gateway service
      ##
      extraPorts: []
      ## @param storegateway.service.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
      ## @param storegateway.service.additionalHeadless Additional Headless service
      ##
      additionalHeadless: false
      ## Headless service properties
      ##
      headless:
        ## @param storegateway.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
    ## Persistence parameters
    ##
    persistence:
      ## @param storegateway.persistence.enabled Enable data persistence using PVC(s) on Thanos Store Gateway pods
      ##
      enabled: true
      ## @param storegateway.persistence.storageClass Specify the `storageClass` used to provision the volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ## set, choosing the default provisioner.
      ##
      storageClass: ""
      ## @param storegateway.persistence.accessModes PVC Access Modes for data volume
      ##
      accessModes:
        - ReadWriteOnce
      ## @param storegateway.persistence.size PVC Storage Request for data volume
      ##
      size: 8Gi
      ## @param storegateway.persistence.labels Labels for the PVC
      ##
      labels: {}
      ## @param storegateway.persistence.annotations Annotations for the PVC
      ##
      annotations: {}
      ## @param storegateway.persistence.existingClaim Name of an existing PVC to use
      ## If defined, PVC must be created manually before volume will be bound
      ##
      existingClaim: ""
    ## Persistent Volume Claim Retention Policy
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
    ##
    persistentVolumeClaimRetentionPolicy:
      ## @param storegateway.persistentVolumeClaimRetentionPolicy.enabled Enable Persistent volume retention policy for Thanos Store Gateway Statefulset
      ##
      enabled: false
      ## @param storegateway.persistentVolumeClaimRetentionPolicy.whenScaled Volume retention behavior when the replica count of the StatefulSet is reduced
      ##
      whenScaled: Retain
      ## @param storegateway.persistentVolumeClaimRetentionPolicy.whenDeleted Volume retention behavior that applies when the StatefulSet is deleted
      ##
      whenDeleted: Retain
    ## @param storegateway.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the sts
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param storegateway.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param storegateway.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param storegateway.serviceAccount.annotations Annotations for Thanos Store Gateway Service Account
    ## @param storegateway.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## Thanos Store Gateway Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param storegateway.autoscaling.enabled Enable autoscaling for Thanos Store Gateway
    ## @param storegateway.autoscaling.minReplicas Minimum number of Thanos Store Gateway replicas
    ## @param storegateway.autoscaling.maxReplicas Maximum number of Thanos Store Gateway replicas
    ## @param storegateway.autoscaling.targetCPU Target CPU utilization percentage
    ## @param storegateway.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
    ## Thanos Store Gateway Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param storegateway.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Store Gateway
    ## @param storegateway.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param storegateway.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Configure the ingress resource that allows you to access Thanos Query Frontend
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param storegateway.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param storegateway.ingress.hostname Default host for the ingress resource
      ##
      hostname: thanos-storegateway.local
      ## @param storegateway.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param storegateway.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param storegateway.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##
      extraHosts: []
      ## @param storegateway.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos.local
      ##   secretName: thanos.local-tls
      ##
      extraTls: []
      ## @param storegateway.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param storegateway.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param storegateway.ingress.tls Enable TLS configuration for the hostname defined at `storegateway.ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.storegateway.ingress.hostname }}`
      ## You can:
      ##   - Use the `storegateway.ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `storegateway.ingress.selfSigned=true`
      ##
      tls: false
      ## @param storegateway.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param storegateway.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param storegateway.ingress.path Ingress path
      ##
      path: /
      ## @param storegateway.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
      ## Create an ingress object for the GRPC service. This requires an HTTP/2
      ## capable Ingress controller (eg. traefik using AWS NLB). Example annotations
      ## - ingress.kubernetes.io/protocol: h2c
      ## - service.beta.kubernetes.io/aws-load-balancer-type: nlb
      ## - service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
      ## For more information see https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/
      ## and also the documentation for your ingress controller.
      ##
      ## The options that are accepted are identical to the HTTP one listed above
      ##
      grpc:
        ## @param storegateway.ingress.grpc.enabled Enable ingress controller resource (GRPC)
        ##
        enabled: false
        ## @param storegateway.ingress.grpc.hostname Default host for the ingress resource (GRPC)
        ##
        hostname: thanos-grpc.local
        ## @param storegateway.ingress.grpc.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
        ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
        ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
        ##
        ingressClassName: ""
        ## @param storegateway.ingress.grpc.annotations Additional annotations for the Ingress resource (GRPC). To enable certificate autogeneration, place here your cert-manager annotations.
        ## For a full list of possible ingress annotations, please see
        ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
        ## Use this parameter to set the required annotations for cert-manager, see
        ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
        ##
        ## Examples:
        ## kubernetes.io/ingress.class: nginx
        ## cert-manager.io/cluster-issuer: cluster-issuer-name
        ##
        annotations: {}
        ## @param storegateway.ingress.grpc.extraHosts The list of additional hostnames to be covered with this ingress record.
        ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
        ## extraHosts:
        ## - name: thanos-grpc.local
        ##   path: /
        ##
        extraHosts: []
        ## @param storegateway.ingress.grpc.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
        ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
        ## extraTls:
        ## - hosts:
        ##     - thanos-grpc.local
        ##   secretName: thanos-grpc.local-tls
        ##
        extraTls: []
        ## @param storegateway.ingress.grpc.secrets If you're providing your own certificates, please use this to add the certificates as secrets
        ## key and certificate should start with -----BEGIN CERTIFICATE----- or
        ## -----BEGIN RSA PRIVATE KEY-----
        ##
        ## name should line up with a tlsSecret set further up
        ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
        ##
        ## It is also possible to create and manage the certificates outside of this helm chart
        ## Please see README.md for more information
        ## e.g:
        ## - name: thanos-grpc.local-tls
        ##   key:
        ##   certificate:
        ##
        secrets: []
        ## @param storegateway.ingress.grpc.extraRules Additional rules to be covered with this ingress record
        ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
        ## e.g:
        ## extraRules:
        ## - host: example.local
        ##     http:
        ##       path: /
        ##       backend:
        ##         service:
        ##           name: example-svc
        ##           port:
        ##             name: http
        ##
        extraRules: []
        ## @param storegateway.ingress.grpc.tls Enable TLS configuration for the hostname defined at `storegateway.ingress.grpc.hostname` parameter
        ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.storegateway.ingress.grpc.hostname }}`
        ## You can:
        ##   - Use the `storegateway.ingress.grpc.secrets` parameter to create this TLS secret
        ##   - Rely on cert-manager to create it by setting the corresponding annotations
        ##   - Rely on Helm to create self-signed certificates by setting `storegateway.ingress.grpc.selfSigned=true`
        ##
        tls: false
        ## @param storegateway.ingress.grpc.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
        ##
        selfSigned: false
        ## @param storegateway.ingress.grpc.apiVersion Override API Version (automatically detected if not set)
        ##
        apiVersion: ""
        ## @param storegateway.ingress.grpc.path Ingress Path
        ##
        path: /
        ## @param storegateway.ingress.grpc.pathType Ingress Path type
        ##
        pathType: ImplementationSpecific
    ## Sharded parameters
    ## @param storegateway.sharded.enabled Enable sharding for Thanos Store Gateway
    ## @param storegateway.sharded.hashPartitioning.shards Setting hashPartitioning will create multiple store statefulsets based on the number of shards specified using the hashmod of the blocks
    ## @param storegateway.sharded.hashPartitioning.extraRelabelingConfigs Setting extra relabel config
    ## e,g:
    ## extraRelabelingConfigs:
    ##   - action: keep
    ##     source_labels: ["region"]
    ##     regex: cn-zhangjiakou
    ## @param storegateway.sharded.timePartitioning [array] Setting time timePartitioning will create multiple store deployments based on the number of partitions
    ## @param storegateway.sharded.service.clusterIPs Array of cluster IPs for each Store Gateway service. Length must be the same as the number of shards
    ## e.g:
    ## clusterIPs:
    ##   - X.X.X.X
    ##   - Y.Y.Y.Y
    ## @param storegateway.sharded.service.loadBalancerIPs Array of load balancer IPs for each Store Gateway service. Length must be the same as the number of shards
    ## e.g:
    ## loadBalancerIPs:
    ##   - X.X.X.X
    ##   - Y.Y.Y.Y
    ## @param storegateway.sharded.service.http.nodePorts Array of http node ports used for Store Gateway service. Length must be the same as the number of shards
    ## e.g:
    ## nodePorts:
    ##   - 30001
    ##   - 30002
    ## @param storegateway.sharded.service.grpc.nodePorts Array of grpc node ports used for Store Gateway service. Length must be the same as the number of shards
    ## e.g:
    ## nodePorts:
    ##   - 30011
    ##   - 30012
    ##
    sharded:
      enabled: false
      hashPartitioning:
        shards: ""
        extraRelabelingConfigs: []
      timePartitioning:
        - min: ""
          max: ""
      service:
        clusterIPs: []
        loadBalancerIPs: []
        http:
          nodePorts: []
        grpc:
          nodePorts: []
  ## @section Thanos Ruler parameters
  ruler:
    ## @param ruler.enabled Enable/disable Thanos Ruler component
    ##
    enabled: false
    ## @param ruler.logLevel Thanos Ruler log level
    ##
    logLevel: info
    ## @param ruler.logFormat Thanos Ruler log format
    ##
    logFormat: logfmt
    ## @param ruler.replicaLabel Label to treat as a replica indicator along which data is de-duplicated
    ##
    replicaLabel: replica
    ## @param ruler.dnsDiscovery.enabled Dynamically configure Query APIs using DNS discovery
    ##
    dnsDiscovery:
      enabled: true
    ## @param ruler.queryURL Thanos query/query-frontend URL to link in Ruler UI.
    ##
    queryURL: ""
    ## @param ruler.alertmanagers Alert managers URLs array
    ## NOTE: This is only used when ruler.alertmanagersConfig is not set
    ##
    alertmanagers: []
    ## @param ruler.alertmanagersConfig Alert managers configuration
    ## NOTE: This is only used when ruler.alertmanagers is not set
    ## ref: https://thanos.io/tip/components/rule.md/#alertmanager
    ## e.g:
    ## alertmanagersConfig:
    ##   alertmanagers:
    ##     - http_config:
    ##         basic_auth:
    ##           username: some_user
    ##           password: some_pass
    ##       static_configs:
    ##         - alertmanager.thanos.io
    ##       scheme: http
    ##       timeout: 10s
    ##       api_version: v2
    ##
    alertmanagersConfig: ""
    ## @param ruler.evalInterval The default evaluation interval to use
    ##
    evalInterval: 1m
    ## @param ruler.clusterName Used to set the 'ruler_cluster' label
    ##
    clusterName: ""
    ## @param ruler.config Ruler configuration
    ## Specify content for ruler.yml
    ##
    config: ""
    ## @param ruler.dataPath Path to the data directory
    ##
    ## e.g. /data
    dataPath: ""
    ## @param ruler.existingConfigmap Name of existing ConfigMap with Ruler configuration
    ## NOTE: This will override ruler.config
    ##
    existingConfigmap: ""
    ## @param ruler.extraEnvVars Extra environment variables for Thanos Ruler container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param ruler.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Ruler nodes
    ##
    extraEnvVarsCM: ""
    ## @param ruler.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Ruler nodes
    ##
    extraEnvVarsSecret: ""
    ## @param ruler.extraFlags Extra Flags to passed to Thanos Ruler
    ##
    extraFlags: []
    ## @param ruler.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param ruler.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param ruler.replicaCount Number of Thanos Ruler replicas to deploy
    ##
    replicaCount: 1
    ## @param ruler.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param ruler.updateStrategy.type Update strategy type for Thanos Ruler replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param ruler.podManagementPolicy Statefulset Pod Management Policy Type
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: OrderedReady
    ## @param ruler.containerPorts.http HTTP container port
    ## @param ruler.containerPorts.grpc GRPC container port
    ##
    containerPorts:
      http: 10902
      grpc: 10901
    ## K8s Pod Security Context for Thanos Ruler pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param ruler.podSecurityContext.enabled Enable security context for the Thanos Ruler pods
    ## @param ruler.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param ruler.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param ruler.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param ruler.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Ruler pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Ruler containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param ruler.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param ruler.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param ruler.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param ruler.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param ruler.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param ruler.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param ruler.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param ruler.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param ruler.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param ruler.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Ruler containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param ruler.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if ruler.resources is set (ruler.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param ruler.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Ruler containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param ruler.livenessProbe.enabled Enable livenessProbe on Thanos Ruler containers
    ## @param ruler.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param ruler.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param ruler.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param ruler.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param ruler.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param ruler.readinessProbe.enabled Enable readinessProbe on Thanos Ruler containers
    ## @param ruler.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param ruler.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param ruler.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param ruler.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param ruler.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param ruler.startupProbe.enabled Enable startupProbe on Thanos Ruler containers
    ## @param ruler.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param ruler.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param ruler.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param ruler.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param ruler.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param ruler.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param ruler.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param ruler.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param ruler.initContainers Add additional init containers to the Thanos Ruler pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param ruler.sidecars Extra containers running as sidecars to Thanos Ruler pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param ruler.extraVolumes Extra volumes to add to Thanos Ruler
    ##
    extraVolumes: []
    ## @param ruler.extraVolumeMounts Extra volume mounts to add to the ruler container
    ##
    extraVolumeMounts: []
    ## @param ruler.podAffinityPreset Thanos Ruler pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param ruler.podAntiAffinityPreset Thanos Ruler pod anti-affinity preset. Ignored if `ruler.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Ruler node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param ruler.nodeAffinityPreset.type Thanos Ruler node affinity preset type. Ignored if `ruler.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param ruler.nodeAffinityPreset.key Thanos Ruler node label key to match. Ignored if `ruler.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param ruler.nodeAffinityPreset.values Thanos Ruler node label values to match. Ignored if `ruler.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param ruler.affinity Thanos Ruler affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: ruler.podAffinityPreset, ruler.podAntiAffinityPreset, and ruler.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param ruler.nodeSelector Thanos Ruler node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param ruler.tolerations Thanos Ruler tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param ruler.podLabels Thanos Ruler pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param ruler.podAnnotations Annotations for Thanos Ruler pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param ruler.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param ruler.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param ruler.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param ruler.lifecycleHooks for the Thanos Ruler container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param ruler.priorityClassName Thanos Ruler priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param ruler.schedulerName Name of the k8s scheduler (other than default) for Thanos Ruler pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param ruler.topologySpreadConstraints Topology Spread Constraints for Thanos Ruler pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param ruler.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param ruler.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param ruler.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param ruler.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param ruler.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param ruler.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param ruler.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param ruler.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param ruler.service.ports.http Thanos Ruler service HTTP port
      ## @param ruler.service.ports.grpc Thanos Ruler service GRPC port
      ##
      ports:
        http: 9090
        grpc: 10901
      ## @param ruler.service.nodePorts.http Specify the Thanos Ruler HTTP nodePort value for the LoadBalancer and NodePort service types
      ## @param ruler.service.nodePorts.grpc Specify the Thanos Ruler GRPC nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param ruler.service.clusterIP Thanos Ruler service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param ruler.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## Set the LoadBalancer service type to internal only
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param ruler.service.loadBalancerSourceRanges Address that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param ruler.service.externalTrafficPolicy Thanos Ruler service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param ruler.service.labels Extra labels for Thanos Ruler service
      ##
      labels: {}
      ## @param ruler.service.annotations Annotations for Thanos Ruler service
      ##
      annotations: {}
      ## @param ruler.service.extraPorts Extra ports to expose in the Thanos Ruler service
      ##
      extraPorts: []
      ## @param ruler.service.labelSelectorsOverride Selector for Thanos Query service
      ##
      labelSelectorsOverride: {}
      ## @param ruler.service.additionalHeadless Additional Headless service
      ##
      additionalHeadless: false
      ## Headless service properties
      ##
      headless:
        ## @param ruler.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
    ## Persistence parameters
    ##
    persistence:
      ## @param ruler.persistence.enabled Enable data persistence using PVC(s) on Thanos Ruler pods
      ##
      enabled: true
      ## @param ruler.persistence.storageClass Specify the `storageClass` used to provision the volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ## set, choosing the default provisioner.
      ##
      storageClass: ""
      ## @param ruler.persistence.accessModes PVC Access Modes for data volume
      ##
      accessModes:
        - ReadWriteOnce
      ## @param ruler.persistence.size PVC Storage Request for data volume
      ##
      size: 8Gi
      ## @param ruler.persistence.annotations Annotations for the PVC
      ##
      annotations: {}
      ## @param ruler.persistence.existingClaim Name of an existing PVC to use
      ## If defined, PVC must be created manually before volume will be bound
      ##
      existingClaim: ""
    ## Persistent Volume Claim Retention Policy
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
    ##
    persistentVolumeClaimRetentionPolicy:
      ## @param ruler.persistentVolumeClaimRetentionPolicy.enabled Enable Persistent volume retention policy for Thanos Ruler Statefulset
      ##
      enabled: false
      ## @param ruler.persistentVolumeClaimRetentionPolicy.whenScaled Volume retention behavior when the replica count of the StatefulSet is reduced
      ##
      whenScaled: Retain
      ## @param ruler.persistentVolumeClaimRetentionPolicy.whenDeleted Volume retention behavior that applies when the StatefulSet is deleted
      ##
      whenDeleted: Retain
    ## @param ruler.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the sts
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param ruler.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param ruler.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param ruler.serviceAccount.annotations Annotations for Thanos Ruler Service Account
    ## @param ruler.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## Thanos Ruler Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param ruler.autoscaling.enabled Enable autoscaling for Thanos Ruler
    ## @param ruler.autoscaling.minReplicas Minimum number of Thanos Ruler replicas
    ## @param ruler.autoscaling.maxReplicas Maximum number of Thanos Ruler replicas
    ## @param ruler.autoscaling.targetCPU Target CPU utilization percentage
    ## @param ruler.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
    ## Thanos Ruler Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param ruler.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Ruler
    ## @param ruler.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param ruler.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Configure the ingress resource that allows you to access Thanos Ruler
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param ruler.ingress.enabled Enable ingress controller resource
      ##
      enabled: false
      ## @param ruler.ingress.hostname Default host for the ingress resource
      ##
      hostname: thanos-ruler.local
      ## @param ruler.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param ruler.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param ruler.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##
      extraHosts: []
      ## @param ruler.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos.local
      ##   secretName: thanos.local-tls
      ##
      extraTls: []
      ## @param ruler.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param ruler.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param ruler.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param ruler.ingress.path Ingress path
      ##
      path: /
      ## @param ruler.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
  ## @section Thanos Receive parameters
  receive:
    ## @param receive.enabled Enable/disable Thanos Receive component
    ##
    enabled: false
    ## @param receive.mode Mode to run receiver in. Valid options are "standalone" or "dual-mode"
    ## ref: https://github.com/thanos-io/thanos/blob/release-0.22/docs/proposals-accepted/202012-receive-split.md
    ## Enables running the Thanos Receiver in dual mode. Setting this to "dual-mode" will create a deployment for
    ## the stateless thanos distributor.
    mode: standalone
    ## @param receive.logLevel Thanos Receive log level
    ##
    logLevel: info
    ## @param receive.logFormat Thanos Receive log format
    ##
    logFormat: logfmt
    ## @param receive.tsdbRetention Thanos Receive TSDB retention period
    ##
    tsdbRetention: 15d
    ## @param receive.replicationFactor Thanos Receive replication-factor
    ##
    replicationFactor: 1
    ## @param receive.config Receive Hashring configuration
    ## Note: json formatted string and yaml allowed.
    ## e.g:
    ## config:
    ##   - endpoints:
    ##     - "127.0.0.1:10901"
    ##
    config: []
    ## @param receive.tsdbPath Thanos Receive path to the time series database
    ##
    ## e.g.: /var/thanos/receive
    tsdbPath: ""
    ## @param receive.existingConfigmap Name of existing ConfigMap with Thanos Receive Hashring configuration
    ## NOTE: This will override receive.config
    ##
    existingConfigmap: ""
    ## @param receive.replicaLabel Label to treat as a replica indicator along which data is de-duplicated
    ##
    replicaLabel: replica
    ## Thanos Receive parameters
    ## ref: https://github.com/thanos-io/thanos/blob/master/docs/components/receive.md#flags
    ##
    grpc:
      ## GRPC server side
      ##
      server:
        ## TLS configuration
        ## @param receive.grpc.server.tls.enabled Enable TLS encryption in the GRPC server
        ## @param receive.grpc.server.tls.autoGenerated Create self-signed TLS certificates. Currently only supports PEM certificates
        ## @param receive.grpc.server.tls.cert TLS Certificate for GRPC server - ignored if existingSecret is provided
        ## @param receive.grpc.server.tls.key TLS Key for GRPC server - ignored if existingSecret is provided
        ## @param receive.grpc.server.tls.ca TLS CA to verify clients against - ignored if existingSecret is provided
        ## @param receive.grpc.server.tls.clientAuthEnabled Enable TLS client verification against provided CA
        ## @param receive.grpc.server.tls.existingSecret Existing secret containing your own TLS certificates
        ## e.g:
        ## existingSecret:
        ##   name: foo
        ##   keyMapping:
        ##     ca-cert: ca.pem
        ##     tls-cert: cert.pem
        ##     tls-key: key.pem
        ##
        tls:
          enabled: false
          autoGenerated: false
          cert: ""
          key: ""
          ca: ""
          clientAuthEnabled: true
          existingSecret: {}
    ## @param receive.extraEnvVars Extra environment variables for Thanos Receive container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param receive.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Receive nodes
    ##
    extraEnvVarsCM: ""
    ## @param receive.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Receive nodes
    ##
    extraEnvVarsSecret: ""
    ## @param receive.extraFlags Extra Flags to passed to Thanos Receive
    ##
    extraFlags: []
    ## @param receive.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param receive.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param receive.replicaCount Number of Thanos Receive replicas to deploy
    ##
    replicaCount: 1
    ## @param receive.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param receive.updateStrategy.type Update strategy type for Thanos Receive replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## @param receive.podManagementPolicy
    ## @param receive.podManagementPolicy Statefulset Pod management policy: OrderedReady (default) or Parallel
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: OrderedReady
    ## @param receive.minReadySeconds How many seconds a pod needs to be ready before killing the next, during update
    ##
    minReadySeconds: 0
    ## @param receive.containerPorts.http HTTP container port
    ## @param receive.containerPorts.grpc GRPC container port
    ## @param receive.containerPorts.remote remote-write container port
    ##
    containerPorts:
      http: 10902
      grpc: 10901
      remote: 19291
    ## K8s Pod Security Context for Thanos Receive pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param receive.podSecurityContext.enabled Enable security context for the Thanos Receive pods
    ## @param receive.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param receive.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param receive.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param receive.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Receive pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Receive containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param receive.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param receive.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param receive.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param receive.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param receive.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param receive.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param receive.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param receive.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param receive.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param receive.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Receive containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param receive.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if receive.resources is set (receive.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param receive.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Receive containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param receive.livenessProbe.enabled Enable livenessProbe on Thanos Receive containers
    ## @param receive.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param receive.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param receive.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param receive.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param receive.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param receive.readinessProbe.enabled Enable readinessProbe on Thanos Receive containers
    ## @param receive.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param receive.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param receive.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param receive.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param receive.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param receive.startupProbe.enabled Enable startupProbe on Thanos Receive containers
    ## @param receive.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param receive.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param receive.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param receive.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param receive.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param receive.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param receive.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param receive.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param receive.initContainers Add additional init containers to the Thanos Receive pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param receive.sidecars Extra containers running as sidecars to Thanos Receive pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param receive.extraVolumes Extra volumes to add to Thanos Receive
    ##
    extraVolumes: []
    ## @param receive.extraVolumeMounts Extra volume mounts to add to the receive container
    ##
    extraVolumeMounts: []
    ## @param receive.podAffinityPreset Thanos Receive pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ## Allowed values: soft, hard
    ##
    podAffinityPreset: ""
    ## @param receive.podAntiAffinityPreset Thanos Receive pod anti-affinity preset. Ignored if `ruler.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Receive node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param receive.nodeAffinityPreset.type Thanos Receive node affinity preset type. Ignored if `receive.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param receive.nodeAffinityPreset.key Thanos Receive node label key to match. Ignored if `receive.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param receive.nodeAffinityPreset.values Thanos Receive node label values to match. Ignored if `receive.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param receive.affinity Thanos Receive affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: receive.podAffinityPreset, receive.podAntiAffinityPreset, and receive.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param receive.nodeSelector Thanos Receive node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param receive.tolerations Thanos Receive tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param receive.statefulsetLabels Thanos Receive statefulset labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    statefulsetLabels: {}
    ## @param receive.podLabels Thanos Receive pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param receive.podAnnotations Annotations for Thanos Receive pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param receive.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param receive.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param receive.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param receive.terminationGracePeriodSeconds for the Thanos Receive containers(s) to extend the grace period
    ##
    terminationGracePeriodSeconds: ""
    ## @param receive.lifecycleHooks for the Thanos Receive container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param receive.priorityClassName Thanos Receive priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param receive.schedulerName Name of the k8s scheduler (other than default) for Thanos Receive pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param receive.topologySpreadConstraints Topology Spread Constraints for Thanos Receive pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param receive.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param receive.networkPolicy.allowExternal Don't require client label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## client label will have network access to the ports the application is listening
      ## on. When true, the app will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param receive.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param receive.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param receive.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param receive.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
      ## @param receive.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Service parameters
    ##
    service:
      ## @param receive.service.type Kubernetes service type
      ##
      type: ClusterIP
      ## @param receive.service.ports.http Thanos Ruler service HTTP port
      ## @param receive.service.ports.grpc Thanos Ruler service GRPC port
      ## @param receive.service.ports.remote Thanos Ruler service remote port
      ##
      ports:
        http: 10902
        grpc: 10901
        remote: 19291
      ## @param receive.service.nodePorts.http Specify the Thanos Ruler HTTP nodePort value for the LoadBalancer and NodePort service types
      ## @param receive.service.nodePorts.grpc Specify the Thanos Ruler GRPC nodePort value for the LoadBalancer and NodePort service types
      ## @param receive.service.nodePorts.remote Specify the Thanos Ruler remote nodePort value for the LoadBalancer and NodePort service types
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
      ##
      nodePorts:
        http: ""
        grpc: ""
        remote: ""
      ## @param receive.service.clusterIP Thanos Ruler service clusterIP IP
      ## e.g:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param receive.service.loadBalancerIP Load balancer IP if service type is `LoadBalancer`
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      ##
      loadBalancerIP: ""
      ## @param receive.service.loadBalancerSourceRanges Addresses that are allowed when service is LoadBalancer
      ## https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ## - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param receive.service.externalTrafficPolicy Thanos Ruler service externalTrafficPolicy
      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster
      ## @param receive.service.labels Extra labels for Thanos Receive service
      ##
      labels: {}
      ## @param receive.service.annotations Annotations for Thanos Receive service
      ##
      annotations: {}
      ## @param receive.service.extraPorts Extra ports to expose in the Thanos Receive service
      ##
      extraPorts: []
      ## @param receive.service.labelSelectorsOverride Selector for Thanos receive service
      ##
      labelSelectorsOverride: {}
      ## @param receive.service.additionalHeadless Additional Headless service
      ##
      additionalHeadless: false
      ## Headless service properties
      ##
      headless:
        ## @param receive.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
    ## @param receive.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the sts
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param receive.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param receive.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param receive.serviceAccount.annotations Annotations for Thanos Receive Service Account
    ## @param receive.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## Thanos Receive Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param receive.autoscaling.enabled Enable autoscaling for Thanos Receive
    ## @param receive.autoscaling.minReplicas Minimum number of Thanos Receive replicas
    ## @param receive.autoscaling.maxReplicas Maximum number of Thanos Receive replicas
    ## @param receive.autoscaling.targetCPU Target CPU utilization percentage
    ## @param receive.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
    ## Thanos Receive Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param receive.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Receive
    ## @param receive.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param receive.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Persistence parameters
    ##
    persistence:
      ## @param receive.persistence.enabled Enable data persistence using PVC(s) on Thanos Receive pods
      ##
      enabled: true
      ## @param receive.persistence.storageClass Specify the `storageClass` used to provision the volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ## set, choosing the default provisioner.
      ##
      storageClass: ""
      ## @param receive.persistence.accessModes PVC Access Modes for data volume
      ##
      accessModes:
        - ReadWriteOnce
      ## @param receive.persistence.size PVC Storage Request for data volume
      ##
      size: 8Gi
      ## @param receive.persistence.labels Labels for the PVC
      ##
      labels: {}
      ## @param receive.persistence.annotations Annotations for the PVC
      ##
      annotations: {}
      ## @param receive.persistence.existingClaim Name of an existing PVC to use
      ## If defined, PVC must be created manually before volume will be bound
      ##
      existingClaim: ""
    ## Persistent Volume Claim Retention Policy
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
    ##
    persistentVolumeClaimRetentionPolicy:
      ## @param receive.persistentVolumeClaimRetentionPolicy.enabled Enable Persistent volume retention policy for Thanos Receive Statefulset
      ##
      enabled: false
      ## @param receive.persistentVolumeClaimRetentionPolicy.whenScaled Volume retention behavior when the replica count of the StatefulSet is reduced
      ##
      whenScaled: Retain
      ## @param receive.persistentVolumeClaimRetentionPolicy.whenDeleted Volume retention behavior that applies when the StatefulSet is deleted
      ##
      whenDeleted: Retain
    ## Configure the ingress resource that allows you to access Thanos Receive
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param receive.ingress.enabled Set to true to enable ingress record generation
      ##
      enabled: false
      ## @param receive.ingress.hostname When the ingress is enabled, a host pointing to this will be created
      ##
      hostname: thanos-receive.local
      ## @param receive.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param receive.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param receive.ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
      ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
      ## extraHosts:
      ## - name: thanos.local
      ##   path: /
      ##   pathType: ImplementationSpecific
      ##   portName: "http" # or "remote"
      ##
      extraHosts: []
      ## @param receive.ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
      ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## extraTls:
      ## - hosts:
      ##     - thanos.local
      ##   secretName: thanos.local-tls
      ##
      extraTls: []
      ## @param receive.ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
      ## key and certificate should start with -----BEGIN CERTIFICATE----- or
      ## -----BEGIN RSA PRIVATE KEY-----
      ##
      ## name should line up with a tlsSecret set further up
      ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
      ##
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## - name: thanos.local-tls
      ##   key:
      ##   certificate:
      ##
      secrets: []
      ## @param receive.ingress.extraRules Additional rules to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
      ## e.g:
      ## extraRules:
      ## - host: example.local
      ##     http:
      ##       path: /
      ##       backend:
      ##         service:
      ##           name: example-svc
      ##           port:
      ##             name: http
      ##
      extraRules: []
      ## @param receive.ingress.tls Enable TLS configuration for the hostname defined at `receive.ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.receive.ingress.hostname }}`
      ## You can:
      ##   - Use the `receive.ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `receive.ingress.selfSigned=true`
      ##
      tls: false
      ## @param receive.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param receive.ingress.apiVersion Override API Version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param receive.ingress.path Ingress Path
      ##
      path: /
      ## @param receive.ingress.pathType Ingress Path type
      ##
      pathType: ImplementationSpecific
  ## @section Thanos Receive Distributor parameters
  receiveDistributor:
    ## @param receiveDistributor.enabled Enable/disable Thanos Receive Distributor component
    ##
    enabled: false
    ## @param receiveDistributor.logLevel Thanos Receive Distributor log level
    ##
    logLevel: info
    ## @param receiveDistributor.logFormat Thanos Receive Distributor log format
    ##
    logFormat: logfmt
    ## @param receiveDistributor.replicaLabel Label to treat as a replica indicator along which data is de-duplicated
    ##
    replicaLabel: replica
    ## @param receiveDistributor.replicationFactor Thanos Receive Distributor replication-factor
    ##
    replicationFactor: 1
    ## @param receiveDistributor.extraEnvVars Extra environment variables for Thanos Receive Distributor container
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param receiveDistributor.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Thanos Receive Distributor nodes
    ##
    extraEnvVarsCM: ""
    ## @param receiveDistributor.extraEnvVarsSecret Name of existing Secret containing extra env vars for Thanos Receive Distributor nodes
    ##
    extraEnvVarsSecret: ""
    ## @param receiveDistributor.extraFlags Extra Flags to passed to Thanos Receive Distributor
    ##
    extraFlags: []
    ## @param receiveDistributor.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param receiveDistributor.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param receiveDistributor.replicaCount Number of Thanos Receive Distributor replicas to deploy
    ##
    replicaCount: 1
    ## @param receiveDistributor.revisionHistoryLimit The number of old history to retain to allow rollback
    ##
    revisionHistoryLimit: 10
    ## @param receiveDistributor.updateStrategy.type Update strategy type for Thanos Receive Distributor replicas
    ##
    updateStrategy:
      type: RollingUpdate
    ## K8s Pod Security Context for Thanos Receive Distributor pods
    ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ## @param receiveDistributor.podSecurityContext.enabled Enable security context for the Thanos Receive Distributor pods
    ## @param receiveDistributor.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param receiveDistributor.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param receiveDistributor.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param receiveDistributor.podSecurityContext.fsGroup Group ID for the filesystem used by Thanos Receive Distributor pods
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## K8s containers' Security Context for Thanos Receive Distributor containers
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param receiveDistributor.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param receiveDistributor.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param receiveDistributor.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param receiveDistributor.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param receiveDistributor.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param receiveDistributor.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param receiveDistributor.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param receiveDistributor.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param receiveDistributor.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param receiveDistributor.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## Thanos Receive Distributor containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param receiveDistributor.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if receiveDistributor.resources is set (receiveDistributor.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param receiveDistributor.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure extra options for Thanos Receive Distributor containers' liveness and readiness probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
    ## @param receiveDistributor.livenessProbe.enabled Enable livenessProbe on Thanos Receive Distributor containers
    ## @param receiveDistributor.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param receiveDistributor.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param receiveDistributor.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param receiveDistributor.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param receiveDistributor.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param receiveDistributor.readinessProbe.enabled Enable readinessProbe on Thanos Receive Distributor containers
    ## @param receiveDistributor.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param receiveDistributor.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param receiveDistributor.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param receiveDistributor.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param receiveDistributor.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      initialDelaySeconds: 30
      timeoutSeconds: 30
      periodSeconds: 10
      successThreshold: 1
      failureThreshold: 6
    ## @param receiveDistributor.startupProbe.enabled Enable startupProbe on Thanos Receive Distributor containers
    ## @param receiveDistributor.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param receiveDistributor.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param receiveDistributor.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param receiveDistributor.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param receiveDistributor.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 5
      periodSeconds: 5
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param receiveDistributor.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param receiveDistributor.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param receiveDistributor.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param receiveDistributor.terminationGracePeriodSeconds for the Thanos Receive containers(s) to extend the grace period
    ##
    terminationGracePeriodSeconds: ""
    ## @param receiveDistributor.initContainers Add additional init containers to the Thanos Receive Distributor pods
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param receiveDistributor.sidecars Extra containers running as sidecars to Thanos Receive Distributor pods
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param receiveDistributor.extraVolumes Extra volumes to add to Thanos Receive Distributor
    ##
    extraVolumes: []
    ## @param receiveDistributor.extraVolumeMounts Extra volume mounts to add to the receive distributor container
    ##
    extraVolumeMounts: []
    ## @param receiveDistributor.podAffinityPreset Thanos Receive pod affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ## Allowed values: soft, hard
    ##
    podAffinityPreset: ""
    ## @param receiveDistributor.podAntiAffinityPreset Thanos Receive pod anti-affinity preset. Ignored if `receiveDistributor.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Thanos Receive node affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param receiveDistributor.nodeAffinityPreset.type Thanos Receive node affinity preset type. Ignored if `receiveDistributor.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param receiveDistributor.nodeAffinityPreset.key Thanos Receive node label key to match. Ignored if `receiveDistributor.affinity` is set.
      ## e.g:
      ## key: "kubernetes.io/e2e-az-name"
      ##
      key: ""
      ## @param receiveDistributor.nodeAffinityPreset.values Thanos Receive node label values to match. Ignored if `receiveDistributor.affinity` is set.
      ## e.g:
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param receiveDistributor.affinity Thanos Receive Distributor affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## Note: receiveDistributor.podAffinityPreset, receiveDistributor.podAntiAffinityPreset, and receiveDistributor.nodeAffinityPreset will be ignored when it's set
    ##
    affinity: {}
    ## @param receiveDistributor.nodeSelector Thanos Receive Distributor node labels for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param receiveDistributor.tolerations Thanos Receive Distributor tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param receiveDistributor.podLabels Thanos Receive Distributor pod labels
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param receiveDistributor.podAnnotations Annotations for Thanos Receive Distributor pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param receiveDistributor.dnsConfig Deployment pod DNS config
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsConfig:
    ##   options:
    ##   - name: ndots
    ##     value: "4"
    ##   - name: single-request-reopen
    ##
    dnsConfig: {}
    ## @param receiveDistributor.dnsPolicy Deployment pod DNS policy
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
    ## E.g.
    ## dnsPolicy: ClusterFirstWithHostNet
    ##
    dnsPolicy: ""
    ## @param receiveDistributor.hostAliases Deployment pod host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param receiveDistributor.lifecycleHooks for the Thanos Receive Distributor container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param receiveDistributor.priorityClassName Thanos Receive Distributor priorityClassName
    ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
    ##
    priorityClassName: ""
    ## @param receiveDistributor.schedulerName Name of the k8s scheduler (other than default) for Thanos Receive Distributor pods
    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param receiveDistributor.topologySpreadConstraints Topology Spread Constraints for Thanos Receive Distributor pods assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param receiveDistributor.automountServiceAccountToken Enable/disable auto mounting of the service account token only for the deployment
    ##
    automountServiceAccountToken: true
    ## ServiceAccount configuration
    ## @param receiveDistributor.serviceAccount.create Specifies whether a ServiceAccount should be created
    ## @param receiveDistributor.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
    ## @param receiveDistributor.serviceAccount.annotations Annotations for Thanos Receive Distributor Service Account
    ## @param receiveDistributor.serviceAccount.automountServiceAccountToken Enable/disable auto mounting of the service account token
    ##
    serviceAccount:
      create: true
      name: ""
      annotations: {}
      automountServiceAccountToken: false
    ## Thanos Receive Distributor Autoscaling configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## @param receiveDistributor.autoscaling.enabled Enable autoscaling for Thanos Receive Distributor
    ## @param receiveDistributor.autoscaling.minReplicas Minimum number of Thanos Receive Distributor replicas
    ## @param receiveDistributor.autoscaling.maxReplicas Maximum number of Thanos Receive Distributor replicas
    ## @param receiveDistributor.autoscaling.targetCPU Target CPU utilization percentage
    ## @param receiveDistributor.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: ""
      maxReplicas: ""
      targetCPU: ""
      targetMemory: ""
    ## Thanos Receive Distributor Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param receiveDistributor.pdb.create Enable/disable a Pod Disruption Budget creation for Thanos Receive Distributor
    ## @param receiveDistributor.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param receiveDistributor.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
  ## @section Metrics parameters

  ## Prometheus metrics
  ##
  metrics:
    ## @param metrics.enabled Enable the export of Prometheus metrics
    ##
    enabled: false
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Specify if a ServiceMonitor will be deployed for Prometheus Operator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.labels Extra labels for the ServiceMonitor
      ##
      labels: {}
      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in Prometheus
      ##
      jobLabel: ""
      ## @param metrics.serviceMonitor.interval How frequently to scrape metrics
      ## e.g:
      ## interval: 10s
      ##
      interval: ""
      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## e.g:
      ## scrapeTimeout: 10s
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.metricRelabelings [array] Specify additional relabeling of metrics
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.relabelings [array] Specify general relabeling
      ##
      relabelings: []
      ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param metrics.serviceMonitor.extraParameters Any extra parameter to be added to the endpoint configured in the ServiceMonitor
      ## (e.g. tlsConfig for further customization of the HTTPS behavior)
      ## Note that the 'scheme' is automatically set to 'https' when the 'https.enabled' flag is used in this chart.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Endpoint
      ##
      extraParameters: {}
    ## PrometheusRule CRD configuration
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled If `true`, creates a Prometheus Operator PrometheusRule (also requires `metrics.enabled` to be `true`)
      ##
      enabled: false
      ## Configure prometheus rules
      ##
      default:
        ## @extra metrics.prometheusRule.default.absent_rules Enable absent_rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @extra metrics.prometheusRule.default.compaction Enable compaction rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @extra metrics.prometheusRule.default.query Enable query when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @extra metrics.prometheusRule.default.receive Enable receive rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @extra metrics.prometheusRule.default.replicate Enable replicate rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @extra metrics.prometheusRule.default.ruler Enable ruler rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @extra metrics.prometheusRule.default.sidecar Enable sidecar rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @param metrics.prometheusRule.default.sidecarJobRegex Allows the customization of the thanos-sidecar job name to use in the sidecar prometheus alerts
        sidecarJobRegex: ".*thanos-sidecar.*"
        ## @extra metrics.prometheusRule.default.store_gateway Enable store_gateway rules when metrics.prometheusRule.default.create is false (also requires `metrics.enabled` to be `true`)
        ## @param metrics.prometheusRule.default.create would create all default prometheus alerts
        ##
        create: false
        ## @extra metrics.prometheusRule.default.disabled.ThanosCompactIsDown Disable ThanosCompactIsDown rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.absent_rules is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryIsDown Disable ThanosQueryIsDown rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.absent_rules is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveIsDown Disable ThanosReceiveIsDown rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.absent_rules is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleIsDown Disable ThanosRuleIsDown rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.absent_rules is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosSidecarIsDown Disable ThanosSidecarIsDown rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.absent_rules is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosStoreIsDown Disable ThanosStoreIsDown rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.absent_rules is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosCompactMultipleRunning Disable ThanosCompactMultipleRunning rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.compaction is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosCompactHalted Disable ThanosCompactMultipleRunning rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.compaction is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosCompactHighCompactionFailures Disable ThanosCompactMultipleRunning rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.compaction is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosCompactBucketHighOperationFailures Disable ThanosCompactMultipleRunning rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.compaction is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosCompactHasNotRun Disable ThanosCompactMultipleRunning rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.compaction is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryHttpRequestQueryErrorRateHigh Disable ThanosQueryHttpRequestQueryErrorRateHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryHttpRequestQueryRangeErrorRateHigh Disable ThanosQueryHttpRequestQueryRangeErrorRateHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryGrpcServerErrorRate Disable ThanosQueryGrpcServerErrorRate rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryGrpcClientErrorRate Disable ThanosQueryGrpcClientErrorRate rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryHighDNSFailures Disable ThanosQueryHighDNSFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryInstantLatencyHigh Disable ThanosQueryInstantLatencyHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryRangeLatencyHigh Disable ThanosQueryRangeLatencyHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosQueryOverload Disable ThanosQueryOverload rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.query is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveHttpRequestErrorRateHigh Disable ThanosReceiveHttpRequestErrorRateHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveHttpRequestLatencyHigh Disable ThanosReceiveHttpRequestLatencyHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveHighReplicationFailures Disable ThanosReceiveHighReplicationFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveHighForwardRequestFailures Disable ThanosReceiveHighForwardRequestFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveHighHashringFileRefreshFailures Disable ThanosReceiveHighHashringFileRefreshFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveConfigReloadFailure Disable ThanosReceiveConfigReloadFailure rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveNoUpload Disable ThanosReceiveNoUpload rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosReceiveTrafficBelowThreshold Disable ThanosReceiveTrafficBelowThreshold rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosBucketReplicateErrorRate Disable ThanosBucketReplicateErrorRate rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosBucketReplicateRunLatency Disable ThanosBucketReplicateRunLatency rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.receive is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleQueueIsDroppingAlerts Disable ThanosRuleQueueIsDroppingAlerts rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleSenderIsFailingAlerts Disable ThanosRuleSenderIsFailingAlerts rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleHighRuleEvaluationFailures Disable ThanosRuleHighRuleEvaluationFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleHighRuleEvaluationWarnings Disable ThanosRuleHighRuleEvaluationWarnings rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleRuleEvaluationLatencyHigh Disable ThanosRuleRuleEvaluationLatencyHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleGrpcErrorRate Disable ThanosRuleGrpcErrorRate rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleConfigReloadFailure Disable ThanosRuleConfigReloadFailure rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleQueryHighDNSFailures Disable ThanosRuleQueryHighDNSFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleAlertmanagerHighDNSFailures Disable ThanosRuleAlertmanagerHighDNSFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosRuleNoEvaluationFor10Intervals Disable ThanosRuleNoEvaluationFor10Intervals rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosNoRuleEvaluations Disable ThanosNoRuleEvaluations rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.ruler is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosSidecarBucketOperationsFailed Disable ThanosSidecarBucketOperationsFailed rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.sidecar is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosSidecarNoConnectionToStartedPrometheus Disable ThanosSidecarNoConnectionToStartedPrometheus rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.sidecar is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosStoreGrpcErrorRate Disable ThanosSidecarNoConnectionToStartedPrometheus rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.store_gateway  is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosStoreSeriesGateLatencyHigh Disable ThanosStoreSeriesGateLatencyHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.store_gateway  is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosStoreBucketHighOperationFailures Disable ThanosStoreBucketHighOperationFailures rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.store_gateway  is true
        ## @extra metrics.prometheusRule.default.disabled.ThanosStoreObjstoreOperationLatencyHigh Disable ThanosStoreObjstoreOperationLatencyHigh rule when metrics.prometheusRule.default.create or metrics.prometheusRule.default.store_gateway  is true
        ## @param metrics.prometheusRule.default.disabled disable one specific prometheus alert rule
        ##
        disabled: {}
      ## @param metrics.prometheusRule.runbookUrl Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules
      ##
      runbookUrl: "https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-"
      ## @param metrics.prometheusRule.namespace Namespace in which the PrometheusRule CRD is created
      ##
      namespace: ""
      ## @param metrics.prometheusRule.additionalLabels Additional labels for the prometheusRule
      ##
      additionalLabels: {}
      ## @param metrics.prometheusRule.groups Prometheus Rule Groups for Thanos components
      ## These are just examples rules, please adapt them to your needs.
      ##   groups:
      ##     - name: Compactor
      ##       rules:
      ##         - alert: ThanosCompactMultipleRunning
      ##           annotations:
      ##             description: No more than one Thanos Compact instance should be running at once. There are {{`{{`}}$value{{`}}`}} instances running.
      ##             runbook_url: {{ .Values.metrics.prometheusRule.runbookUrl }}thanoscompactmultiplerunning
      ##             summary: Thanos Compact has multiple instances running.
      ##           expr: sum by (job) (up{job=~"{{ template "common.names.fullname" . }}-compact.*"}) > 1
      ##           for: 5m
      ##           labels:
      ##             severity: warning
      groups: []
  ## @section Volume Permissions parameters

  ## 'volumePermissions' init container parameters
  ## Changes the owner and group of the persistent volume mount point to runAsUser:fsGroup values
  ##   based on the *podSecurityContext/*containerSecurityContext parameters
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`
    ##
    enabled: false
    ## @param volumePermissions.image.registry [default: REGISTRY_NAME] Init container volume-permissions image registry
    ## @param volumePermissions.image.repository [default: REPOSITORY_NAME/os-shell] Init container volume-permissions image repository
    ## @skip volumePermissions.image.tag Init container volume-permissions image tag
    ## @param volumePermissions.image.digest Init container volume-permissions image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
    ## @param volumePermissions.image.pullSecrets Specify docker-registry secret names as an array
    ##
    image:
      registry: docker.io
      repository: bitnami/os-shell
      tag: 12-debian-12-r34
      digest: ""
      ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
  ## @section MinIO&reg; chart parameters
  ## @extra minio For full list of MinIO&reg; values configurations please refer [here](https://github.com/bitnami/charts/tree/main/bitnami/minio)
  minio:
    ## @param minio.enabled Enable/disable MinIO&reg; chart installation
    ## to be used as an objstore for Thanos
    ##
    enabled: false
    ## MinIO&reg; authentication parameters
    ##
    auth:
      ## @param minio.auth.rootUser MinIO&reg; root username
      ##
      rootUser: admin
      ## @param minio.auth.rootPassword Password for MinIO&reg; root user
      ##
      rootPassword: ""
    ## @param minio.defaultBuckets Comma, semi-colon or space separated list of MinIO&reg; buckets to create
    ##
    defaultBuckets: "thanos"
    ## MinIO&reg; containers' resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    ## @param minio.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "micro"
    ## @param minio.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}


# Grafana-Loki
grafana-loki:
  enabled: true
  # Copyright Broadcom, Inc. All Rights Reserved.
  # SPDX-License-Identifier: APACHE-2.0

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
  ##

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ## @param global.defaultStorageClass Global default StorageClass for Persistent Volume(s)
  ## @param global.storageClass DEPRECATED: use global.defaultStorageClass instead
  ##
  global:
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    defaultStorageClass: ""
    storageClass: ""
    ## Compatibility adaptations for Kubernetes platforms
    ##
    compatibility:
      ## Compatibility adaptations for Openshift
      ##
      openshift:
        ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
        ##
        adaptSecurityContext: auto
  ## @section Common parameters
  ##

  ## @param kubeVersion Override Kubernetes version
  ##
  kubeVersion: ""
  ## @param nameOverride String to partially override common.names.fullname
  ##
  nameOverride: ""
  ## @param fullnameOverride String to fully override common.names.fullname
  ##
  fullnameOverride: ""
  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations: {}
  ## @param clusterDomain Kubernetes cluster domain name
  ##
  clusterDomain: cluster.local
  ## @param extraDeploy Array of extra objects to deploy with the release
  ##
  extraDeploy: []
  ## Enable diagnostic mode in the deployments/statefulsets
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the deployments/statefulsets
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the deployments/statefulsets
    ##
    args:
      - infinity
  ## @section Common Grafana Loki Parameters
  ##
  loki:
    ## Bitnami Grafana Loki image
    ## ref: https://hub.docker.com/r/bitnami/grafana-loki/tags/
    ## @param loki.image.registry [default: REGISTRY_NAME] Grafana Loki image registry
    ## @param loki.image.repository [default: REPOSITORY_NAME/grafana-loki] Grafana Loki image repository
    ## @skip loki.image.tag Grafana Loki image tag (immutable tags are recommended)
    ## @param loki.image.digest Grafana Loki image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param loki.image.pullPolicy Grafana Loki image pull policy
    ## @param loki.image.pullSecrets Grafana Loki image pull secrets
    ##
    image:
      registry: docker.io
      repository: bitnami/grafana-loki
      tag: 3.2.0-debian-12-r1
      digest: ""
      ## Specify a imagePullPolicy
      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## @param loki.configuration [string] Loki components configuration
    ##
    configuration: |
      auth_enabled: false

      server:
        http_listen_port: {{ .Values.loki.containerPorts.http }}
        grpc_listen_port: {{ .Values.loki.containerPorts.grpc }}
      common:
        compactor_address: http://{{ include "grafana-loki.compactor.fullname" . }}:{{ .Values.compactor.service.ports.http }}

      distributor:
        ring:
          kvstore:
            store: memberlist

      memberlist:
        join_members:
          - {{ include "grafana-loki.gossip-ring.fullname" . }}

      ingester:
        lifecycler:
          ring:
            kvstore:
              store: memberlist
            replication_factor: 1
        chunk_idle_period: 30m
        chunk_block_size: 262144
        chunk_encoding: snappy
        chunk_retain_period: 1m
        wal:
          dir: {{ .Values.loki.dataDir }}/wal

      limits_config:
        retention_period: 336h
        reject_old_samples: true
        reject_old_samples_max_age: 168h
        max_cache_freshness_per_query: 10m
        split_queries_by_interval: 15m
        allow_structured_metadata: true

      schema_config:
        configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
        - from: 2024-03-12
          store: tsdb
          object_store: filesystem
          schema: v12
          index:
            period: 24h
            prefix: index_
        - from: 2024-04-23
          object_store: filesystem
          store: tsdb
          schema: v13
          index:
            prefix: index_
            period: 24h

      storage_config:
        boltdb_shipper:
          active_index_directory: {{ .Values.loki.dataDir }}/loki/index
          cache_location: {{ .Values.loki.dataDir }}/loki/cache
          cache_ttl: 168h
          {{- if .Values.indexGateway.enabled }}
          index_gateway_client:
            server_address: {{ (printf "dns:///%s:9095" (include "grafana-loki.index-gateway.fullname" .)) }}
          {{- end }}
        filesystem:
          directory: {{ .Values.loki.dataDir }}/chunks
        index_queries_cache_config:
          {{- if .Values.memcachedindexqueries.enabled }}
          memcached:
            batch_size: 100
            parallelism: 100
          memcached_client:
            consistent_hash: true
            addresses: dns+{{ include "grafana-loki.memcached-index-queries.host" . }}
            service: http
          {{- end }}
        tsdb_shipper:
          active_index_directory: {{ .Values.loki.dataDir }}/loki/tsdb-index
          cache_location: {{ .Values.loki.dataDir }}/loki/tsdb-cache
          {{- if .Values.indexGateway.enabled }}
          index_gateway_client:
            server_address: {{ (printf "dns:///%s:9095" (include "grafana-loki.index-gateway.fullname" .)) }}
          {{- end }}

      query_scheduler:
        max_outstanding_requests_per_tenant: 32768

      querier:
        max_concurrent: 16

      chunk_store_config:
        {{- if .Values.memcachedchunks.enabled }}
        chunk_cache_config:
          memcached:
            batch_size: 100
            parallelism: 100
          memcached_client:
            consistent_hash: true
            addresses: dns+{{ include "grafana-loki.memcached-chunks.host" . }}
        {{- end }}
        {{- if .Values.memcachedindexwrites.enabled }}
        write_dedupe_cache_config:
          memcached:
            batch_size: 100
            parallelism: 100
          memcached_client:
            consistent_hash: true
            addresses: dns+{{ include "grafana-loki.memcached-index-writes.host" . }}
        {{- end }}

      table_manager:
        retention_deletes_enabled: false
        retention_period: 0s

      query_range:
        align_queries_with_step: true
        max_retries: 5
        cache_results: true
        results_cache:
          cache:
            {{- if .Values.memcachedfrontend.enabled }}
            memcached_client:
              consistent_hash: true
              addresses: dns+{{ include "grafana-loki.memcached-frontend.host" . }}
              max_idle_conns: 16
              timeout: 500ms
              update_interval: 1m
            {{- else }}
            embedded-cache:
              enabled: true
              max_size_items: 1024
              validity: 24h
            {{- end }}
      {{- if not .Values.queryScheduler.enabled }}
      frontend_worker:
        frontend_address: {{ include "grafana-loki.query-frontend.fullname" . }}:{{ .Values.queryFrontend.service.ports.grpc }}
      {{- end }}

      frontend:
        log_queries_longer_than: 5s
        compress_responses: true
        tail_proxy_url: http://{{ include "grafana-loki.querier.fullname" . }}:{{ .Values.querier.service.ports.http }}

      compactor:
        working_directory: {{ .Values.loki.dataDir }}/loki/retention
        compaction_interval: 10m
        retention_enabled: true
        retention_delete_delay: 2h
        retention_delete_worker_count: 150
        delete_request_store: filesystem

      ruler:
        storage:
          type: local
          local:
            directory: {{ .Values.loki.dataDir }}/conf/rules
        ring:
          kvstore:
            store: memberlist
        rule_path: /tmp/loki/scratch
        alertmanager_url: https://alertmanager.xx
        external_url: https://alertmanager.xx
    ## @param loki.overrideConfiguration [object] Loki components configuration override. Values defined here takes precedence over loki.configuration
    ## e.g:
    ## overrideConfiguration:
    ##   auth_enabled: true
    ##
    overrideConfiguration: {}
    ## @param loki.existingConfigmap Name of a ConfigMap with the Loki configuration
    ##
    existingConfigmap: ""
    ## @param loki.dataDir path to the Loki data directory
    ##
    dataDir: "/bitnami/grafana-loki"
    ## @param loki.containerPorts.http Loki components web container port
    ## @param loki.containerPorts.grpc Loki components GRPC container port
    ## @param loki.containerPorts.gossipRing Loki components Gossip Ring container port
    ##
    containerPorts:
      http: 3100
      grpc: 9095
      gossipRing: 7946
    ## Gossip Ring parameters
    ##
    gossipRing:
      ## Gossip Ring service parameters
      ##
      service:
        ## @param loki.gossipRing.service.ports.http Gossip Ring HTTP headless service port
        ##
        ports:
          http: 7946
        ## @param loki.gossipRing.service.annotations Additional custom annotations for Gossip Ring headless service
        ##
        annotations: {}
  ## @section Compactor Deployment Parameters
  ##
  compactor:
    ## @param compactor.enabled Enable Compactor deployment
    ##
    enabled: true
    ## @param compactor.extraEnvVars Array with extra environment variables to add to compactor nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param compactor.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for compactor nodes
    ##
    extraEnvVarsCM: ""
    ## @param compactor.extraEnvVarsSecret Name of existing Secret containing extra env vars for compactor nodes
    ##
    extraEnvVarsSecret: ""
    ## @param compactor.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param compactor.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param compactor.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param compactor.replicaCount Number of Compactor replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for Compactor containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param compactor.livenessProbe.enabled Enable livenessProbe on Compactor nodes
    ## @param compactor.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param compactor.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param compactor.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param compactor.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param compactor.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 60
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param compactor.readinessProbe.enabled Enable readinessProbe on Compactor nodes
    ## @param compactor.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param compactor.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param compactor.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param compactor.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param compactor.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 60
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param compactor.startupProbe.enabled Enable startupProbe on Compactor containers
    ## @param compactor.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param compactor.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param compactor.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param compactor.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param compactor.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param compactor.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param compactor.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param compactor.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## compactor resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param compactor.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if compactor.resources is set (compactor.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param compactor.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param compactor.podSecurityContext.enabled Enabled Compactor pods' Security Context
    ## @param compactor.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param compactor.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param compactor.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param compactor.podSecurityContext.fsGroup Set Compactor pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param compactor.containerSecurityContext.enabled Enable containers' Security Context
    ## @param compactor.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param compactor.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param compactor.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param compactor.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param compactor.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param compactor.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param compactor.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param compactor.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param compactor.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param compactor.lifecycleHooks for the compactor container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param compactor.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param compactor.hostAliases compactor pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param compactor.podLabels Extra labels for compactor pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param compactor.podAnnotations Annotations for compactor pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param compactor.podAffinityPreset Pod affinity preset. Ignored if `compactor.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param compactor.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `compactor.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node compactor.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param compactor.nodeAffinityPreset.type Node affinity preset type. Ignored if `compactor.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param compactor.nodeAffinityPreset.key Node label key to match. Ignored if `compactor.affinity` is set
      ##
      key: ""
      ## @param compactor.nodeAffinityPreset.values Node label values to match. Ignored if `compactor.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param compactor.affinity Affinity for Compactor pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `compactor.podAffinityPreset`, `compactor.podAntiAffinityPreset`, and `compactor.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param compactor.nodeSelector Node labels for Compactor pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param compactor.tolerations Tolerations for Compactor pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param compactor.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param compactor.priorityClassName Compactor pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param compactor.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param compactor.updateStrategy.type Compactor statefulset strategy type
    ## @param compactor.updateStrategy.rollingUpdate [object,nullable] Compactor statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param compactor.extraVolumes Optionally specify extra list of additional volumes for the Compactor pod(s)
    ##
    extraVolumes: []
    ## @param compactor.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Compactor container(s)
    ##
    extraVolumeMounts: []
    ## @param compactor.sidecars Add additional sidecar containers to the Compactor pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param compactor.initContainers Add additional init containers to the Compactor pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param compactor.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param compactor.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param compactor.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `compactor.pdb.minAvailable` and `compactor.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param compactor.persistence.enabled Enable persistence in Compactor instances
      ##
      enabled: true
      ## @param compactor.persistence.existingClaim Name of an existing PVC to use
      ##
      existingClaim: ""
      ## @param compactor.persistence.storageClass PVC Storage Class for Memcached data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ""
      ## @param compactor.persistence.accessModes PVC Access modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param compactor.persistence.size PVC Storage Request for Memcached data volume
      ##
      size: 8Gi
      ## @param compactor.persistence.annotations Additional PVC annotations
      ##
      annotations: {}
      ## @param compactor.persistence.selector Selector to match an existing Persistent Volume for Compactor's data PVC
      ## If set, the PVC can't have a PV dynamically provisioned for it
      ## E.g.
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
      ## @param compactor.persistence.dataSource PVC data source
      ##
      dataSource: {}
    ## @param compactor.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## @section Compactor Traffic Exposure Parameters
    ##

    ## compactor service parameters
    ##
    service:
      ## @param compactor.service.type Compactor service type
      ##
      type: ClusterIP
      ## @param compactor.service.ports.http Compactor HTTP service port
      ## @param compactor.service.ports.grpc Compactor gRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param compactor.service.nodePorts.http Node port for HTTP
      ##
      nodePorts:
        http: ""
      ## @param compactor.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param compactor.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param compactor.service.clusterIP Compactor service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param compactor.service.loadBalancerIP Compactor service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param compactor.service.loadBalancerSourceRanges Compactor service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param compactor.service.externalTrafficPolicy Compactor service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param compactor.service.annotations Additional custom annotations for Compactor service
      ##
      annotations: {}
      ## @param compactor.service.extraPorts Extra ports to expose in the Compactor service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param compactor.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param compactor.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param compactor.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param compactor.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `compactor.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param compactor.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param compactor.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param compactor.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `compactor.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param compactor.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `compactor.networkPolicy.allowExternal` is true.
      ## @param compactor.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `compactor.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Gateway Deployment Parameters
  ##
  gateway:
    ## @param gateway.enabled Enable Gateway deployment
    ##
    enabled: true
    ## Bitnami Nginx image
    ## ref: https://hub.docker.com/r/bitnami/grafana-nginx/tags/
    ## @param gateway.image.registry [default: REGISTRY_NAME] Nginx image registry
    ## @param gateway.image.repository [default: REPOSITORY_NAME/nginx] Nginx image repository
    ## @skip gateway.image.tag Nginx image tag (immutable tags are recommended)
    ## @param gateway.image.digest Nginx image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param gateway.image.pullPolicy Nginx image pull policy
    ## @param gateway.image.pullSecrets Nginx image pull secrets
    ## @param gateway.image.debug Enable debugging in the initialization process
    ##
    image:
      registry: docker.io
      repository: bitnami/nginx
      tag: 1.27.1-debian-12-r5
      digest: ""
      ## Specify a imagePullPolicy
      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
      debug: false
    ## @param gateway.extraEnvVars Array with extra environment variables to add to gateway nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param gateway.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for gateway nodes
    ##
    extraEnvVarsCM: ""
    ## @param gateway.extraEnvVarsSecret Name of existing Secret containing extra env vars for gateway nodes
    ##
    extraEnvVarsSecret: ""
    ## @param gateway.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param gateway.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param gateway.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param gateway.verboseLogging Show the gateway access_log
    ##
    verboseLogging: false
    ## @param gateway.replicaCount Number of Gateway replicas to deploy
    ##
    replicaCount: 1
    ## @param gateway.auth.enabled Enable basic auth
    ## @param gateway.auth.username Basic auth username
    ## @param gateway.auth.password Basic auth password
    ## @param gateway.auth.existingSecret Name of a secret containing the Basic auth password
    ##
    auth:
      enabled: false
      username: "user"
      password: ""
      existingSecret: ""
    ## Configure extra options for Gateway containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param gateway.livenessProbe.enabled Enable livenessProbe on Gateway nodes
    ## @param gateway.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param gateway.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param gateway.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param gateway.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param gateway.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param gateway.readinessProbe.enabled Enable readinessProbe on Gateway nodes
    ## @param gateway.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param gateway.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param gateway.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param gateway.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param gateway.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param gateway.startupProbe.enabled Enable startupProbe on Gateway containers
    ## @param gateway.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param gateway.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param gateway.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param gateway.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param gateway.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param gateway.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param gateway.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param gateway.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param gateway.containerPorts.http Gateway HTTP port
    ##
    containerPorts:
      http: 8080
    ## gateway resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param gateway.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if gateway.resources is set (gateway.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param gateway.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param gateway.podSecurityContext.enabled Enabled Gateway pods' Security Context
    ## @param gateway.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param gateway.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param gateway.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param gateway.podSecurityContext.fsGroup Set Gateway pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param gateway.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param gateway.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param gateway.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param gateway.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param gateway.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param gateway.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param gateway.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param gateway.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param gateway.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param gateway.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param gateway.lifecycleHooks for the gateway container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param gateway.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param gateway.hostAliases gateway pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param gateway.podLabels Extra labels for gateway pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param gateway.podAnnotations Annotations for gateway pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param gateway.podAffinityPreset Pod affinity preset. Ignored if `gateway.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param gateway.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `gateway.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node gateway.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param gateway.nodeAffinityPreset.type Node affinity preset type. Ignored if `gateway.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param gateway.nodeAffinityPreset.key Node label key to match. Ignored if `gateway.affinity` is set
      ##
      key: ""
      ## @param gateway.nodeAffinityPreset.values Node label values to match. Ignored if `gateway.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param gateway.affinity Affinity for Gateway pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `gateway.podAffinityPreset`, `gateway.podAntiAffinityPreset`, and `gateway.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param gateway.nodeSelector Node labels for Gateway pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param gateway.tolerations Tolerations for Gateway pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param gateway.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param gateway.priorityClassName Gateway pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param gateway.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param gateway.updateStrategy.type Gateway statefulset strategy type
    ## @param gateway.updateStrategy.rollingUpdate [object,nullable] Gateway statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param gateway.extraVolumes Optionally specify extra list of additional volumes for the Gateway pod(s)
    ##
    extraVolumes: []
    ## @param gateway.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Gateway container(s)
    ##
    extraVolumeMounts: []
    ## @param gateway.sidecars Add additional sidecar containers to the Gateway pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param gateway.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param gateway.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param gateway.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `gateway.pdb.minAvailable` and `gateway.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @param gateway.initContainers Add additional init containers to the Gateway pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param gateway.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## @section Gateway Traffic Exposure Parameters
    ##

    ## gateway service parameters
    ##
    service:
      ## @param gateway.service.type Gateway service type
      ##
      type: ClusterIP
      ## @param gateway.service.ports.http Gateway HTTP service port
      ##
      ports:
        http: 80
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param gateway.service.nodePorts.http Node port for HTTP
      ##
      nodePorts:
        http: ""
      ## @param gateway.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param gateway.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param gateway.service.clusterIP Gateway service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param gateway.service.loadBalancerIP Gateway service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param gateway.service.loadBalancerSourceRanges Gateway service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param gateway.service.externalTrafficPolicy Gateway service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param gateway.service.annotations Additional custom annotations for Gateway service
      ##
      annotations: {}
      ## @param gateway.service.extraPorts Extra ports to expose in the Gateway service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param gateway.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param gateway.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param gateway.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param gateway.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `gateway.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param gateway.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param gateway.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param gateway.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `gateway.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param gateway.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `gateway.networkPolicy.allowExternal` is true.
      ## @param gateway.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `gateway.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Configure the ingress resource that allows you to access the Loki Gateway installation
    ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
    ##
    ingress:
      ## @param gateway.ingress.enabled Enable ingress record generation for Loki Gateway
      ##
      enabled: false
      ## @param gateway.ingress.pathType Ingress path type
      ##
      pathType: ImplementationSpecific
      ## @param gateway.ingress.apiVersion Force Ingress API version (automatically detected if not set)
      ##
      apiVersion: ""
      ## @param gateway.ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
      ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
      ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
      ##
      ingressClassName: ""
      ## @param gateway.ingress.hostname Default host for the ingress record
      ##
      hostname: grafana-loki.local
      ## @param gateway.ingress.path Default path for the ingress record
      ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
      ##
      path: /
      ## @param gateway.ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
      ## For a full list of possible ingress annotations, please see
      ## ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md
      ## Use this parameter to set the required annotations for cert-manager, see
      ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
      ##
      ## e.g:
      ## annotations:
      ##   kubernetes.io/ingress.class: nginx
      ##   cert-manager.io/cluster-issuer: cluster-issuer-name
      ##
      annotations: {}
      ## @param gateway.ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
      ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
      ## You can:
      ##   - Use the `ingress.secrets` parameter to create this TLS secret
      ##   - Rely on cert-manager to create it by setting the corresponding annotations
      ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
      ##
      tls: false
      ## @param gateway.ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
      ##
      selfSigned: false
      ## @param gateway.ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record
      ## e.g:
      ## extraHosts:
      ##   - name: Loki Gateway.local
      ##     path: /
      ##
      extraHosts: []
      ## @param gateway.ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
      ## e.g:
      ## extraPaths:
      ## - path: /*
      ##   backend:
      ##     serviceName: ssl-redirect
      ##     servicePort: use-annotation
      ##
      extraPaths: []
      ## @param gateway.ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
      ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
      ## e.g:
      ## extraTls:
      ## - hosts:
      ##     - Loki Gateway.local
      ##   secretName: Loki Gateway.local-tls
      ##
      extraTls: []
      ## @param gateway.ingress.secrets Custom TLS certificates as secrets
      ## NOTE: 'key' and 'certificate' are expected in PEM format
      ## NOTE: 'name' should line up with a 'secretName' set further up
      ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
      ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
      ## It is also possible to create and manage the certificates outside of this helm chart
      ## Please see README.md for more information
      ## e.g:
      ## secrets:
      ##   - name: Loki Gateway.local-tls
      ##     key: |-
      ##       -----BEGIN RSA PRIVATE KEY-----
      ##       ...
      ##       -----END RSA PRIVATE KEY-----
      ##     certificate: |-
      ##       -----BEGIN CERTIFICATE-----
      ##       ...
      ##       -----END CERTIFICATE-----
      ##
      secrets: []
  ## @section index-gateway Deployment Parameters
  ##
  indexGateway:
    ## @param indexGateway.enabled Enable index-gateway deployment
    ##
    enabled: false
    ## @param indexGateway.extraEnvVars Array with extra environment variables to add to indexGateway nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param indexGateway.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for indexGateway nodes
    ##
    extraEnvVarsCM: ""
    ## @param indexGateway.extraEnvVarsSecret Name of existing Secret containing extra env vars for indexGateway nodes
    ##
    extraEnvVarsSecret: ""
    ## @param indexGateway.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param indexGateway.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param indexGateway.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param indexGateway.replicaCount Number of index-gateway replicas to deploy
    ##
    replicaCount: 1
    ## @param indexGateway.podManagementPolicy podManagementPolicy to manage scaling operation
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: ""
    ## Configure extra options for index-gateway containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param indexGateway.livenessProbe.enabled Enable livenessProbe on index-gateway nodes
    ## @param indexGateway.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param indexGateway.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param indexGateway.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param indexGateway.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param indexGateway.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 60
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param indexGateway.readinessProbe.enabled Enable readinessProbe on index-gateway nodes
    ## @param indexGateway.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param indexGateway.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param indexGateway.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param indexGateway.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param indexGateway.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 60
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param indexGateway.startupProbe.enabled Enable startupProbe on index-gateway containers
    ## @param indexGateway.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param indexGateway.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param indexGateway.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param indexGateway.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param indexGateway.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param indexGateway.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param indexGateway.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param indexGateway.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## indexGateway resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param indexGateway.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if indexGateway.resources is set (indexGateway.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param indexGateway.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param indexGateway.podSecurityContext.enabled Enabled index-gateway pods' Security Context
    ## @param indexGateway.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param indexGateway.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param indexGateway.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param indexGateway.podSecurityContext.fsGroup Set index-gateway pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param indexGateway.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param indexGateway.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param indexGateway.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param indexGateway.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param indexGateway.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param indexGateway.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param indexGateway.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param indexGateway.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param indexGateway.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param indexGateway.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param indexGateway.lifecycleHooks for the indexGateway container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param indexGateway.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param indexGateway.hostAliases indexGateway pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param indexGateway.podLabels Extra labels for indexGateway pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param indexGateway.podAnnotations Annotations for indexGateway pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param indexGateway.podAffinityPreset Pod affinity preset. Ignored if `indexGateway.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param indexGateway.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `indexGateway.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node indexGateway.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param indexGateway.nodeAffinityPreset.type Node affinity preset type. Ignored if `indexGateway.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param indexGateway.nodeAffinityPreset.key Node label key to match. Ignored if `indexGateway.affinity` is set
      ##
      key: ""
      ## @param indexGateway.nodeAffinityPreset.values Node label values to match. Ignored if `indexGateway.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param indexGateway.affinity Affinity for index-gateway pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `indexGateway.podAffinityPreset`, `indexGateway.podAntiAffinityPreset`, and `indexGateway.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param indexGateway.nodeSelector Node labels for index-gateway pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param indexGateway.tolerations Tolerations for index-gateway pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param indexGateway.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param indexGateway.priorityClassName index-gateway pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param indexGateway.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param indexGateway.updateStrategy.type index-gateway statefulset strategy type
    ## @param indexGateway.updateStrategy.rollingUpdate [object,nullable] index-gateway statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param indexGateway.extraVolumes Optionally specify extra list of additional volumes for the index-gateway pod(s)
    ##
    extraVolumes: []
    ## @param indexGateway.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the index-gateway container(s)
    ##
    extraVolumeMounts: []
    ## @param indexGateway.sidecars Add additional sidecar containers to the index-gateway pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param indexGateway.initContainers Add additional init containers to the index-gateway pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param indexGateway.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param indexGateway.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param indexGateway.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param indexGateway.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `indexGateway.pdb.minAvailable` and `indexGateway.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section index-gateway Persistence Parameters
    ##

    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param indexGateway.persistence.enabled Enable persistence in index-gateway instances
      ##
      enabled: false
      ## @param indexGateway.persistence.storageClass PVC Storage Class for index-gateway's data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ""
      ## @param indexGateway.persistence.subPath The subdirectory of the volume to mount to
      ##
      subPath: ""
      ## @param indexGateway.persistence.accessModes PVC Access modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param indexGateway.persistence.size PVC Storage Request for index-gateway's data volume
      ##
      size: 8Gi
      ## @param indexGateway.persistence.annotations Additional PVC annotations
      ##
      annotations: {}
      ## @param indexGateway.persistence.selector Selector to match an existing Persistent Volume for index-gateway's data PVC
      ## If set, the PVC can't have a PV dynamically provisioned for it
      ## E.g.
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @section index-gateway Traffic Exposure Parameters
    ##

    ## indexGateway service parameters
    ##
    service:
      ## @param indexGateway.service.type index-gateway service type
      ##
      type: ClusterIP
      ## @param indexGateway.service.ports.http index-gateway HTTP service port
      ## @param indexGateway.service.ports.grpc index-gateway GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param indexGateway.service.nodePorts.http Node port for HTTP
      ## @param indexGateway.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param indexGateway.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param indexGateway.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param indexGateway.service.clusterIP index-gateway service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param indexGateway.service.loadBalancerIP index-gateway service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param indexGateway.service.loadBalancerSourceRanges index-gateway service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param indexGateway.service.externalTrafficPolicy index-gateway service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param indexGateway.service.annotations Additional custom annotations for index-gateway service
      ##
      annotations: {}
      ## @param indexGateway.service.extraPorts Extra ports to expose in the index-gateway service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param indexGateway.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param indexGateway.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param indexGateway.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param indexGateway.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `indexGateway.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param indexGateway.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param indexGateway.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param indexGateway.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `indexGateway.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param indexGateway.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `indexGateway.networkPolicy.allowExternal` is true.
      ## @param indexGateway.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `indexGateway.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Distributor Deployment Parameters
  ##
  distributor:
    ## @param distributor.extraEnvVars Array with extra environment variables to add to distributor nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param distributor.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for distributor nodes
    ##
    extraEnvVarsCM: ""
    ## @param distributor.extraEnvVarsSecret Name of existing Secret containing extra env vars for distributor nodes
    ##
    extraEnvVarsSecret: ""
    ## @param distributor.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param distributor.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param distributor.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param distributor.replicaCount Number of Distributor replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for Distributor containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param distributor.livenessProbe.enabled Enable livenessProbe on Distributor nodes
    ## @param distributor.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param distributor.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param distributor.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param distributor.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param distributor.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param distributor.readinessProbe.enabled Enable readinessProbe on Distributor nodes
    ## @param distributor.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param distributor.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param distributor.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param distributor.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param distributor.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param distributor.startupProbe.enabled Enable startupProbe on Distributor containers
    ## @param distributor.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param distributor.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param distributor.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param distributor.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param distributor.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param distributor.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param distributor.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param distributor.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## distributor resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param distributor.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if distributor.resources is set (distributor.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param distributor.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param distributor.podSecurityContext.enabled Enabled Distributor pods' Security Context
    ## @param distributor.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param distributor.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param distributor.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param distributor.podSecurityContext.fsGroup Set Distributor pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param distributor.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param distributor.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param distributor.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param distributor.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param distributor.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param distributor.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param distributor.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param distributor.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param distributor.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param distributor.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param distributor.lifecycleHooks for the distributor container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param distributor.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param distributor.hostAliases distributor pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param distributor.podLabels Extra labels for distributor pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param distributor.podAnnotations Annotations for distributor pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param distributor.podAffinityPreset Pod affinity preset. Ignored if `distributor.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param distributor.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `distributor.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node distributor.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param distributor.nodeAffinityPreset.type Node affinity preset type. Ignored if `distributor.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param distributor.nodeAffinityPreset.key Node label key to match. Ignored if `distributor.affinity` is set
      ##
      key: ""
      ## @param distributor.nodeAffinityPreset.values Node label values to match. Ignored if `distributor.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param distributor.affinity Affinity for Distributor pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `distributor.podAffinityPreset`, `distributor.podAntiAffinityPreset`, and `distributor.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param distributor.nodeSelector Node labels for Distributor pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param distributor.tolerations Tolerations for Distributor pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param distributor.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param distributor.priorityClassName Distributor pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param distributor.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param distributor.updateStrategy.type Distributor statefulset strategy type
    ## @param distributor.updateStrategy.rollingUpdate [object,nullable] Distributor statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param distributor.extraVolumes Optionally specify extra list of additional volumes for the Distributor pod(s)
    ##
    extraVolumes: []
    ## @param distributor.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the Distributor container(s)
    ##
    extraVolumeMounts: []
    ## @param distributor.sidecars Add additional sidecar containers to the Distributor pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param distributor.initContainers Add additional init containers to the Distributor pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param distributor.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param distributor.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param distributor.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param distributor.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `distributor.pdb.minAvailable` and `distributor.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section Distributor Traffic Exposure Parameters
    ##

    ## distributor service parameters
    ##
    service:
      ## @param distributor.service.type Distributor service type
      ##
      type: ClusterIP
      ## @param distributor.service.ports.http Distributor HTTP service port
      ## @param distributor.service.ports.grpc Distributor GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param distributor.service.nodePorts.http Node port for HTTP
      ## @param distributor.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param distributor.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param distributor.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param distributor.service.clusterIP Distributor service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param distributor.service.loadBalancerIP Distributor service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param distributor.service.loadBalancerSourceRanges Distributor service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param distributor.service.externalTrafficPolicy Distributor service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param distributor.service.annotations Additional custom annotations for Distributor service
      ##
      annotations: {}
      ## @param distributor.service.extraPorts Extra ports to expose in the Distributor service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param distributor.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param distributor.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param distributor.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param distributor.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `distributor.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param distributor.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param distributor.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param distributor.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `distributor.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param distributor.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `distributor.networkPolicy.allowExternal` is true.
      ## @param distributor.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `distributor.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Ingester Deployment Parameters
  ##
  ingester:
    ## @param ingester.extraEnvVars Array with extra environment variables to add to ingester nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param ingester.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for ingester nodes
    ##
    extraEnvVarsCM: ""
    ## @param ingester.extraEnvVarsSecret Name of existing Secret containing extra env vars for ingester nodes
    ##
    extraEnvVarsSecret: ""
    ## @param ingester.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param ingester.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param ingester.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param ingester.replicaCount Number of Ingester replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for Ingester containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param ingester.livenessProbe.enabled Enable livenessProbe on Ingester nodes
    ## @param ingester.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param ingester.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param ingester.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param ingester.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param ingester.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param ingester.readinessProbe.enabled Enable readinessProbe on Ingester nodes
    ## @param ingester.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param ingester.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param ingester.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param ingester.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param ingester.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param ingester.startupProbe.enabled Enable startupProbe on Ingester containers
    ## @param ingester.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param ingester.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param ingester.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param ingester.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param ingester.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param ingester.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param ingester.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param ingester.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param ingester.lifecycleHooks for the ingester container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## ingester resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param ingester.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if ingester.resources is set (ingester.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "micro"
    ## @param ingester.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param ingester.podSecurityContext.enabled Enabled Ingester pods' Security Context
    ## @param ingester.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param ingester.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param ingester.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param ingester.podSecurityContext.fsGroup Set Ingester pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param ingester.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param ingester.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param ingester.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param ingester.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param ingester.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param ingester.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param ingester.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param ingester.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param ingester.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param ingester.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param ingester.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param ingester.hostAliases ingester pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param ingester.podLabels Extra labels for ingester pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param ingester.podAnnotations Annotations for ingester pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param ingester.podAffinityPreset Pod affinity preset. Ignored if `ingester.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param ingester.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `ingester.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node ingester.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param ingester.nodeAffinityPreset.type Node affinity preset type. Ignored if `ingester.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param ingester.nodeAffinityPreset.key Node label key to match. Ignored if `ingester.affinity` is set
      ##
      key: ""
      ## @param ingester.nodeAffinityPreset.values Node label values to match. Ignored if `ingester.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param ingester.affinity Affinity for ingester pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `ingester.podAffinityPreset`, `ingester.podAntiAffinityPreset`, and `ingester.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param ingester.nodeSelector Node labels for Ingester pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param ingester.tolerations Tolerations for Ingester pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param ingester.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param ingester.podManagementPolicy podManagementPolicy to manage scaling operation
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: ""
    ## @param ingester.priorityClassName Ingester pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param ingester.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param ingester.updateStrategy.type Ingester statefulset strategy type
    ## @param ingester.updateStrategy.rollingUpdate [object,nullable] Ingester statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param ingester.extraVolumes Optionally specify extra list of additional volumes for the Ingester pod(s)
    ##
    extraVolumes: []
    ## @param ingester.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the ingester container(s)
    ##
    extraVolumeMounts: []
    ## @param ingester.sidecars Add additional sidecar containers to the Ingester pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param ingester.initContainers Add additional init containers to the Ingester pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param ingester.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param ingester.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param ingester.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param ingester.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `ingester.pdb.minAvailable` and `ingester.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section Ingester Persistence Parameters
    ##

    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param ingester.persistence.enabled Enable persistence in Ingester instances
      ##
      enabled: true
      ## @param ingester.persistence.storageClass PVC Storage Class for Memcached data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ""
      ## @param ingester.persistence.subPath The subdirectory of the volume to mount to
      ##
      subPath: ""
      ## @param ingester.persistence.accessModes PVC Access modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param ingester.persistence.size PVC Storage Request for Memcached data volume
      ##
      size: 8Gi
      ## @param ingester.persistence.annotations Additional PVC annotations
      ##
      annotations: {}
      ## @param ingester.persistence.selector Selector to match an existing Persistent Volume for Ingester's data PVC
      ## If set, the PVC can't have a PV dynamically provisioned for it
      ## E.g.
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @section Ingester Traffic Exposure Parameters
    ##

    ## ingester service parameters
    ##
    service:
      ## @param ingester.service.type Ingester service type
      ##
      type: ClusterIP
      ## @param ingester.service.ports.http Ingester HTTP service port
      ## @param ingester.service.ports.grpc Ingester GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param ingester.service.nodePorts.http Node port for HTTP
      ## @param ingester.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param ingester.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param ingester.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param ingester.service.clusterIP Ingester service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param ingester.service.loadBalancerIP Ingester service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param ingester.service.loadBalancerSourceRanges Ingester service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param ingester.service.externalTrafficPolicy Ingester service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param ingester.service.annotations Additional custom annotations for Ingester service
      ##
      annotations: {}
      ## @param ingester.service.extraPorts Extra ports to expose in the Ingester service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param ingester.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param ingester.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param ingester.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param ingester.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `ingester.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param ingester.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param ingester.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param ingester.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `ingester.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param ingester.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `ingester.networkPolicy.allowExternal` is true.
      ## @param ingester.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `ingester.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Querier Deployment Parameters
  ##
  querier:
    ## @param querier.replicaCount Number of Querier replicas to deploy
    ##
    replicaCount: 1
    ## @param querier.extraEnvVars Array with extra environment variables to add to Querier nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param querier.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for Querier nodes
    ##
    extraEnvVarsCM: ""
    ## @param querier.extraEnvVarsSecret Name of existing Secret containing extra env vars for Querier nodes
    ##
    extraEnvVarsSecret: ""
    ## @param querier.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param querier.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param querier.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param querier.podManagementPolicy podManagementPolicy to manage scaling operation
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: ""
    ## Configure extra options for Querier containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param querier.livenessProbe.enabled Enable livenessProbe on Querier nodes
    ## @param querier.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param querier.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param querier.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param querier.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param querier.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param querier.readinessProbe.enabled Enable readinessProbe on Querier nodes
    ## @param querier.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param querier.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param querier.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param querier.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param querier.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param querier.startupProbe.enabled Enable startupProbe on Querier containers
    ## @param querier.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param querier.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param querier.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param querier.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param querier.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param querier.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param querier.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param querier.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## querier resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param querier.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if querier.resources is set (querier.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param querier.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param querier.podSecurityContext.enabled Enabled Querier pods' Security Context
    ## @param querier.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param querier.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param querier.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param querier.podSecurityContext.fsGroup Set Querier pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param querier.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param querier.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param querier.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param querier.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param querier.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param querier.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param querier.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param querier.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param querier.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param querier.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param querier.lifecycleHooks for the Querier container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param querier.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param querier.hostAliases querier pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param querier.podLabels Extra labels for querier pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param querier.podAnnotations Annotations for querier pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param querier.podAffinityPreset Pod affinity preset. Ignored if `querier.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param querier.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `querier.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node querier.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param querier.nodeAffinityPreset.type Node affinity preset type. Ignored if `querier.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param querier.nodeAffinityPreset.key Node label key to match. Ignored if `querier.affinity` is set
      ##
      key: ""
      ## @param querier.nodeAffinityPreset.values Node label values to match. Ignored if `querier.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param querier.affinity Affinity for Querier pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `querier.podAffinityPreset`, `querier.podAntiAffinityPreset`, and `querier.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param querier.nodeSelector Node labels for Querier pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param querier.tolerations Tolerations for Querier pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param querier.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param querier.priorityClassName Querier pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param querier.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param querier.updateStrategy.type Querier statefulset strategy type
    ## @param querier.updateStrategy.rollingUpdate [object,nullable] Querier statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param querier.extraVolumes Optionally specify extra list of additional volumes for the Querier pod(s)
    ##
    extraVolumes: []
    ## @param querier.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the querier container(s)
    ##
    extraVolumeMounts: []
    ## @param querier.sidecars Add additional sidecar containers to the Querier pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param querier.initContainers Add additional init containers to the Querier pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param querier.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param querier.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param querier.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param querier.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `querier.pdb.minAvailable` and `querier.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section Querier Persistence Parameters
    ##

    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param querier.persistence.enabled Enable persistence in Querier instances
      ##
      enabled: true
      ## @param querier.persistence.storageClass PVC Storage Class for Memcached data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ""
      ## @param querier.persistence.subPath The subdirectory of the volume to mount to
      ##
      subPath: ""
      ## @param querier.persistence.accessModes PVC Access modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param querier.persistence.size PVC Storage Request for Memcached data volume
      ##
      size: 8Gi
      ## @param querier.persistence.annotations Additional PVC annotations
      ##
      annotations: {}
      ## @param querier.persistence.selector Selector to match an existing Persistent Volume for Querier's data PVC
      ## If set, the PVC can't have a PV dynamically provisioned for it
      ## E.g.
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @section Querier Traffic Exposure Parameters
    ##

    ## querier service parameters
    ##
    service:
      ## @param querier.service.type Querier service type
      ##
      type: ClusterIP
      ## @param querier.service.ports.http Querier HTTP service port
      ## @param querier.service.ports.grpc Querier GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param querier.service.nodePorts.http Node port for HTTP
      ## @param querier.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param querier.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param querier.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param querier.service.clusterIP Querier service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param querier.service.loadBalancerIP Querier service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param querier.service.loadBalancerSourceRanges Querier service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param querier.service.externalTrafficPolicy Querier service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param querier.service.annotations Additional custom annotations for Querier service
      ##
      annotations: {}
      ## @param querier.service.extraPorts Extra ports to expose in the Querier service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param querier.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param querier.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param querier.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param querier.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `querier.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param querier.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param querier.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param querier.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `querier.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param querier.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `querier.networkPolicy.allowExternal` is true.
      ## @param querier.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `querier.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Query Frontend Deployment Parameters
  ##
  queryFrontend:
    ## @param queryFrontend.extraEnvVars Array with extra environment variables to add to queryFrontend nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param queryFrontend.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for queryFrontend nodes
    ##
    extraEnvVarsCM: ""
    ## @param queryFrontend.extraEnvVarsSecret Name of existing Secret containing extra env vars for queryFrontend nodes
    ##
    extraEnvVarsSecret: ""
    ## @param queryFrontend.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param queryFrontend.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param queryFrontend.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param queryFrontend.replicaCount Number of queryFrontend replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for queryFrontend containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param queryFrontend.livenessProbe.enabled Enable livenessProbe on queryFrontend nodes
    ## @param queryFrontend.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param queryFrontend.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param queryFrontend.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param queryFrontend.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param queryFrontend.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param queryFrontend.readinessProbe.enabled Enable readinessProbe on queryFrontend nodes
    ## @param queryFrontend.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param queryFrontend.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param queryFrontend.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param queryFrontend.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param queryFrontend.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param queryFrontend.startupProbe.enabled Enable startupProbe on queryFrontend containers
    ## @param queryFrontend.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param queryFrontend.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param queryFrontend.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param queryFrontend.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param queryFrontend.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param queryFrontend.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param queryFrontend.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param queryFrontend.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## queryFrontend resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param queryFrontend.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if queryFrontend.resources is set (queryFrontend.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param queryFrontend.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param queryFrontend.podSecurityContext.enabled Enabled queryFrontend pods' Security Context
    ## @param queryFrontend.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param queryFrontend.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param queryFrontend.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param queryFrontend.podSecurityContext.fsGroup Set queryFrontend pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param queryFrontend.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param queryFrontend.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param queryFrontend.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param queryFrontend.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param queryFrontend.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param queryFrontend.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param queryFrontend.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param queryFrontend.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param queryFrontend.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param queryFrontend.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param queryFrontend.lifecycleHooks for the queryFrontend container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param queryFrontend.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param queryFrontend.hostAliases queryFrontend pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param queryFrontend.podLabels Extra labels for queryFrontend pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param queryFrontend.podAnnotations Annotations for queryFrontend pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param queryFrontend.podAffinityPreset Pod affinity preset. Ignored if `queryFrontend.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param queryFrontend.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `queryFrontend.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node queryFrontend.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param queryFrontend.nodeAffinityPreset.type Node affinity preset type. Ignored if `queryFrontend.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param queryFrontend.nodeAffinityPreset.key Node label key to match. Ignored if `queryFrontend.affinity` is set
      ##
      key: ""
      ## @param queryFrontend.nodeAffinityPreset.values Node label values to match. Ignored if `queryFrontend.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param queryFrontend.affinity Affinity for queryFrontend pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `queryFrontend.podAffinityPreset`, `queryFrontend.podAntiAffinityPreset`, and `queryFrontend.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param queryFrontend.nodeSelector Node labels for queryFrontend pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param queryFrontend.tolerations Tolerations for queryFrontend pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param queryFrontend.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param queryFrontend.priorityClassName queryFrontend pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param queryFrontend.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param queryFrontend.updateStrategy.type queryFrontend statefulset strategy type
    ## @param queryFrontend.updateStrategy.rollingUpdate [object,nullable] queryFrontend statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param queryFrontend.extraVolumes Optionally specify extra list of additional volumes for the queryFrontend pod(s)
    ##
    extraVolumes: []
    ## @param queryFrontend.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the queryFrontend container(s)
    ##
    extraVolumeMounts: []
    ## @param queryFrontend.sidecars Add additional sidecar containers to the queryFrontend pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param queryFrontend.initContainers Add additional init containers to the queryFrontend pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param queryFrontend.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param queryFrontend.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param queryFrontend.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param queryFrontend.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `queryFrontend.pdb.minAvailable` and `queryFrontend.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section Query Frontend Traffic Exposure Parameters
    ##

    ## queryFrontend service parameters
    ##
    service:
      ## @param queryFrontend.service.type queryFrontend service type
      ##
      type: ClusterIP
      ## @param queryFrontend.service.ports.http queryFrontend HTTP service port
      ## @param queryFrontend.service.ports.grpc queryFrontend GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param queryFrontend.service.nodePorts.http Node port for HTTP
      ## @param queryFrontend.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param queryFrontend.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param queryFrontend.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param queryFrontend.service.clusterIP queryFrontend service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param queryFrontend.service.loadBalancerIP queryFrontend service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param queryFrontend.service.loadBalancerSourceRanges queryFrontend service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param queryFrontend.service.externalTrafficPolicy queryFrontend service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param queryFrontend.service.annotations Additional custom annotations for queryFrontend service
      ##
      annotations: {}
      ## @param queryFrontend.service.extraPorts Extra ports to expose in the queryFrontend service
      ##
      extraPorts: []
      ## Headless service properties
      ##
      headless:
        ## @param queryFrontend.service.headless.annotations Annotations for the headless service.
        ##
        annotations: {}
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param queryFrontend.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param queryFrontend.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param queryFrontend.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param queryFrontend.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `queryFrontend.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param queryFrontend.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param queryFrontend.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param queryFrontend.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `queryFrontend.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param queryFrontend.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `queryFrontend.networkPolicy.allowExternal` is true.
      ## @param queryFrontend.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `queryFrontend.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Query Scheduler Deployment Parameters
  ##
  queryScheduler:
    ## @param queryScheduler.extraEnvVars Array with extra environment variables to add to queryScheduler nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param queryScheduler.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for queryScheduler nodes
    ##
    extraEnvVarsCM: ""
    ## @param queryScheduler.extraEnvVarsSecret Name of existing Secret containing extra env vars for queryScheduler nodes
    ##
    extraEnvVarsSecret: ""
    ## @param queryScheduler.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param queryScheduler.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param queryScheduler.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param queryScheduler.replicaCount Number of queryScheduler replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for queryScheduler containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param queryScheduler.livenessProbe.enabled Enable livenessProbe on queryScheduler nodes
    ## @param queryScheduler.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param queryScheduler.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param queryScheduler.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param queryScheduler.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param queryScheduler.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param queryScheduler.minReadySeconds Minimum time to wait before performing readiness check
    ##
    minReadySeconds: 10
    ## @param queryScheduler.readinessProbe.enabled Enable readinessProbe on queryScheduler nodes
    ## @param queryScheduler.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param queryScheduler.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param queryScheduler.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param queryScheduler.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param queryScheduler.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param queryScheduler.startupProbe.enabled Enable startupProbe on queryScheduler containers
    ## @param queryScheduler.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param queryScheduler.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param queryScheduler.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param queryScheduler.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param queryScheduler.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param queryScheduler.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param queryScheduler.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param queryScheduler.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## queryScheduler resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param queryScheduler.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if queryScheduler.resources is set (queryScheduler.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param queryScheduler.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param queryScheduler.podSecurityContext.enabled Enabled queryScheduler pods' Security Context
    ## @param queryScheduler.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param queryScheduler.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param queryScheduler.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param queryScheduler.podSecurityContext.fsGroup Set queryScheduler pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param queryScheduler.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param queryScheduler.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param queryScheduler.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param queryScheduler.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param queryScheduler.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param queryScheduler.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param queryScheduler.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param queryScheduler.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param queryScheduler.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param queryScheduler.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param queryScheduler.lifecycleHooks for the queryScheduler container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param queryScheduler.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param queryScheduler.hostAliases queryScheduler pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param queryScheduler.podLabels Extra labels for queryScheduler pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param queryScheduler.podAnnotations Annotations for queryScheduler pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param queryScheduler.podAffinityPreset Pod affinity preset. Ignored if `queryScheduler.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param queryScheduler.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `queryScheduler.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node queryScheduler.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param queryScheduler.nodeAffinityPreset.type Node affinity preset type. Ignored if `queryScheduler.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param queryScheduler.nodeAffinityPreset.key Node label key to match. Ignored if `queryScheduler.affinity` is set
      ##
      key: ""
      ## @param queryScheduler.nodeAffinityPreset.values Node label values to match. Ignored if `queryScheduler.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param queryScheduler.affinity Affinity for queryScheduler pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `queryScheduler.podAffinityPreset`, `queryScheduler.podAntiAffinityPreset`, and `queryScheduler.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param queryScheduler.nodeSelector Node labels for queryScheduler pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param queryScheduler.tolerations Tolerations for queryScheduler pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param queryScheduler.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param queryScheduler.priorityClassName queryScheduler pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param queryScheduler.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param queryScheduler.updateStrategy.type queryScheduler statefulset strategy type
    ## @param queryScheduler.updateStrategy.rollingUpdate [object,nullable] queryScheduler statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param queryScheduler.extraVolumes Optionally specify extra list of additional volumes for the queryScheduler pod(s)
    ##
    extraVolumes: []
    ## @param queryScheduler.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the queryScheduler container(s)
    ##
    extraVolumeMounts: []
    ## @param queryScheduler.sidecars Add additional sidecar containers to the queryScheduler pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param queryScheduler.initContainers Add additional init containers to the queryScheduler pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param queryScheduler.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param queryScheduler.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param queryScheduler.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param queryScheduler.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `queryScheduler.pdb.minAvailable` and `queryScheduler.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section Query Scheduler Traffic Exposure Parameters
    ##

    ## queryScheduler service parameters
    ##
    service:
      ## @param queryScheduler.service.type queryScheduler service type
      ##
      type: ClusterIP
      ## @param queryScheduler.service.ports.http queryScheduler HTTP service port
      ## @param queryScheduler.service.ports.grpc queryScheduler GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param queryScheduler.service.nodePorts.http Node port for HTTP
      ## @param queryScheduler.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param queryScheduler.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param queryScheduler.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param queryScheduler.service.clusterIP queryScheduler service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param queryScheduler.service.loadBalancerIP queryScheduler service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param queryScheduler.service.loadBalancerSourceRanges queryScheduler service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param queryScheduler.service.externalTrafficPolicy queryScheduler service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param queryScheduler.service.annotations Additional custom annotations for queryScheduler service
      ##
      annotations: {}
      ## @param queryScheduler.service.extraPorts Extra ports to expose in the queryScheduler service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param queryScheduler.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param queryScheduler.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param queryScheduler.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param queryScheduler.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `queryScheduler.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param queryScheduler.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param queryScheduler.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param queryScheduler.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `queryScheduler.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param queryScheduler.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `queryScheduler.networkPolicy.allowExternal` is true.
      ## @param queryScheduler.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `queryScheduler.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Ruler Deployment Parameters
  ##
  ruler:
    ## @param ruler.enabled Deploy ruler component
    ##
    enabled: false
    ## @param ruler.extraEnvVars Array with extra environment variables to add to ruler nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param ruler.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for ruler nodes
    ##
    extraEnvVarsCM: ""
    ## @param ruler.extraEnvVarsSecret Name of existing Secret containing extra env vars for ruler nodes
    ##
    extraEnvVarsSecret: ""
    ## @param ruler.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param ruler.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param ruler.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param ruler.podManagementPolicy podManagementPolicy to manage scaling operation
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    ##
    podManagementPolicy: ""
    ## @param ruler.replicaCount Number of Ruler replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for Ruler containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param ruler.livenessProbe.enabled Enable livenessProbe on Ruler nodes
    ## @param ruler.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param ruler.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param ruler.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param ruler.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param ruler.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param ruler.readinessProbe.enabled Enable readinessProbe on Ruler nodes
    ## @param ruler.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param ruler.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param ruler.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param ruler.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param ruler.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param ruler.startupProbe.enabled Enable startupProbe on Ruler containers
    ## @param ruler.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param ruler.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param ruler.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param ruler.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param ruler.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param ruler.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param ruler.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param ruler.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param ruler.lifecycleHooks for the ruler container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## ruler resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param ruler.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if ruler.resources is set (ruler.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param ruler.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param ruler.podSecurityContext.enabled Enabled Ruler pods' Security Context
    ## @param ruler.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param ruler.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param ruler.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param ruler.podSecurityContext.fsGroup Set Ruler pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param ruler.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param ruler.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param ruler.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param ruler.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param ruler.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param ruler.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param ruler.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param ruler.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param ruler.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param ruler.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param ruler.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param ruler.hostAliases ruler pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param ruler.podLabels Extra labels for ruler pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param ruler.podAnnotations Annotations for ruler pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param ruler.podAffinityPreset Pod affinity preset. Ignored if `ruler.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param ruler.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `ruler.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node ruler.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param ruler.nodeAffinityPreset.type Node affinity preset type. Ignored if `ruler.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param ruler.nodeAffinityPreset.key Node label key to match. Ignored if `ruler.affinity` is set
      ##
      key: ""
      ## @param ruler.nodeAffinityPreset.values Node label values to match. Ignored if `ruler.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param ruler.affinity Affinity for ruler pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `ruler.podAffinityPreset`, `ruler.podAntiAffinityPreset`, and `ruler.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param ruler.nodeSelector Node labels for Ruler pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param ruler.tolerations Tolerations for Ruler pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param ruler.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param ruler.priorityClassName Ruler pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param ruler.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param ruler.updateStrategy.type Ruler statefulset strategy type
    ## @param ruler.updateStrategy.rollingUpdate [object,nullable] Ruler statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param ruler.extraVolumes Optionally specify extra list of additional volumes for the Ruler pod(s)
    ##
    extraVolumes: []
    ## @param ruler.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the ruler container(s)
    ##
    extraVolumeMounts: []
    ## @param ruler.sidecars Add additional sidecar containers to the Ruler pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param ruler.initContainers Add additional init containers to the Ruler pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param ruler.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param ruler.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param ruler.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param ruler.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `ruler.pdb.minAvailable` and `ruler.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section Ruler Persistence Parameters
    ##

    ## Enable persistence using Persistent Volume Claims
    ## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
    ##
    persistence:
      ## @param ruler.persistence.enabled Enable persistence in Ruler instances
      ##
      enabled: true
      ## @param ruler.persistence.storageClass PVC Storage Class for Memcached data volume
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: ""
      ## @param ruler.persistence.subPath The subdirectory of the volume to mount to
      ##
      subPath: ""
      ## @param ruler.persistence.accessModes PVC Access modes
      ##
      accessModes:
        - ReadWriteOnce
      ## @param ruler.persistence.size PVC Storage Request for Memcached data volume
      ##
      size: 8Gi
      ## @param ruler.persistence.annotations Additional PVC annotations
      ##
      annotations: {}
      ## @param ruler.persistence.selector Selector to match an existing Persistent Volume for Ruler's data PVC
      ## If set, the PVC can't have a PV dynamically provisioned for it
      ## E.g.
      ## selector:
      ##   matchLabels:
      ##     app: my-app
      ##
      selector: {}
    ## @section Ruler Traffic Exposure Parameters
    ##

    ## ruler service parameters
    ##
    service:
      ## @param ruler.service.type Ruler service type
      ##
      type: ClusterIP
      ## @param ruler.service.ports.http Ruler HTTP service port
      ## @param ruler.service.ports.grpc Ruler GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param ruler.service.nodePorts.http Node port for HTTP
      ## @param ruler.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param ruler.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param ruler.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param ruler.service.clusterIP Ruler service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param ruler.service.loadBalancerIP Ruler service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param ruler.service.loadBalancerSourceRanges Ruler service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param ruler.service.externalTrafficPolicy Ruler service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param ruler.service.annotations Additional custom annotations for Ruler service
      ##
      annotations: {}
      ## @param ruler.service.extraPorts Extra ports to expose in the Ruler service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param ruler.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param ruler.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param ruler.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param ruler.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `ruler.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param ruler.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param ruler.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param ruler.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `ruler.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param ruler.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `ruler.networkPolicy.allowExternal` is true.
      ## @param ruler.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `ruler.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section table-manager Deployment Parameters
  ##
  tableManager:
    ## @param tableManager.enabled Deploy table-manager
    ##
    enabled: false
    ## @param tableManager.extraEnvVars Array with extra environment variables to add to tableManager nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param tableManager.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for tableManager nodes
    ##
    extraEnvVarsCM: ""
    ## @param tableManager.extraEnvVarsSecret Name of existing Secret containing extra env vars for tableManager nodes
    ##
    extraEnvVarsSecret: ""
    ## @param tableManager.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param tableManager.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param tableManager.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param tableManager.replicaCount Number of table-manager replicas to deploy
    ##
    replicaCount: 1
    ## Configure extra options for table-manager containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param tableManager.livenessProbe.enabled Enable livenessProbe on table-manager nodes
    ## @param tableManager.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param tableManager.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param tableManager.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param tableManager.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param tableManager.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param tableManager.readinessProbe.enabled Enable readinessProbe on table-manager nodes
    ## @param tableManager.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param tableManager.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param tableManager.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param tableManager.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param tableManager.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param tableManager.startupProbe.enabled Enable startupProbe on table-manager containers
    ## @param tableManager.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param tableManager.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param tableManager.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param tableManager.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param tableManager.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param tableManager.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param tableManager.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param tableManager.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## tableManager resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param tableManager.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if tableManager.resources is set (tableManager.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param tableManager.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param tableManager.podSecurityContext.enabled Enabled table-manager pods' Security Context
    ## @param tableManager.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param tableManager.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param tableManager.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param tableManager.podSecurityContext.fsGroup Set table-manager pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 1001
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param tableManager.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param tableManager.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param tableManager.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param tableManager.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param tableManager.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param tableManager.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param tableManager.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param tableManager.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param tableManager.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param tableManager.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 1001
      runAsGroup: 1001
      runAsNonRoot: true
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param tableManager.lifecycleHooks for the tableManager container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## @param tableManager.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: false
    ## @param tableManager.hostAliases tableManager pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param tableManager.podLabels Extra labels for tableManager pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param tableManager.podAnnotations Annotations for tableManager pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param tableManager.podAffinityPreset Pod affinity preset. Ignored if `tableManager.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param tableManager.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `tableManager.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node tableManager.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param tableManager.nodeAffinityPreset.type Node affinity preset type. Ignored if `tableManager.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param tableManager.nodeAffinityPreset.key Node label key to match. Ignored if `tableManager.affinity` is set
      ##
      key: ""
      ## @param tableManager.nodeAffinityPreset.values Node label values to match. Ignored if `tableManager.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param tableManager.affinity Affinity for table-manager pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `tableManager.podAffinityPreset`, `tableManager.podAntiAffinityPreset`, and `tableManager.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param tableManager.nodeSelector Node labels for table-manager pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param tableManager.tolerations Tolerations for table-manager pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param tableManager.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param tableManager.priorityClassName table-manager pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param tableManager.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param tableManager.updateStrategy.type table-manager statefulset strategy type
    ## @param tableManager.updateStrategy.rollingUpdate [object,nullable] table-manager statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param tableManager.extraVolumes Optionally specify extra list of additional volumes for the table-manager pod(s)
    ##
    extraVolumes: []
    ## @param tableManager.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the table-manager container(s)
    ##
    extraVolumeMounts: []
    ## @param tableManager.sidecars Add additional sidecar containers to the table-manager pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param tableManager.initContainers Add additional init containers to the table-manager pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param tableManager.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## Pod Disruption Budget configuration
    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
    ## @param tableManager.pdb.create Enable/disable a Pod Disruption Budget creation
    ## @param tableManager.pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
    ## @param tableManager.pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable. Defaults to `1` if both `tableManager.pdb.minAvailable` and `tableManager.pdb.maxUnavailable` are empty.
    ##
    pdb:
      create: true
      minAvailable: ""
      maxUnavailable: ""
    ## @section table-manager Traffic Exposure Parameters
    ##

    ## tableManager service parameters
    ##
    service:
      ## @param tableManager.service.type table-manager service type
      ##
      type: ClusterIP
      ## @param tableManager.service.ports.http table-manager HTTP service port
      ## @param tableManager.service.ports.grpc table-manager GRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param tableManager.service.nodePorts.http Node port for HTTP
      ## @param tableManager.service.nodePorts.grpc Node port for GRPC
      ##
      nodePorts:
        http: ""
        grpc: ""
      ## @param tableManager.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param tableManager.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param tableManager.service.clusterIP table-manager service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param tableManager.service.loadBalancerIP table-manager service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param tableManager.service.loadBalancerSourceRanges table-manager service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param tableManager.service.externalTrafficPolicy table-manager service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param tableManager.service.annotations Additional custom annotations for table-manager service
      ##
      annotations: {}
      ## @param tableManager.service.extraPorts Extra ports to expose in the table-manager service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param tableManager.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param tableManager.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param tableManager.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param tableManager.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `tableManager.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param tableManager.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param tableManager.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param tableManager.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `tableManager.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param tableManager.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `tableManager.networkPolicy.allowExternal` is true.
      ## @param tableManager.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `tableManager.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
  ## @section Promtail Deployment Parameters
  ##
  promtail:
    ## @param promtail.enabled Deploy promtail
    ##
    enabled: true
    ## Bitnami Promtail image
    ## ref: https://hub.docker.com/r/bitnami/grafana-promtail/tags/
    ## @param promtail.image.registry [default: REGISTRY_NAME] Grafana Promtail image registry
    ## @param promtail.image.repository [default: REPOSITORY_NAME/promtail] Grafana Promtail image repository
    ## @skip promtail.image.tag Grafana Promtail image tag (immutable tags are recommended)
    ## @param promtail.image.digest Grafana Promtail image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param promtail.image.pullPolicy Grafana Promtail image pull policy
    ## @param promtail.image.pullSecrets Grafana Promtail image pull secrets
    ##
    image:
      registry: docker.io
      repository: bitnami/promtail
      tag: 3.2.0-debian-12-r0
      digest: ""
      ## Specify a imagePullPolicy
      ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
      ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
      ##
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## @param promtail.extraEnvVars Array with extra environment variables to add to promtail nodes
    ## e.g:
    ## extraEnvVars:
    ##   - name: FOO
    ##     value: "bar"
    ##
    extraEnvVars: []
    ## @param promtail.extraEnvVarsCM Name of existing ConfigMap containing extra env vars for promtail nodes
    ##
    extraEnvVarsCM: ""
    ## @param promtail.extraEnvVarsSecret Name of existing Secret containing extra env vars for promtail nodes
    ##
    extraEnvVarsSecret: ""
    ## @param promtail.command Override default container command (useful when using custom images)
    ##
    command: []
    ## @param promtail.args Override default container args (useful when using custom images)
    ##
    args: []
    ## @param promtail.extraArgs Additional container args (will be concatenated to args, unless diagnosticMode is enabled)
    ##
    extraArgs: []
    ## @param promtail.containerPorts.http Promtail HTTP port
    ## @param promtail.containerPorts.grpc Promtail HTTP port
    ##
    containerPorts:
      http: 8080
      grpc: 9095
    ## Configure extra options for Promtail containers' liveness, readiness and startup probes
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
    ## @param promtail.livenessProbe.enabled Enable livenessProbe on Promtail nodes
    ## @param promtail.livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
    ## @param promtail.livenessProbe.periodSeconds Period seconds for livenessProbe
    ## @param promtail.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    ## @param promtail.livenessProbe.failureThreshold Failure threshold for livenessProbe
    ## @param promtail.livenessProbe.successThreshold Success threshold for livenessProbe
    ##
    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param promtail.readinessProbe.enabled Enable readinessProbe on Promtail nodes
    ## @param promtail.readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
    ## @param promtail.readinessProbe.periodSeconds Period seconds for readinessProbe
    ## @param promtail.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    ## @param promtail.readinessProbe.failureThreshold Failure threshold for readinessProbe
    ## @param promtail.readinessProbe.successThreshold Success threshold for readinessProbe
    ##
    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    ## @param promtail.startupProbe.enabled Enable startupProbe on Promtail containers
    ## @param promtail.startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
    ## @param promtail.startupProbe.periodSeconds Period seconds for startupProbe
    ## @param promtail.startupProbe.timeoutSeconds Timeout seconds for startupProbe
    ## @param promtail.startupProbe.failureThreshold Failure threshold for startupProbe
    ## @param promtail.startupProbe.successThreshold Success threshold for startupProbe
    ##
    startupProbe:
      enabled: false
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 1
      failureThreshold: 15
      successThreshold: 1
    ## @param promtail.customLivenessProbe Custom livenessProbe that overrides the default one
    ##
    customLivenessProbe: {}
    ## @param promtail.customReadinessProbe Custom readinessProbe that overrides the default one
    ##
    customReadinessProbe: {}
    ## @param promtail.customStartupProbe Custom startupProbe that overrides the default one
    ##
    customStartupProbe: {}
    ## @param promtail.lifecycleHooks for the promtail container(s) to automate configuration before or after startup
    ##
    lifecycleHooks: {}
    ## promtail resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param promtail.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if promtail.resources is set (promtail.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param promtail.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Configure Pods Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param promtail.podSecurityContext.enabled Enabled Promtail pods' Security Context
    ## @param promtail.podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
    ## @param promtail.podSecurityContext.sysctls Set kernel settings using the sysctl interface
    ## @param promtail.podSecurityContext.supplementalGroups Set filesystem extra groups
    ## @param promtail.podSecurityContext.fsGroup Set Promtail pod's Security Context fsGroup
    ##
    podSecurityContext:
      enabled: true
      fsGroupChangePolicy: Always
      sysctls: []
      supplementalGroups: []
      fsGroup: 0
    ## Configure Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
    ## @param promtail.containerSecurityContext.enabled Enabled containers' Security Context
    ## @param promtail.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param promtail.containerSecurityContext.runAsUser Set containers' Security Context runAsUser
    ## @param promtail.containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
    ## @param promtail.containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
    ## @param promtail.containerSecurityContext.privileged Set container's Security Context privileged
    ## @param promtail.containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
    ## @param promtail.containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
    ## @param promtail.containerSecurityContext.capabilities.drop List of capabilities to be dropped
    ## @param promtail.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ##
    containerSecurityContext:
      enabled: true
      seLinuxOptions: {}
      runAsUser: 0
      runAsGroup: 0
      runAsNonRoot: false
      privileged: false
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      seccompProfile:
        type: "RuntimeDefault"
    ## @param promtail.automountServiceAccountToken Mount Service Account token in pod
    ##
    automountServiceAccountToken: true
    ## @param promtail.hostAliases promtail pods host aliases
    ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
    ##
    hostAliases: []
    ## @param promtail.podLabels Extra labels for promtail pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
    ##
    podLabels: {}
    ## @param promtail.podAnnotations Annotations for promtail pods
    ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
    ##
    podAnnotations: {}
    ## @param promtail.podAffinityPreset Pod affinity preset. Ignored if `promtail.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAffinityPreset: ""
    ## @param promtail.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `promtail.affinity` is set. Allowed values: `soft` or `hard`
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    ##
    podAntiAffinityPreset: soft
    ## Node promtail.affinity preset
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    ##
    nodeAffinityPreset:
      ## @param promtail.nodeAffinityPreset.type Node affinity preset type. Ignored if `promtail.affinity` is set. Allowed values: `soft` or `hard`
      ##
      type: ""
      ## @param promtail.nodeAffinityPreset.key Node label key to match. Ignored if `promtail.affinity` is set
      ##
      key: ""
      ## @param promtail.nodeAffinityPreset.values Node label values to match. Ignored if `promtail.affinity` is set
      ## E.g.
      ## values:
      ##   - e2e-az1
      ##   - e2e-az2
      ##
      values: []
    ## @param promtail.affinity Affinity for promtail pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ## NOTE: `promtail.podAffinityPreset`, `promtail.podAntiAffinityPreset`, and `promtail.nodeAffinityPreset` will be ignored when it's set
    ##
    affinity: {}
    ## @param promtail.nodeSelector Node labels for Promtail pods assignment
    ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
    ##
    nodeSelector: {}
    ## @param promtail.tolerations Tolerations for Promtail pods assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    ## @param promtail.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
    ##
    topologySpreadConstraints: []
    ## @param promtail.priorityClassName Promtail pods' priorityClassName
    ##
    priorityClassName: ""
    ## @param promtail.schedulerName Kubernetes pod scheduler registry
    ## https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
    ##
    schedulerName: ""
    ## @param promtail.updateStrategy.type Promtail statefulset strategy type
    ## @param promtail.updateStrategy.rollingUpdate [object,nullable] Promtail statefulset rolling update configuration parameters
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
    ##
    updateStrategy:
      type: RollingUpdate
      rollingUpdate: null
    ## @param promtail.extraVolumes Optionally specify extra list of additional volumes for the Promtail pod(s)
    ##
    extraVolumes: []
    ## @param promtail.extraVolumeMounts Optionally specify extra list of additional volumeMounts for the promtail container(s)
    ##
    extraVolumeMounts: []
    ## @param promtail.sidecars Add additional sidecar containers to the Promtail pod(s)
    ## e.g:
    ## sidecars:
    ##   - name: your-image-name
    ##     image: your-image
    ##     imagePullPolicy: Always
    ##     ports:
    ##       - name: portname
    ##         containerPort: 1234
    ##
    sidecars: []
    ## @param promtail.initContainers Add additional init containers to the Promtail pod(s)
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ## e.g:
    ## initContainers:
    ##  - name: your-image-name
    ##    image: your-image
    ##    imagePullPolicy: Always
    ##    command: ['sh', '-c', 'echo "hello world"']
    ##
    initContainers: []
    ## @param promtail.enableServiceLinks Whether information about services should be injected into pod's environment variable
    ## The environment variables injected by service links are not used, but can lead to slow boot times or slow running of the scripts when there are many services in the current namespace.
    ## If you experience slow pod startups or slow running of the scripts you probably want to set this to `false`.
    ##
    enableServiceLinks: true
    ## @param promtail.configuration [string] Promtail configuration
    ##
    configuration: |
      server:
        log_level: {{ .Values.promtail.logLevel }}
        http_listen_port: {{ .Values.promtail.containerPorts.http }}
        grpc_listen_port: {{ .Values.promtail.containerPorts.grpc }}

      clients:
        - url: http://{{ include "grafana-loki.gateway.fullname" . }}:{{ .Values.gateway.service.ports.http }}/loki/api/v1/push
          {{- if .Values.gateway.auth.enabled }}
          basic_auth:
            # The username to use for basic auth
            username: {{ .Values.gateway.auth.username }}
            password_file: /bitnami/promtail/conf/secrets/password
          {{- end }}
      positions:
        filename: /run/promtail/positions.yaml

      scrape_configs:
        # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference
        - job_name: kubernetes-pods
          pipeline_stages:
            - cri: {}
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels:
                - __meta_kubernetes_pod_controller_name
              regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
              action: replace
              target_label: __tmp_controller_name
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - __meta_kubernetes_pod_label_app
                - __tmp_controller_name
                - __meta_kubernetes_pod_name
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: app
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_component
                - __meta_kubernetes_pod_label_component
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: component
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: node_name
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              replacement: $1
              separator: /
              source_labels:
              - namespace
              - app
              target_label: job
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_container_name
              target_label: container
            - action: replace
              replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
              target_label: __path__
            - action: replace
              regex: true/(.*)
              replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
              - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash
              - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash
              - __meta_kubernetes_pod_container_name
              target_label: __path__
    ## @param promtail.existingSecret Name of a Secret that contains the Promtail configuration
    ##
    existingSecret: ""
    ## @param promtail.logLevel Promtail logging level
    ##
    logLevel: info
    ## @section Promtail Traffic Exposure Parameters
    ##

    ## promtail service parameters
    ##
    service:
      ## @param promtail.service.type Promtail service type
      ##
      type: ClusterIP
      ## @param promtail.service.ports.http Promtail HTTP service port
      ## @param promtail.service.ports.grpc Promtail gRPC service port
      ##
      ports:
        http: 3100
        grpc: 9095
      ## Node ports to expose
      ## NOTE: choose port between <30000-32767>
      ## @param promtail.service.nodePorts.http Node port for HTTP
      ##
      nodePorts:
        http: ""
      ## @param promtail.service.sessionAffinityConfig Additional settings for the sessionAffinity
      ## sessionAffinityConfig:
      ##   clientIP:
      ##     timeoutSeconds: 300
      ##
      sessionAffinityConfig: {}
      ## @param promtail.service.sessionAffinity Control where client requests go, to the same pod or round-robin
      ## Values: ClientIP or None
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/
      ##
      sessionAffinity: None
      ## @param promtail.service.clusterIP Promtail service Cluster IP
      ## e.g.:
      ## clusterIP: None
      ##
      clusterIP: ""
      ## @param promtail.service.loadBalancerIP Promtail service Load Balancer IP
      ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
      ##
      loadBalancerIP: ""
      ## @param promtail.service.loadBalancerSourceRanges Promtail service Load Balancer sources
      ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
      ## e.g:
      ## loadBalancerSourceRanges:
      ##   - 10.10.10.0/24
      ##
      loadBalancerSourceRanges: []
      ## @param promtail.service.externalTrafficPolicy Promtail service external traffic policy
      ## ref https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      ##
      externalTrafficPolicy: Cluster
      ## @param promtail.service.annotations Additional custom annotations for Promtail service
      ##
      annotations: {}
      ## @param promtail.service.extraPorts Extra ports to expose in the Promtail service
      ##
      extraPorts: []
    ## Network Policies
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
    ##
    networkPolicy:
      ## @param promtail.networkPolicy.enabled Specifies whether a NetworkPolicy should be created
      ##
      enabled: true
      ## @param promtail.networkPolicy.allowExternal Don't require server label for connections
      ## The Policy model to apply. When set to false, only pods with the correct
      ## server label will have network access to the ports server is listening
      ## on. When true, server will accept connections from any source
      ## (with the correct destination port).
      ##
      allowExternal: true
      ## @param promtail.networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
      ##
      allowExternalEgress: true
      ## @param promtail.networkPolicy.addExternalClientAccess Allow access from pods with client label set to "true". Ignored if `promtail.networkPolicy.allowExternal` is true.
      ##
      addExternalClientAccess: true
      ## @param promtail.networkPolicy.kubeAPIServerPorts [array] List of possible endpoints to kube-apiserver (limit to your cluster settings to increase security)
      ##
      kubeAPIServerPorts: [443, 6443, 8443]
      ## @param promtail.networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraIngress:
      ##   - ports:
      ##       - port: 1234
      ##     from:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      extraIngress: []
      ## @param promtail.networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
      ## e.g:
      ## extraEgress:
      ##   - ports:
      ##       - port: 1234
      ##     to:
      ##       - podSelector:
      ##           - matchLabels:
      ##               - role: frontend
      ##       - podSelector:
      ##           - matchExpressions:
      ##               - key: role
      ##                 operator: In
      ##                 values:
      ##                   - frontend
      ##
      extraEgress: []
      ## @param promtail.networkPolicy.ingressPodMatchLabels [object] Labels to match to allow traffic from other pods. Ignored if `promtail.networkPolicy.allowExternal` is true.
      ## e.g:
      ## ingressPodMatchLabels:
      ##   my-client: "true"
      #
      ingressPodMatchLabels: {}
      ## @param promtail.networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces. Ignored if `promtail.networkPolicy.allowExternal` is true.
      ## @param promtail.networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces. Ignored if `promtail.networkPolicy.allowExternal` is true.
      ##
      ingressNSMatchLabels: {}
      ingressNSPodMatchLabels: {}
    ## Create RBAC
    ## @param promtail.rbac.create Create RBAC rules
    ##
    rbac:
      create: true
    ## Service account for Loki to use
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      ## @param promtail.serviceAccount.create Enable creation of ServiceAccount for Promtail pods
      ##
      create: true
      ## @param promtail.serviceAccount.name The name of the ServiceAccount to use
      ## If not set and create is true, a name is generated using the common.names.fullname template
      ##
      name: ""
      ## @param promtail.serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the promtail.serviceAccount.created
      ## Can be set to false if pods using this promtail.serviceAccount.do not need to use K8s API
      ##
      automountServiceAccountToken: false
      ## @param promtail.serviceAccount.annotations Additional custom annotations for the ServiceAccount
      ##
      annotations: {}
  ## @section Init Container Parameters
  ##

  ## 'volumePermissions' init container parameters
  ## Changes the owner and group of the persistent volume mount point to runAsUser:fsGroup values
  ##   based on the *podSecurityContext/*containerSecurityContext parameters
  ##
  volumePermissions:
    ## @param volumePermissions.enabled Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`
    ##
    enabled: false
    ## OS Shell + Utility image
    ## ref: https://hub.docker.com/r/bitnami/os-shell/tags/
    ## @param volumePermissions.image.registry [default: REGISTRY_NAME] OS Shell + Utility image registry
    ## @param volumePermissions.image.repository [default: REPOSITORY_NAME/os-shell] OS Shell + Utility image repository
    ## @skip volumePermissions.image.tag OS Shell + Utility image tag (immutable tags are recommended)
    ## @param volumePermissions.image.digest OS Shell + Utility image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param volumePermissions.image.pullPolicy OS Shell + Utility image pull policy
    ## @param volumePermissions.image.pullSecrets OS Shell + Utility image pull secrets
    ##
    image:
      registry: docker.io
      repository: bitnami/os-shell
      tag: 12-debian-12-r30
      digest: ""
      pullPolicy: IfNotPresent
      ## Optionally specify an array of imagePullSecrets.
      ## Secrets must be manually created in the namespace.
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
      ## e.g:
      ## pullSecrets:
      ##   - myRegistryKeySecretName
      ##
      pullSecrets: []
    ## Init container's resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param volumePermissions.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if volumePermissions.resources is set (volumePermissions.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param volumePermissions.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
    ## Init container Container Security Context
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
    ## @param volumePermissions.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
    ## @param volumePermissions.containerSecurityContext.runAsUser Set init container's Security Context runAsUser
    ## @param volumePermissions.containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
    ## NOTE: when runAsUser is set to special value "auto", init container will try to chown the
    ##   data folder to auto-determined user&group, using commands: `id -u`:`id -G | cut -d" " -f2`
    ##   "auto" is especially useful for OpenShift which has scc with dynamic user ids (and 0 is not allowed)
    ##
    containerSecurityContext:
      seLinuxOptions: {}
      runAsUser: 0
      seccompProfile:
        type: "RuntimeDefault"
  ## @section Other Parameters
  ##

  ## Service account for Loki to use
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Enable creation of ServiceAccount for Loki pods
    ##
    create: true
    ## @param serviceAccount.name The name of the ServiceAccount to use
    ## If not set and create is true, a name is generated using the common.names.fullname template
    ##
    name: ""
    ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
    ## Can be set to false if pods using this serviceAccount do not need to use K8s API
    ##
    automountServiceAccountToken: false
    ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount
    ##
    annotations: {}
  ## @section Metrics Parameters
  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param metrics.enabled Enable metrics
    ##
    enabled: false
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using Prometheus Operator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.namespace Namespace for the ServiceMonitor Resource (defaults to the Release Namespace)
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped.
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""
      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
  ## @section External Memcached (Chunks) Parameters
  ##
  externalMemcachedChunks:
    ## @param externalMemcachedChunks.host Host of a running external memcached instance
    ##
    host: ""
    ## @param externalMemcachedChunks.port Port of a running external memcached instance
    ##
    port: 11211
  ## @section Memcached Sub-chart Parameters (Chunks)
  ## Memcached sub-chart (Chunks)
  ##
  memcachedchunks:
    ## @param memcachedchunks.enabled Deploy memcached sub-chart
    ##
    enabled: true
    ## @param memcachedchunks.nameOverride override the subchart name
    ##
    nameOverride: ""
    ## @param memcachedchunks.architecture Memcached architecture
    ##
    architecture: high-availability
    ## @param memcachedchunks.service.ports.memcached Memcached service port
    ##
    service:
      ports:
        memcached: 11211
    ## Memcached resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param memcachedchunks.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param memcachedchunks.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
  ## @section External Memcached (Frontend) Parameters
  ##
  externalMemcachedFrontend:
    ## @param externalMemcachedFrontend.host Host of a running external memcached instance
    ##
    host: ""
    ## @param externalMemcachedFrontend.port Port of a running external memcached instance
    ##
    port: 11211
  ## @section Memcached Sub-chart Parameters (Frontend)
  ## Memcached sub-chart (Frontend)
  ##
  memcachedfrontend:
    ## @param memcachedfrontend.enabled Deploy memcached sub-chart
    ##
    enabled: true
    ## @param memcachedfrontend.architecture Memcached architecture
    ##
    architecture: high-availability
    ## @param memcachedfrontend.nameOverride override the subchart name
    ##
    nameOverride: ""
    ## @param memcachedfrontend.service.ports.memcached Memcached service port
    ##
    service:
      ports:
        memcached: 11211
    ## Memcached resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param memcachedfrontend.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param memcachedfrontend.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
  ## @section External Memcached (Index-Queries) Parameters
  ##
  externalMemcachedIndexQueries:
    ## @param externalMemcachedIndexQueries.host Host of a running external memcached instance
    ##
    host: ""
    ## @param externalMemcachedIndexQueries.port Port of a running external memcached instance
    ##
    port: 11211
  ## @section Memcached Sub-chart Parameters (Index-Queries)
  ## Memcached sub-chart (Index-Queries)
  ##
  memcachedindexqueries:
    ## @param memcachedindexqueries.enabled Deploy memcached sub-chart
    ##
    enabled: false
    ## @param memcachedindexqueries.architecture Memcached architecture
    ##
    architecture: high-availability
    ## @param memcachedindexqueries.nameOverride override the subchart name
    ##
    nameOverride: ""
    ## @param memcachedindexqueries.service.ports.memcached Memcached service port
    ##
    service:
      ports:
        memcached: 11211
    ## Memcached resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param memcachedindexqueries.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param memcachedindexqueries.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}
  ## @section External Memcached (IndexWrites) Parameters
  ##
  externalMemcachedIndexWrites:
    ## @param externalMemcachedIndexWrites.host Host of a running external memcached instance
    ##
    host: ""
    ## @param externalMemcachedIndexWrites.port Port of a running external memcached instance
    ##
    port: 11211
  ## @section Memcached Sub-chart Parameters (Index-Writes)
  ## Memcached sub-chart (Index-Writes)
  ##
  memcachedindexwrites:
    ## @param memcachedindexwrites.enabled Deploy memcached sub-chart
    ##
    enabled: false
    ## @param memcachedindexwrites.architecture Memcached architecture
    ##
    architecture: high-availability
    ## @param memcachedindexwrites.nameOverride override the subchart name
    ##
    nameOverride: ""
    ## @param memcachedindexwrites.service.ports.memcached Memcached service port
    ##
    service:
      ports:
        memcached: 11211
    ## Memcached resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param memcachedindexwrites.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "nano"
    ## @param memcachedindexwrites.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: {}

# Promtail 
promtail:
  enabled: true
  # -- Overrides the chart's name
  nameOverride: null

  # -- Overrides the chart's computed fullname
  fullnameOverride: null

  global:
    # -- Allow parent charts to override registry hostname
    imageRegistry: ""
    # -- Allow parent charts to override registry credentials
    imagePullSecrets: []

  daemonset:
    # -- Deploys Promtail as a DaemonSet
    enabled: true
    autoscaling:
      # -- Creates a VerticalPodAutoscaler for the daemonset
      enabled: false

      # Recommender responsible for generating recommendation for the object.
      # List should be empty (then the default recommender will generate the recommendation)
      # or contain exactly one recommender.
      # recommenders:
      # - name: custom-recommender-performance

      # -- List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      controlledResources: []

      # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.
      # controlledValues: RequestsAndLimits

      # -- Defines the max allowed resources for the pod
      maxAllowed: {}
      # cpu: 200m
      # memory: 100Mi
      # -- Defines the min allowed resources for the pod
      minAllowed: {}
      # cpu: 200m
      # memory: 100Mi

      # updatePolicy:
        # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
        # minReplicas: 1
        # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
        # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
        # updateMode: Auto

  deployment:
    # -- Deploys Promtail as a Deployment
    enabled: false
    replicaCount: 1
    autoscaling:
      # -- Creates a HorizontalPodAutoscaler for the deployment
      enabled: false
      minReplicas: 1
      maxReplicas: 10
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage:
      # behavior: {}

    # -- Set deployment object update strategy
    strategy:
      type: RollingUpdate

  service:
    enabled: false
    # -- Labels for the service
    labels: {}
    # -- Annotations for the service
    annotations: {}

  secret:
    # -- Labels for the Secret
    labels: {}
    # -- Annotations for the Secret
    annotations: {}

  configmap:
    # -- If enabled, promtail config will be created as a ConfigMap instead of a secret
    enabled: false

  initContainer: []
    # # -- Specifies whether the init container for setting inotify max user instances is to be enabled
    # - name: init
    #   # -- Docker registry, image and tag for the init container image
    #   image: docker.io/busybox:1.33
    #   # -- Docker image pull policy for the init container image
    #   imagePullPolicy: IfNotPresent
    #   # -- The inotify max user instances to configure
    #   command:
    #     - sh
    #     - -c
    #     - sysctl -w fs.inotify.max_user_instances=128
    #   securityContext:
    #     privileged: true

  image:
    # -- The Docker registry
    registry: docker.io
    # -- Docker image repository
    repository: grafana/promtail
    # -- Overrides the image tag whose default is the chart's appVersion
    tag: ""
    # -- Docker image pull policy
    pullPolicy: IfNotPresent

  # -- Image pull secrets for Docker images
  imagePullSecrets: []

  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld

  # -- Controls whether the pod has the `hostNetwork` flag set.
  hostNetwork: null

  # -- Annotations for the DaemonSet
  annotations: {}

  # -- Number of old history to retain to allow rollback (If not set, default Kubernetes value is set to 10)
  # revisionHistoryLimit: 1

  # -- The update strategy for the DaemonSet
  updateStrategy: {}

  # -- Pod labels
  podLabels: {}

  # -- Pod annotations
  podAnnotations: {}
  #  prometheus.io/scrape: "true"
  #  prometheus.io/port: "http-metrics"

  # -- The name of the PriorityClass
  priorityClassName: null

  # -- Liveness probe
  livenessProbe: {}

  # -- Readiness probe
  # @default -- See `values.yaml`
  readinessProbe:
    failureThreshold: 5
    httpGet:
      path: "{{ printf `%s/ready` .Values.httpPathPrefix }}"
      port: http-metrics
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1

  # -- Resource requests and limits
  resources: {}
  #  limits:
  #    cpu: 200m
  #    memory: 128Mi
  #  requests:
  #    cpu: 100m
  #    memory: 128Mi

  # -- The security context for pods
  podSecurityContext:
    runAsUser: 0
    runAsGroup: 0

  # -- The security context for containers
  containerSecurityContext:
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false

  rbac:
    # -- Specifies whether RBAC resources are to be created
    create: true
    # -- Specifies whether a PodSecurityPolicy is to be created
    pspEnabled: false

  # -- The name of the Namespace to deploy
  # If not set, `.Release.Namespace` is used
  namespace: null

  serviceAccount:
    # -- Specifies whether a ServiceAccount should be created
    create: true
    # -- The name of the ServiceAccount to use.
    # If not set and `create` is true, a name is generated using the fullname template
    name: null
    # -- Image pull secrets for the service account
    imagePullSecrets: []
    # -- Annotations for the service account
    annotations: {}
    # -- Automatically mount a ServiceAccount's API credentials
    automountServiceAccountToken: true

  # -- Automatically mount API credentials for a particular Pod
  automountServiceAccountToken: true

  # -- Node selector for pods
  nodeSelector: {}

  # -- Affinity configuration for pods
  affinity: {}

  # -- Tolerations for pods. By default, pods will be scheduled on master/control-plane nodes.
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

  # -- Default volumes that are mounted into pods. In most cases, these should not be changed.
  # Use `extraVolumes`/`extraVolumeMounts` for additional custom volumes.
  # @default -- See `values.yaml`
  defaultVolumes:
    - name: run
      hostPath:
        path: /run/promtail
    - name: containers
      hostPath:
        path: /var/lib/docker/containers
    - name: pods
      hostPath:
        path: /var/log/pods

  # -- Default volume mounts. Corresponds to `volumes`.
  # @default -- See `values.yaml`
  defaultVolumeMounts:
    - name: run
      mountPath: /run/promtail
    - name: containers
      mountPath: /var/lib/docker/containers
      readOnly: true
    - name: pods
      mountPath: /var/log/pods
      readOnly: true

  # Extra volumes to be added in addition to those specified under `defaultVolumes`.
  extraVolumes: []

  # Extra volume mounts together. Corresponds to `extraVolumes`.
  extraVolumeMounts: []

  # Extra args for the Promtail container.
  extraArgs: []
  # -- Example:
  # -- extraArgs:
  # --   - -client.external-labels=hostname=$(HOSTNAME)

  # -- Extra environment variables. Set up tracing enviroment variables here if .Values.config.enableTracing is true.
  # Tracing currently only support configure via environment variables. See:
  # https://grafana.com/docs/loki/latest/clients/promtail/configuration/#tracing_config
  # https://www.jaegertracing.io/docs/1.16/client-features/
  extraEnv: []

  # -- Extra environment variables from secrets or configmaps
  extraEnvFrom: []

  # -- Configure enableServiceLinks in pod
  enableServiceLinks: true

  # ServiceMonitor configuration
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: false
    # -- Alternative namespace for ServiceMonitor resources
    namespace: null
    # -- Namespace selector for ServiceMonitor resources
    namespaceSelector: {}
    # -- ServiceMonitor annotations
    annotations: {}
    # -- Additional ServiceMonitor labels
    labels: {}
    # -- ServiceMonitor scrape interval
    interval: null
    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
    scrapeTimeout: null
    # -- ServiceMonitor relabel configs to apply to samples before scraping
    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    # (defines `relabel_configs`)
    relabelings: []
    # -- ServiceMonitor relabel configs to apply to samples as the last
    # step before ingestion
    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    # (defines `metric_relabel_configs`)
    metricRelabelings: []
    # -- ServiceMonitor will add labels from the service to the Prometheus metric
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec
    targetLabels: []
    # -- ServiceMonitor will use http by default, but you can pick https as well
    scheme: http
    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
    tlsConfig: null
    # -- Prometheus rules will be deployed for alerting purposes
    prometheusRule:
      enabled: false
      additionalLabels: {}
      # namespace:
      rules: []
      #  - alert: PromtailRequestErrors
      #    expr: 100 * sum(rate(promtail_request_duration_seconds_count{status_code=~"5..|failed"}[1m])) by (namespace, job, route, instance) / sum(rate(promtail_request_duration_seconds_count[1m])) by (namespace, job, route, instance) > 10
      #    for: 5m
      #    labels:
      #      severity: critical
      #    annotations:
      #      description: |
      #        The {{ $labels.job }} {{ $labels.route }} is experiencing
      #        {{ printf \"%.2f\" $value }} errors.
      #        VALUE = {{ $value }}
      #        LABELS = {{ $labels }}
      #      summary: Promtail request errors (instance {{ $labels.instance }})
      #  - alert: PromtailRequestLatency
      #    expr: histogram_quantile(0.99, sum(rate(promtail_request_duration_seconds_bucket[5m])) by (le)) > 1
      #    for: 5m
      #    labels:
      #      severity: critical
      #    annotations:
      #      summary: Promtail request latency (instance {{ $labels.instance }})
      #      description: |
      #        The {{ $labels.job }} {{ $labels.route }} is experiencing
      #        {{ printf \"%.2f\" $value }}s 99th percentile latency.
      #        VALUE = {{ $value }}
      #        LABELS = {{ $labels }}

  # Extra containers created as part of a Promtail Deployment resource
  # - spec for Container:
  #   https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core
  #
  # Note that the key is used as the `name` field, i.e. below will create a
  # container named `promtail-proxy`.
  extraContainers: {}
    # promtail-proxy:
    #   image: nginx
    #   ...

  # -- Configure additional ports and services. For each configured port, a corresponding service is created.
  # See values.yaml for details
  extraPorts: {}
  #  syslog:
  #    name: tcp-syslog
  #    annotations: {}
  #    labels: {}
  #    containerPort: 1514
  #    protocol: TCP
  #    service:
  #      type: ClusterIP
  #      clusterIP: null
  #      port: 1514
  #      externalIPs: []
  #      nodePort: null
  #      loadBalancerIP: null
  #      loadBalancerSourceRanges: []
  #      externalTrafficPolicy: null
  #    ingress:
  #      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
  #      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
  #      # ingressClassName: nginx
  #      # Values can be templated
  #      annotations: {}
  #        # kubernetes.io/ingress.class: nginx
  #        # kubernetes.io/tls-acme: "true"
  #      paths: "/"
  #      hosts:
  #        - chart-example.local
  #
  #      tls: []
  #      #  - secretName: chart-example-tls
  #      #    hosts:
  #      #      - chart-example.local


  # -- PodSecurityPolicy configuration.
  # @default -- See `values.yaml`
  podSecurityPolicy:
    privileged: true
    allowPrivilegeEscalation: true
    volumes:
      - 'secret'
      - 'hostPath'
      - 'downwardAPI'
    hostNetwork: false
    hostIPC: false
    hostPID: false
    runAsUser:
      rule: 'RunAsAny'
    seLinux:
      rule: 'RunAsAny'
    supplementalGroups:
      rule: 'RunAsAny'
    fsGroup:
      rule: 'RunAsAny'
    readOnlyRootFilesystem: true
    requiredDropCapabilities:
      - ALL

  # -- Section for crafting Promtails config file. The only directly relevant value is `config.file`
  # which is a templated string that references the other values and snippets below this key.
  # @default -- See `values.yaml`
  config:
    # -- Enable Promtail config from Helm chart
    # Set `configmap.enabled: true` and this to `false` to manage your own Promtail config
    # See default config in `values.yaml`
    enabled: true
    # -- The log level of the Promtail server
    # Must be reference in `config.file` to configure `server.log_level`
    # See default config in `values.yaml`
    logLevel: info
    # -- The log format of the Promtail server
    # Must be reference in `config.file` to configure `server.log_format`
    # Valid formats: `logfmt, json`
    # See default config in `values.yaml`
    logFormat: logfmt
    # -- The port of the Promtail server
    # Must be reference in `config.file` to configure `server.http_listen_port`
    # See default config in `values.yaml`
    serverPort: 3101
    # -- The config of clients of the Promtail server
    # Must be reference in `config.file` to configure `clients`
    # @default -- See `values.yaml`
    clients:
      - url: http://loki-gateway/loki/api/v1/push
    # -- Configures where Promtail will save it's positions file, to resume reading after restarts.
    # Must be referenced in `config.file` to configure `positions`
    positions:
      filename: /run/promtail/positions.yaml
    # -- The config to enable tracing
    enableTracing: false
    # -- A section of reusable snippets that can be reference in `config.file`.
    # Custom snippets may be added in order to reduce redundancy.
    # This is especially helpful when multiple `kubernetes_sd_configs` are use which usually have large parts in common.
    # @default -- See `values.yaml`
    snippets:
      pipelineStages:
        - cri: {}
        - replace:
            # SSN
            expression: '([0-9]{3}-[0-9]{2}-[0-9]{4})'
            replace: '*SSN*{{ .Value | Hash "salt" }}*'
        - replace:
            # IP4
            expression: '(\d{1,3}[.]\d{1,3}[.]\d{1,3}[.]\d{1,3})'
            replace: '*IP4*{{ .Value | Hash "salt" }}*'    
        - replace:
            # email
            expression: '([\w\.=-]+@[\w\.-]+\.[\w]{2,64})'
            replace: '*email*{{ .Value | Hash "salt" }}*'  
        - replace:
            # creditcard
            expression: '((?:\d[ -]*?){13,16})'
            replace: '*creditcard*{{ .Value | Hash "salt" }}*'
        - replace:
            # mobile number
            expression: '(\+?\d{1,3}[-.\s]??\d{5}[-.\s]?\d{5})'
            replace: '*mobile*{{ .Value | Hash "salt" }}*'
      common:
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_node_name
          target_label: node_name
        - action: replace
          source_labels:
            - __meta_kubernetes_namespace
          target_label: namespace
        - action: replace
          replacement: $1
          separator: /
          source_labels:
            - namespace
            - app
          target_label: job
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_name
          target_label: pod
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_container_name
          target_label: container
        - action: replace
          replacement: /var/log/pods/*$1/*.log
          separator: /
          source_labels:
            - __meta_kubernetes_pod_uid
            - __meta_kubernetes_pod_container_name
          target_label: __path__
        - action: replace
          replacement: /var/log/pods/*$1/*.log
          regex: true/(.*)
          separator: /
          source_labels:
            - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash
            - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash
            - __meta_kubernetes_pod_container_name
          target_label: __path__

      # If set to true, adds an additional label for the scrape job.
      # This helps debug the Promtail config.
      addScrapeJobLabel: false

      # -- You can put here any keys that will be directly added to the config file's 'limits_config' block.
      # @default -- empty
      extraLimitsConfig: ""

      # -- You can put here any keys that will be directly added to the config file's 'server' block.
      # @default -- empty
      extraServerConfigs: ""

      # -- You can put here any additional scrape configs you want to add to the config file.
      # @default -- empty
      extraScrapeConfigs: ""

      # -- You can put here any additional relabel_configs to "kubernetes-pods" job
      extraRelabelConfigs: []

      scrapeConfigs: |
        # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference
        - job_name: kubernetes-pods
          pipeline_stages:
            {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels:
                - __meta_kubernetes_pod_controller_name
              regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
              action: replace
              target_label: __tmp_controller_name
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - __meta_kubernetes_pod_label_app
                - __tmp_controller_name
                - __meta_kubernetes_pod_name
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: app
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_instance
                - __meta_kubernetes_pod_label_instance
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: instance
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_component
                - __meta_kubernetes_pod_label_component
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: component
            {{- if .Values.config.snippets.addScrapeJobLabel }}
            - replacement: kubernetes-pods
              target_label: scrape_job
            {{- end }}
            {{- toYaml .Values.config.snippets.common | nindent 4 }}
            {{- with .Values.config.snippets.extraRelabelConfigs }}
            {{- toYaml . | nindent 4 }}
            {{- end }}

    # -- Config file contents for Promtail.
    # Must be configured as string.
    # It is templated so it can be assembled from reusable snippets in order to avoid redundancy.
    # @default -- See `values.yaml`
    file: |
      server:
        log_level: {{ .Values.config.logLevel }}
        log_format: {{ .Values.config.logFormat }}
        http_listen_port: {{ .Values.config.serverPort }}
        {{- with .Values.httpPathPrefix }}
        http_path_prefix: {{ . }}
        {{- end }}
        {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}

      clients:
        {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}

      positions:
        {{- tpl (toYaml .Values.config.positions) . | nindent 2 }}

      scrape_configs:
        {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}
        {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}

      limits_config:
        {{- tpl .Values.config.snippets.extraLimitsConfig . | nindent 2 }}

      tracing:
        enabled: {{ .Values.config.enableTracing }}

  networkPolicy:
    # -- Specifies whether Network Policies should be created
    enabled: false
    metrics:
      # -- Specifies the Pods which are allowed to access the metrics port.
      # As this is cross-namespace communication, you also neeed the namespaceSelector.
      podSelector: {}
      # -- Specifies the namespaces which are allowed to access the metrics port
      namespaceSelector: {}
      # -- Specifies specific network CIDRs which are allowed to access the metrics port.
      # In case you use namespaceSelector, you also have to specify your kubelet networks here.
      # The metrics ports are also used for probes.
      cidrs: []
    k8sApi:
      # -- Specify the k8s API endpoint port
      port: 8443
      # -- Specifies specific network CIDRs you want to limit access to
      cidrs: []

  # -- Base path to server all API routes fro
  httpPathPrefix: ""

  sidecar:
    configReloader:
      enabled: false
      image:
        # -- The Docker registry for sidecar config-reloader
        registry: ghcr.io
        # -- Docker image repository for sidecar config-reloader
        repository: jimmidyson/configmap-reload
        # -- Docker image tag for sidecar config-reloader
        tag: v0.12.0
        # -- Docker image pull policy for sidecar config-reloader
        pullPolicy: IfNotPresent
      # Extra args for the config-reloader container.
      extraArgs: []
      # -- Extra environment variables for sidecar config-reloader
      extraEnv: []
      # -- Extra environment variables from secrets or configmaps for sidecar config-reloader
      extraEnvFrom: []
      # -- The security context for containers for sidecar config-reloader
      containerSecurityContext:
        readOnlyRootFilesystem: true
        capabilities:
          drop:
            - ALL
        allowPrivilegeEscalation: false
      # -- Readiness probe for sidecar config-reloader
      readinessProbe: {}
      # -- Liveness probe for sidecar config-reloader
      livenessProbe: {}
      # -- Resource requests and limits for sidecar config-reloader
      resources: {}
      #  limits:
      #    cpu: 200m
      #    memory: 128Mi
      #  requests:
      #    cpu: 100m
      #    memory: 128Mi
      config:
        # -- The port of the config-reloader server
        serverPort: 9533
      serviceMonitor:
        enabled: true

  # -- Extra K8s manifests to deploy
  extraObjects: []
    # - apiVersion: "kubernetes-client.io/v1"
    #   kind: ExternalSecret
    #   metadata:
    #     name: promtail-secrets
    #   spec:
    #     backendType: gcpSecretsManager
    #     data:
    #       - key: promtail-oauth2-creds
    #         name: client_secret

# Tempo
tempo-distributed:
  enabled: true
  global:
    image:
      # -- Overrides the Docker registry globally for all images, excluding enterprise.
      registry: docker.io
      # -- Optional list of imagePullSecrets for all images, excluding enterprise.
      # Names of existing secrets with private container registry credentials.
      # Ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
      # Example:
      # pullSecrets: [ my-dockerconfigjson-secret ]
      pullSecrets: []
    # -- Overrides the priorityClassName for all pods
    priorityClassName: null
    # -- configures cluster domain ("cluster.local" by default)
    clusterDomain: 'cluster.local'
    # -- configures DNS service name
    dnsService: 'kube-dns'
    # -- configures DNS service namespace
    dnsNamespace: 'kube-system'
    # -- Common environment variables to add to all pods directly managed by this chart.
    # scope: admin-api, compactor, distributor, enterprise-federation-frontend, gateway, ingester, memcached, metrics-generator, querier, query-frontend, tokengen
    extraEnv: []
  fullnameOverride: ''
  # fullnameOverride: tempo

  # -- Configuration is loaded from the secret called 'externalConfigSecretName'.
  # If 'useExternalConfig' is true, then the configuration is not generated, just
  # consumed.  Top level keys for `tempo.yaml` and `overrides.yaml` are to be
  # provided by the user.
  useExternalConfig: false

  # -- Defines what kind of object stores the configuration, a ConfigMap or a Secret.
  # In order to move sensitive information (such as credentials) from the ConfigMap/Secret to a more secure location (e.g. vault), it is possible to use [environment variables in the configuration](https://grafana.com/docs/mimir/latest/operators-guide/configuring/reference-configuration-parameters/#use-environment-variables-in-the-configuration).
  # Such environment variables can be then stored in a separate Secret and injected via the global.extraEnvFrom value. For details about environment injection from a Secret please see [Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#use-case-as-container-environment-variables).
  configStorageType: ConfigMap

  # -- Name of the Secret or ConfigMap that contains the configuration (used for naming even if config is internal).
  externalConfigSecretName: '{{ include "tempo.resourceName" (dict "ctx" . "component" "config") }}'

  # -- Name of the Secret or ConfigMap that contains the runtime configuration (used for naming even if config is internal).
  externalRuntimeConfigName: '{{ include "tempo.resourceName" (dict "ctx" . "component" "runtime") }}'

  # -- When 'useExternalConfig' is true, then changing 'externalConfigVersion' triggers restart of services - otherwise changes to the configuration cause a restart.
  externalConfigVersion: '0'

  # -- If true, Tempo will report anonymous usage data about the shape of a deployment to Grafana Labs
  reportingEnabled: true

  tempo:
    image:
      # -- The Docker registry
      registry: docker.io
      # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository
      repository: grafana/tempo
      # -- Overrides the image tag whose default is the chart's appVersion
      tag: null
      pullPolicy: IfNotPresent
    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 30
      timeoutSeconds: 1
    # -- Global labels for all tempo pods
    podLabels: {}
    # -- Common annotations for all pods
    podAnnotations: {}
    # -- SecurityContext holds container-level security attributes and common container settings
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
    # -- podSecurityContext holds pod-level security attributes and common container settings
    podSecurityContext:
      fsGroup: 1000
    # -- Structured tempo configuration
    structuredConfig: {}
    # -- Memberlist service configuration.
    memberlist:
      # -- Adds the appProtocol field to the memberlist service. This allows memberlist to work with istio protocol selection. Set the optional service protocol. Ex: "tcp", "http" or "https".
      appProtocol: null

  serviceAccount:
    # -- Specifies whether a ServiceAccount should be created
    create: true
    # -- The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name: null
    # -- Image pull secrets for the service account
    imagePullSecrets: []
    # -- Annotations for the service account
    annotations: {}
    automountServiceAccountToken: false

  rbac:
    # -- Specifies whether RBAC manifests should be created
    create: false
    # -- Specifies whether a PodSecurityPolicy should be created
    pspEnabled: false

  # Configuration for the ingester
  ingester:
    # -- Annotations for the ingester StatefulSet
    annotations: {}
    # -- Number of replicas for the ingester
    replicas: 3
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    initContainers: []
    autoscaling:
      # -- Enable autoscaling for the ingester. WARNING: Autoscaling ingesters can result in lost data. Only do this if you know what you're doing.
      enabled: false
      # -- Minimum autoscaling replicas for the ingester
      minReplicas: 2
      # -- Maximum autoscaling replicas for the ingester
      maxReplicas: 3
      # -- Autoscaling behavior configuration for the ingester
      behavior: {}
      # -- Target CPU utilisation percentage for the ingester
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the ingester
      targetMemoryUtilizationPercentage:
    image:
      # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`
      tag: null
    # -- The name of the PriorityClass for ingester pods
    priorityClassName: null
    # -- Labels for ingester pods
    podLabels: {}
    # -- Annotations for ingester pods
    podAnnotations: {}
    # -- Additional CLI args for the ingester
    extraArgs: []
    # -- Environment variables to add to the ingester pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the ingester pods
    extraEnvFrom: []
    # -- Resource requests and limits for the ingester
    resources: {}
    # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,
    # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring
    # all data and to successfully leave the member ring on shutdown.
    terminationGracePeriodSeconds: 300
    # -- topologySpread for ingester pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 6 }}
    # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Soft node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
              topologyKey: kubernetes.io/hostname
          - weight: 75
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Override Pod Disruption Budget maxUnavailable with a static value
    # maxUnavailable: 1
    # -- Node selector for ingester pods
    nodeSelector: {}
    # -- Tolerations for ingester pods
    tolerations: []
    # -- Extra volumes for ingester pods
    extraVolumeMounts: []
    # -- Extra volumes for ingester deployment
    extraVolumes: []
    persistence:
      # -- Enable creating PVCs which is required when using boltdb-shipper
      enabled: false
      # -- use emptyDir with ramdisk instead of PVC. **Please note that all data in ingester will be lost on pod restart**
      inMemory: false
      # -- Size of persistent or memory disk
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for ingester's persist volume claim
      annotations: {}
    config:
      # -- Number of copies of spans to store in the ingester ring
      replication_factor: 3
      # -- Amount of time a trace must be idle before flushing it to the wal.
      trace_idle_period: null
      # -- How often to sweep all tenants and move traces from live -> wal -> completed blocks.
      flush_check_period: null
      # -- Maximum size of a block before cutting it
      max_block_bytes: null
      # -- Maximum length of time before cutting a block
      max_block_duration: null
      # -- Duration to keep blocks in the ingester after they have been flushed
      complete_block_timeout: null
      # -- Flush all traces to backend when ingester is stopped
      flush_all_on_shutdown: false
    service:
      # -- Annotations for ingester service
      annotations: {}
    # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: null
    # -- EXPERIMENTAL Feature, disabled by default
    zoneAwareReplication:
      # -- Enable zone-aware replication for ingester
      enabled: false
      # -- Maximum number of ingesters that can be unavailable per zone during rollout
      maxUnavailable: 50
      # -- topologyKey to use in pod anti-affinity. If unset, no anti-affinity rules are generated. If set, the generated anti-affinity rule makes sure that pods from different zones do not mix.
      # E.g.: topologyKey: 'kubernetes.io/hostname'
      topologyKey: null
      # -- Zone definitions for ingester zones. Note: you have to redefine the whole list to change parts as YAML does not allow to modify parts of a list.
      zones:
        # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        - name: zone-a
          # -- nodeselector to restrict where pods of this zone can be placed. E.g.:
          # nodeSelector:
          #   topology.kubernetes.io/zone: zone-a
          nodeSelector: null
          # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)
          extraAffinity: {}
          # -- Ingester data Persistent Volume Storage Class
          # If defined, storageClassName: <storageClass>
          # If set to "-", then use `storageClassName: ""`, which disables dynamic provisioning
          # If undefined or set to null (the default), then fall back to the value of `ingester.persistentVolume.storageClass`.
          storageClass: null
        # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        - name: zone-b
          # -- nodeselector to restrict where pods of this zone can be placed. E.g.:
          # nodeSelector:
          #   topology.kubernetes.io/zone: zone-b
          nodeSelector: null
          # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)
          extraAffinity: {}
          # -- Ingester data Persistent Volume Storage Class
          # If defined, storageClassName: <storageClass>
          # If set to "-", then use `storageClassName: ""`, which disables dynamic provisioning
          # If undefined or set to null (the default), then fall back to the value of `ingester.persistentVolume.storageClass`.
          storageClass: null
        # -- Name of the zone, used in labels and selectors. Must follow Kubernetes naming restrictions: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
        - name: zone-c
          # -- nodeselector to restrict where pods of this zone can be placed. E.g.:
          # nodeSelector:
          #   topology.kubernetes.io/zone: zone-c
          nodeSelector: null
          # -- extraAffinity adds user defined custom affinity rules (merged with generated rules)
          extraAffinity: {}
          # -- Ingester data Persistent Volume Storage Class
          # If defined, storageClassName: <storageClass>
          # If set to "-", then use `storageClassName: ""`, which disables dynamic provisioning
          # If undefined or set to null (the default), then fall back to the value of `ingester.persistentVolume.storageClass`.
          storageClass: null

  # Configuration for the metrics-generator
  metricsGenerator:
    # -- Specifies whether a metrics-generator should be deployed
    enabled: false
    # -- Kind of deployment [StatefulSet/Deployment]
    kind: Deployment
    # -- Annotations for the metrics-generator StatefulSet
    annotations: {}
    # -- Number of replicas for the metrics-generator
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    initContainers: []
    image:
      # -- The Docker registry for the metrics-generator image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the metrics-generator image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the metrics-generator image. Overrides `tempo.image.tag`
      tag: null
    # -- The name of the PriorityClass for metrics-generator pods
    priorityClassName: null
    # -- Labels for metrics-generator pods
    podLabels: {}
    # -- Annotations for metrics-generator pods
    podAnnotations: {}
    # -- Additional CLI args for the metrics-generator
    extraArgs: []
    # -- Environment variables to add to the metrics-generator pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the metrics-generator pods
    extraEnvFrom: []
    # -- Resource requests and limits for the metrics-generator
    resources: {}
    # -- Grace period to allow the metrics-generator to shutdown before it is killed. Especially for the ingestor,
    # this must be increased. It must be long enough so metrics-generators can be gracefully shutdown flushing/transferring
    # all data and to successfully leave the member ring on shutdown.
    terminationGracePeriodSeconds: 300
    # -- topologySpread for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 6 }}
    # -- Affinity for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for metrics-generator pods
    nodeSelector: {}
    # -- Tolerations for metrics-generator pods
    tolerations: []
    # -- Persistence configuration for metrics-generator
    persistence:
      # -- Enable creating PVCs if you have kind set to StatefulSet. This disables using local disk or memory configured in walEmptyDir
      enabled: false
      size: 10Gi
      # -- Storage class to be used.
      # If defined, storageClassName: <storageClass>.
      # If set to "-", storageClassName: "", which disables dynamic provisioning.
      # If empty or set to null, no storageClassName spec is
      # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
      storageClass: null
      # -- Annotations for metrics generator PVCs
      annotations: {}
    # -- The EmptyDir location where the /var/tempo will be mounted on. Defaults to local disk, can be set to memory.
    walEmptyDir: {}
      ## Here shows how to configure 1Gi memory as emptyDir.
      ## Ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#emptydirvolumesource-v1-core
      # medium: "Memory"
      # sizeLimit: 1Gi
    # -- Extra volumes for metrics-generator pods
    extraVolumeMounts: []
    # -- Extra volumes for metrics-generator deployment
    extraVolumes: []
    # -- Default ports
    ports:
      - name: grpc
        port: 9095
        service: true
      - name: http-memberlist
        port: 7946
        service: false
      - name: http-metrics
        port: 3100
        service: true
    # -- More information on configuration: https://grafana.com/docs/tempo/latest/configuration/#metrics-generator
    config:
      registry:
        collection_interval: 15s
        external_labels: {}
        stale_duration: 15m
      processor:
        # -- For processors to be enabled and generate metrics, pass the names of the processors to overrides.metrics_generator_processors value like [service-graphs, span-metrics]
        service_graphs:
          # -- Additional dimensions to add to the metrics. Dimensions are searched for in the
          # -- resource and span attributes and are added to the metrics if present.
          dimensions: []
          histogram_buckets: [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8]
          max_items: 10000
          wait: 10s
          workers: 10
        span_metrics:
          # -- Additional dimensions to add to the metrics along with the default dimensions.
          # -- Dimensions are searched for in the resource and span attributes and are added to the metrics if present.
          dimensions: []
          histogram_buckets: [0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128, 0.256, 0.512, 1.02, 2.05, 4.10]
      storage:
        path: /var/tempo/wal
        wal:
        remote_write_flush_deadline: 1m
        # Whether to add X-Scope-OrgID header in remote write requests
        remote_write_add_org_id_header: true
        # -- A list of remote write endpoints.
        # -- https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
        remote_write: []
      # -- Used by the local blocks processor to store a wal for traces.
      traces_storage:
        path: /var/tempo/traces
      metrics_ingestion_time_range_slack: 30s
    service:
      # -- Annotations for Metrics Generator service
      annotations: {}
    # -- Adds the appProtocol field to the metricsGenerator service. This allows metricsGenerator to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: null

  # Configuration for the distributor
  distributor:
    # -- Number of replicas for the distributor
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the distributor
      enabled: false
      # -- Minimum autoscaling replicas for the distributor
      minReplicas: 1
      # -- Maximum autoscaling replicas for the distributor
      maxReplicas: 3
      # -- Autoscaling behavior configuration for the distributor
      behavior: {}
      # -- Target CPU utilisation percentage for the distributor
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the distributor
      targetMemoryUtilizationPercentage:
    image:
      # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`
      tag: null
    service:
      # -- Annotations for distributor service
      annotations: {}
      # -- Labels for distributor service
      labels: {}
      # -- Type of service for the distributor
      type: ClusterIP
      # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
      loadBalancerIP: ''
      # -- If type is LoadBalancer limit incoming traffic from IPs.
      loadBalancerSourceRanges: []
      # -- If type is LoadBalancer you can set it to 'Local' [preserve the client source IP](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)
      externalTrafficPolicy: null
    serviceDiscovery:
      # -- Annotations for distributorDiscovery service
      annotations: {}
      # -- Labels for distributorDiscovery service
      labels: {}
    # -- The name of the PriorityClass for distributor pods
    priorityClassName: null
    # -- Labels for distributor pods
    podLabels: {}
    # -- Annotations for distributor pods
    podAnnotations: {}
    # -- Additional CLI args for the distributor
    extraArgs: []
    # -- Environment variables to add to the distributor pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the distributor pods
    extraEnvFrom: []
    # -- Resource requests and limits for the distributor
    resources: {}
    # -- Grace period to allow the distributor to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- topologySpread for distributor pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 6 }}
    # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for distributor pods
    nodeSelector: {}
    # -- Tolerations for distributor pods
    tolerations: []
    # -- Extra volumes for distributor pods
    extraVolumeMounts: []
    # -- Extra volumes for distributor deployment
    extraVolumes: []
    config:
      # -- Enable to log every received trace id to help debug ingestion
      # -- WARNING: Deprecated. Use log_received_spans instead.
      log_received_traces: null
      # -- Enable to log every received span to help debug ingestion or calculate span error distributions using the logs
      log_received_spans:
        enabled: false
        include_all_attributes: false
        filter_by_status_error: false
      # -- Disables write extension with inactive ingesters
      extend_writes: null
    # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: null

  # Configuration for the compactor
  compactor:
    # -- Number of replicas for the compactor
    replicas: 1

    # -- Autoscaling configurations
    autoscaling:
      # -- Enable autoscaling for the compactor
      enabled: false
      # -- Minimum autoscaling replicas for the compactor
      minReplicas: 1
      # -- Maximum autoscaling replicas for the compactor
      maxReplicas: 3
      # -- Autoscaling via HPA object
      hpa:
        enabled: false
        # -- Autoscaling behavior configuration for the compactor
        behavior: {}
        # -- Target CPU utilisation percentage for the compactor
        targetCPUUtilizationPercentage: 100
        # -- Target memory utilisation percentage for the compactor
        targetMemoryUtilizationPercentage:
      # -- Autoscaling via keda/ScaledObject
      keda:
        # requires https://keda.sh/
        enabled: false
        # -- List of autoscaling triggers for the compactor
        triggers: []
        # - type: prometheus
        #   metadata:
        #     serverAddress: "http://<prometheus-host>:9090"
        #     threshold: "250"
        #     query: |-
        #       sum by (cluster, namespace, tenant) (
        #         tempodb_compaction_outstanding_blocks{container="compactor", namespace=~".*"}
        #       ) /
        #       ignoring(tenant) group_left count by (cluster, namespace)(
        #         tempo_build_info{container="compactor", namespace=~".*"}
        #       )

    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the compactor image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the compactor image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the compactor image. Overrides `tempo.image.tag`
      tag: null
    # -- The name of the PriorityClass for compactor pods
    priorityClassName: null
    # -- Labels for compactor pods
    podLabels: {}
    # -- Annotations for compactor pods
    podAnnotations: {}
    # -- Additional CLI args for the compactor
    extraArgs: []
    # -- Environment variables to add to the compactor pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the compactor pods
    extraEnvFrom: []
    # -- Resource requests and limits for the compactor
    resources: {}
    # -- Grace period to allow the compactor to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for compactor pods
    nodeSelector: {}
    # -- Tolerations for compactor pods
    tolerations: []
    # -- Extra volumes for compactor pods
    extraVolumeMounts: []
    # -- Extra volumes for compactor deployment
    extraVolumes: []
    config:
      compaction:
        # -- Duration to keep blocks
        block_retention: 48h
        # Duration to keep blocks that have been compacted elsewhere
        compacted_block_retention: 1h
        # -- Blocks in this time window will be compacted together
        compaction_window: 1h
        # -- Amount of data to buffer from input blocks
        v2_in_buffer_bytes: 5242880
        # -- Flush data to backend when buffer is this large
        v2_out_buffer_bytes: 20971520
        # -- Maximum number of traces in a compacted block. WARNING: Deprecated. Use max_block_bytes instead.
        max_compaction_objects: 6000000
        # -- Maximum size of a compacted block in bytes
        max_block_bytes: 107374182400
        # -- Number of tenants to process in parallel during retention
        retention_concurrency: 10
        # -- Number of traces to buffer in memory during compaction
        v2_prefetch_traces_count: 1000
        # -- The maximum amount of time to spend compacting a single tenant before moving to the next
        max_time_per_tenant: 5m
        # -- The time between compaction cycles
        compaction_cycle: 30s
    service:
      # -- Annotations for compactor service
      annotations: {}
    dnsConfigOverides:
      enabled: false
      dnsConfig:
        options:
          - name: ndots
            value: "3"    # This is required for Azure Kubernetes Service (AKS) https://github.com/grafana/tempo/issues/1462

  # Configuration for the querier
  querier:
    # -- Number of replicas for the querier
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the querier
      enabled: false
      # -- Minimum autoscaling replicas for the querier
      minReplicas: 1
      # -- Maximum autoscaling replicas for the querier
      maxReplicas: 3
      # -- Autoscaling behavior configuration for the querier
      behavior: {}
      # -- Target CPU utilisation percentage for the querier
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the querier
      targetMemoryUtilizationPercentage:
    image:
      # -- The Docker registry for the querier image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the querier image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the querier image. Overrides `tempo.image.tag`
      tag: null
    # -- The name of the PriorityClass for querier pods
    priorityClassName: null
    # -- Labels for querier pods
    podLabels: {}
    # -- Annotations for querier pods
    podAnnotations: {}
    # -- Additional CLI args for the querier
    extraArgs: []
    # -- Environment variables to add to the querier pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the querier pods
    extraEnvFrom: []
    # -- Resource requests and limits for the querier
    resources: {}
    # -- Grace period to allow the querier to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- topologySpread for querier pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier") | nindent 6 }}
    # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for querier pods
    nodeSelector: {}
    # -- Tolerations for querier pods
    tolerations: []
    # -- Extra volumes for querier pods
    extraVolumeMounts: []
    # -- Extra volumes for querier deployment
    extraVolumes: []
    config:
      frontend_worker:
        # -- grpc client configuration
        grpc_client_config: {}
      trace_by_id:
        # -- Timeout for trace lookup requests
        query_timeout: 10s
      search:
        # -- Timeout for search requests
        query_timeout: 30s
        # -- If search_external_endpoints is set then the querier will primarily act as a proxy for whatever serverless backend you have configured. This setting allows the operator to have the querier prefer itself for a configurable number of subqueries.
        prefer_self: 10
        # -- If set to a non-zero value a second request will be issued at the provided duration. Recommended to be set to p99 of external search requests to reduce long tail latency.
        external_hedge_requests_at: 8s
        # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set.
        external_hedge_requests_up_to: 2
        # -- A list of external endpoints that the querier will use to offload backend search requests
        external_endpoints: []
        # -- The serverless backend to use. The default value of "" omits
        # -- credentials when querying the external backend.
        external_backend: ""
        # -- Google Cloud Run configuration. Will be used only if the value of
        # -- external_backend is "google_cloud_run".
        google_cloud_run: {}
      # -- This value controls the overall number of simultaneous subqueries that the querier will service at once. It does not distinguish between the types of queries.
      max_concurrent_queries: 20

    service:
      # -- Annotations for querier service
      annotations: {}
    # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: null

  # Configuration for the query-frontend
  queryFrontend:
    query:
      # -- Required for grafana version <7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch
      enabled: false
      image:
        # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`
        registry: null
        # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
        pullSecrets: []
        # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`
        repository: grafana/tempo-query
        # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`
        tag: null
      # -- Resource requests and limits for the query
      resources: {}
      # -- Additional CLI args for tempo-query pods
      extraArgs: []
      # -- Environment variables to add to the tempo-query pods
      extraEnv: []
      # -- Environment variables from secrets or configmaps to add to the tempo-query pods
      extraEnvFrom: []
      # -- Extra volumes for tempo-query pods
      extraVolumeMounts: []
      # -- Extra volumes for tempo-query deployment
      extraVolumes: []
      config: |
        backend: 127.0.0.1:3100
    # -- Number of replicas for the query-frontend
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    config:
      # -- Maximum number of outstanding requests per tenant per frontend; requests beyond this error with HTTP 429.
      max_outstanding_per_tenant: 2000
      # -- Number of times to retry a request sent to a querier
      max_retries: 2
      search:
        # -- The number of concurrent jobs to execute when searching the backend
        concurrent_jobs: 1000
        # -- The target number of bytes for each job to handle when performing a backend search
        target_bytes_per_job: 104857600
      # -- Trace by ID lookup configuration
      trace_by_id:
        # -- The number of shards to split a trace by id query into.
        query_shards: 50
    autoscaling:
      # -- Enable autoscaling for the query-frontend
      enabled: false
      # -- Minimum autoscaling replicas for the query-frontend
      minReplicas: 1
      # -- Maximum autoscaling replicas for the query-frontend
      maxReplicas: 3
      # -- Autoscaling behavior configuration for the query-frontend
      behavior: {}
      # -- Target CPU utilisation percentage for the query-frontend
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the query-frontend
      targetMemoryUtilizationPercentage:
    image:
      # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`
      tag: null
    service:
      # -- Port of the query-frontend service
      port: 16686
      # -- Annotations for queryFrontend service
      annotations: {}
      # -- Labels for queryFrontend service
      labels: {}
      # -- Type of service for the queryFrontend
      type: ClusterIP
      # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
      loadBalancerIP: ""
      # -- If type is LoadBalancer limit incoming traffic from IPs.
      loadBalancerSourceRanges: []
    serviceDiscovery:
      # -- Annotations for queryFrontendDiscovery service
      annotations: {}
      # -- Labels for queryFrontendDiscovery service
      labels: {}
    ingress:
      # -- Specifies whether an ingress for the Jaeger should be created
      enabled: false
      # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
      # ingressClassName: nginx
      # -- Annotations for the Jaeger ingress
      annotations: {}
      # -- Hosts configuration for the Jaeger ingress
      hosts:
        - host: query.tempo.example.com
          paths:
            - path: /
              # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
              # pathType: Prefix
      # -- TLS configuration for the Jaeger ingress
      tls:
        - secretName: tempo-query-tls
          hosts:
            - query.tempo.example.com
    # -- The name of the PriorityClass for query-frontend pods
    priorityClassName: null
    # -- Labels for queryFrontend pods
    podLabels: {}
    # -- Annotations for query-frontend pods
    podAnnotations: {}
    # -- Additional CLI args for the query-frontend
    extraArgs: []
    # -- Environment variables to add to the query-frontend pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the query-frontend pods
    extraEnvFrom: []
    # -- Resource requests and limits for the query-frontend
    resources: {}
    # -- Grace period to allow the query-frontend to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- topologySpread for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 6 }}
    # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for query-frontend pods
    nodeSelector: {}
    # -- Tolerations for query-frontend pods
    tolerations: []
    # -- Extra volumes for query-frontend pods
    extraVolumeMounts: []
    # -- Extra volumes for query-frontend deployment
    extraVolumes: []
    # -- Adds the appProtocol field to the queryFrontend service. This allows queryFrontend to work with istio protocol selection.
    appProtocol:
      # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
      grpc: null

  # Configuration for the federation-frontend
  # Can only be enabled if enterprise.enabled is true - requires license.
  enterpriseFederationFrontend:
    # -- Specifies whether a federation-frontend should be deployed
    enabled: false
    # -- Number of replicas for the federation-frontend
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    # proxy_targets:
    #   - name: own-data-center
    #     url: http://get/tempo
    #   - name: grafana-cloud
    #     url: https://tempo-us-central1.grafana.net/tempo
    #     basic_auth:
    #       username: <instance-id>
    #       password: <token>
    proxy_targets: []
    autoscaling:
      # -- Enable autoscaling for the federation-frontend
      enabled: false
      # -- Minimum autoscaling replicas for the federation-frontend
      minReplicas: 1
      # -- Maximum autoscaling replicas for the federation-frontend
      maxReplicas: 3
      # -- Target CPU utilisation percentage for the federation-frontend
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the federation-frontend
      targetMemoryUtilizationPercentage:
    image:
      # -- The Docker registry for the federation-frontend image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the federation-frontend image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the federation-frontend image. Overrides `tempo.image.tag`
      tag: null
    service:
      # -- Port of the federation-frontend service
      port: 3100
      # -- Annotations for enterpriseFederationFrontend service
      annotations: {}
      # -- Type of service for the enterpriseFederationFrontend
      type: ClusterIP
      # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
      loadBalancerIP: ""
      # -- If type is LoadBalancer limit incoming traffic from IPs.
      loadBalancerSourceRanges: []
    # -- The name of the PriorityClass for federation-frontend pods
    priorityClassName: null
    # -- Labels for enterpriseFederationFrontend pods
    podLabels: {}
    # -- Annotations for federation-frontend pods
    podAnnotations: {}
    # -- Additional CLI args for the federation-frontend
    extraArgs: []
    # -- Environment variables to add to the federation-frontend pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the federation-frontend pods
    extraEnvFrom: []
    # -- Resource requests and limits for the federation-frontend
    resources: {}
    # -- Grace period to allow the federation-frontend to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- topologySpread for federation-frontend pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: failure-domain.beta.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "federation-frontend") | nindent 6 }}
    # -- Affinity for federation-frontend pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "federation-frontend") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "federation-frontend") | nindent 12 }}
              topologyKey: failure-domain.beta.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for federation-frontend pods
    nodeSelector: {}
    # -- Tolerations for federation-frontend pods
    tolerations: []
    # -- Extra volumes for federation-frontend pods
    extraVolumeMounts: []
    # -- Extra volumes for federation-frontend deployment
    extraVolumes: []

  multitenancyEnabled: false

  rollout_operator:
  # -- Enable rollout-operator. It must be enabled when using Zone Aware Replication.
    enabled: false

    podSecurityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
      seccompProfile:
        type: RuntimeDefault

    # Set the container security context
    securityContext:
      readOnlyRootFilesystem: true
      capabilities:
        drop: [ALL]
      allowPrivilegeEscalation: false

  traces:
    jaeger:
      grpc:
        # -- Enable Tempo to ingest Jaeger GRPC traces
        enabled: false
        # -- Jaeger GRPC receiver config
        receiverConfig: {}
      thriftBinary:
        # -- Enable Tempo to ingest Jaeger Thrift Binary traces
        enabled: false
        # -- Jaeger Thrift Binary receiver config
        receiverConfig: {}
      thriftCompact:
        # -- Enable Tempo to ingest Jaeger Thrift Compact traces
        enabled: false
        # -- Jaeger Thrift Compact receiver config
        receiverConfig: {}
      thriftHttp:
        # -- Enable Tempo to ingest Jaeger Thrift HTTP traces
        enabled: false
        # -- Jaeger Thrift HTTP receiver config
        receiverConfig: {}
    zipkin:
      # -- Enable Tempo to ingest Zipkin traces
      enabled: false
      # -- Zipkin receiver config
      receiverConfig: {}
    otlp:
      http:
        # -- Enable Tempo to ingest Open Telemetry HTTP traces
        enabled: false
        # -- HTTP receiver advanced config
        receiverConfig: {}
      grpc:
        # -- Enable Tempo to ingest Open Telemetry GRPC traces
        enabled: false
        # -- GRPC receiver advanced config
        receiverConfig: {}
    opencensus:
      # -- Enable Tempo to ingest Open Census traces
      enabled: false
      # -- Open Census receiver config
      receiverConfig: {}
    # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver
    kafka: {}

  # -- Memberlist configuration. Please refer to https://grafana.com/docs/tempo/latest/configuration/#memberlist
  memberlist:
    node_name: ""
    randomize_node_name: true
    stream_timeout: "10s"
    retransmit_factor: 2
    pull_push_interval: "30s"
    gossip_interval: "1s"
    gossip_nodes: 2
    gossip_to_dead_nodes_time: "30s"
    min_join_backoff: "1s"
    max_join_backoff: "1m"
    max_join_retries: 10
    abort_if_cluster_join_fails: false
    rejoin_interval: "0s"
    left_ingesters_timeout: "5m"
    leave_timeout: "5s"
    bind_addr: []
    bind_port: 7946
    packet_dial_timeout: "5s"
    packet_write_timeout: "5s"

  # -- Config file contents for Tempo distributed. Passed through the `tpl` function to allow templating

  # @default -- See values.yaml
  config: |
    multitenancy_enabled: {{ .Values.multitenancyEnabled }}

    usage_report:
      reporting_enabled: {{ .Values.reportingEnabled }}

    {{- if .Values.enterprise.enabled }}
    license:
      path: "/license/license.jwt"

    admin_api:
      leader_election:
        enabled: true
        ring:
          kvstore:
            store: "memberlist"

    auth:
      type: enterprise

    http_api_prefix: {{get .Values.tempo.structuredConfig "http_api_prefix"}}

    admin_client:
      storage:
        backend: {{.Values.storage.admin.backend}}
        {{- if eq .Values.storage.admin.backend "s3"}}
        s3:
          {{- toYaml .Values.storage.admin.s3 | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "gcs"}}
        gcs:
          {{- toYaml .Values.storage.admin.gcs | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "azure"}}
        azure:
          {{- toYaml .Values.storage.admin.azure | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "swift"}}
        swift:
          {{- toYaml .Values.storage.admin.swift | nindent 6}}
        {{- end}}
        {{- if eq .Values.storage.admin.backend "filesystem"}}
        filesystem:
          {{- toYaml .Values.storage.admin.filesystem | nindent 6}}
        {{- end}}
    {{- end }}

    {{- if and .Values.enterprise.enabled .Values.enterpriseGateway.useDefaultProxyURLs }}
    gateway:
      proxy:
        admin_api:
          url: http://{{ template "tempo.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        compactor:
          url: http://{{ template "tempo.fullname" . }}-compactor.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        default:
          url: http://{{ template "tempo.fullname" . }}-admin-api.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        distributor:
          url: http://{{ template "tempo.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
          otlp/grpc:
            url: h2c://{{ template "tempo.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:4317
          otlp/http:
            url: http://{{ template "tempo.fullname" . }}-distributor.{{ .Release.Namespace }}.svc:4318
        ingester:
          url: http://{{ template "tempo.fullname" . }}-ingester.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        querier:
          url: http://{{ template "tempo.fullname" . }}-querier.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}
        query_frontend:
          url: http://{{ template "tempo.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc:{{ include "tempo.serverHttpListenPort" . }}{{get .Values.tempo.structuredConfig "http_api_prefix"}}
    {{else}}
    {{- if and .Values.enterprise.enabled .Values.enterpriseGateway.proxy }}
    gateway:
      proxy: {{- toYaml .Values.enterpriseGateway.proxy | nindent 6 }}
    {{- end }}
    {{- end }}

    compactor:
      compaction:
        block_retention: {{ .Values.compactor.config.compaction.block_retention }}
        compacted_block_retention: {{ .Values.compactor.config.compaction.compacted_block_retention }}
        compaction_window: {{ .Values.compactor.config.compaction.compaction_window }}
        v2_in_buffer_bytes: {{ .Values.compactor.config.compaction.v2_in_buffer_bytes }}
        v2_out_buffer_bytes: {{ .Values.compactor.config.compaction.v2_out_buffer_bytes }}
        max_compaction_objects: {{ .Values.compactor.config.compaction.max_compaction_objects }}
        max_block_bytes: {{ .Values.compactor.config.compaction.max_block_bytes }}
        retention_concurrency: {{ .Values.compactor.config.compaction.retention_concurrency }}
        v2_prefetch_traces_count: {{ .Values.compactor.config.compaction.v2_prefetch_traces_count }}
        max_time_per_tenant: {{ .Values.compactor.config.compaction.max_time_per_tenant }}
        compaction_cycle: {{ .Values.compactor.config.compaction.compaction_cycle }}
      ring:
        kvstore:
          store: memberlist
    {{- if and .Values.enterprise.enabled .Values.enterpriseFederationFrontend.enabled }}
    federation:
      proxy_targets:
        {{- toYaml .Values.enterpriseFederationFrontend.proxy_targets | nindent 6 }}
    {{- end }}
    {{- if .Values.metricsGenerator.enabled }}
    metrics_generator:
      ring:
        kvstore:
          store: memberlist
      processor:
        {{- toYaml .Values.metricsGenerator.config.processor | nindent 6 }}
      storage:
        {{- toYaml .Values.metricsGenerator.config.storage | nindent 6 }}
      traces_storage:
        {{- toYaml .Values.metricsGenerator.config.traces_storage | nindent 6 }}
      registry:
        {{- toYaml .Values.metricsGenerator.config.registry | nindent 6 }}
      metrics_ingestion_time_range_slack: {{ .Values.metricsGenerator.config.metrics_ingestion_time_range_slack }}
    {{- end }}
    distributor:
      ring:
        kvstore:
          store: memberlist
      receivers:
        {{- if  or (.Values.traces.jaeger.thriftCompact.enabled) (.Values.traces.jaeger.thriftBinary.enabled) (.Values.traces.jaeger.thriftHttp.enabled) (.Values.traces.jaeger.grpc.enabled) }}
        jaeger:
          protocols:
            {{- if .Values.traces.jaeger.thriftCompact.enabled }}
            thrift_compact:
              {{- $mergedJaegerThriftCompactConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:6831") .Values.traces.jaeger.thriftCompact.receiverConfig }}
              {{- toYaml $mergedJaegerThriftCompactConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.jaeger.thriftBinary.enabled }}
            thrift_binary:
              {{- $mergedJaegerThriftBinaryConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:6832") .Values.traces.jaeger.thriftBinary.receiverConfig }}
              {{- toYaml $mergedJaegerThriftBinaryConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.jaeger.thriftHttp.enabled }}
            thrift_http:
              {{- $mergedJaegerThriftHttpConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:14268") .Values.traces.jaeger.thriftHttp.receiverConfig }}
              {{- toYaml $mergedJaegerThriftHttpConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.jaeger.grpc.enabled }}
            grpc:
              {{- $mergedJaegerGrpcConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:14250") .Values.traces.jaeger.grpc.receiverConfig }}
              {{- toYaml $mergedJaegerGrpcConfig | nindent 10 }}
            {{- end }}
        {{- end }}
        {{- if .Values.traces.zipkin.enabled }}
        zipkin:
          {{- $mergedZipkinReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:9411") .Values.traces.zipkin.receiverConfig }}
          {{- toYaml $mergedZipkinReceiverConfig | nindent 6 }}
        {{- end }}
        {{- if or (.Values.traces.otlp.http.enabled) (.Values.traces.otlp.grpc.enabled) }}
        otlp:
          protocols:
            {{- if .Values.traces.otlp.http.enabled }}
            http:
              {{- $mergedOtlpHttpReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:4318") .Values.traces.otlp.http.receiverConfig }}
              {{- toYaml $mergedOtlpHttpReceiverConfig | nindent 10 }}
            {{- end }}
            {{- if .Values.traces.otlp.grpc.enabled }}
            grpc:
              {{- $mergedOtlpGrpcReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:4317") .Values.traces.otlp.grpc.receiverConfig }}
              {{- toYaml $mergedOtlpGrpcReceiverConfig | nindent 10 }}
            {{- end }}
        {{- end }}
        {{- if .Values.traces.opencensus.enabled }}
        opencensus:
          {{- $mergedOpencensusReceiverConfig := mustMergeOverwrite (dict "endpoint" "0.0.0.0:55678") .Values.traces.opencensus.receiverConfig }}
          {{- toYaml $mergedOpencensusReceiverConfig | nindent 6 }}
        {{- end }}
        {{- if .Values.traces.kafka }}
        kafka:
          {{- toYaml .Values.traces.kafka | nindent 6 }}
        {{- end }}
      {{- if or .Values.distributor.config.log_received_traces .Values.distributor.config.log_received_spans.enabled }}
      log_received_spans:
        enabled: {{ or .Values.distributor.config.log_received_traces .Values.distributor.config.log_received_spans.enabled }}
        include_all_attributes: {{ .Values.distributor.config.log_received_spans.include_all_attributes }}
        filter_by_status_error: {{ .Values.distributor.config.log_received_spans.filter_by_status_error }}
      {{- end }}
      {{- if .Values.distributor.config.extend_writes }}
      extend_writes: {{ .Values.distributor.config.extend_writes }}
      {{- end }}
    querier:
      frontend_worker:
        frontend_address: {{ include "tempo.resourceName" (dict "ctx" . "component" "query-frontend-discovery") }}:9095
        {{- if .Values.querier.config.frontend_worker.grpc_client_config }}
        grpc_client_config:
          {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}
        {{- end }}
      trace_by_id:
        query_timeout: {{ .Values.querier.config.trace_by_id.query_timeout }}
      search:
        external_endpoints: {{- toYaml .Values.querier.config.search.external_endpoints | nindent 6 }}
        query_timeout: {{ .Values.querier.config.search.query_timeout }}
        prefer_self: {{ .Values.querier.config.search.prefer_self }}
        external_hedge_requests_at: {{ .Values.querier.config.search.external_hedge_requests_at }}
        external_hedge_requests_up_to: {{ .Values.querier.config.search.external_hedge_requests_up_to }}
        external_backend: {{ .Values.querier.config.search.external_backend }}
        {{- if .Values.querier.config.search.google_cloud_run }}
        google_cloud_run:
          {{- toYaml .Values.querier.config.search.google_cloud_run | nindent 6 }}
        {{- end }}
      max_concurrent_queries: {{ .Values.querier.config.max_concurrent_queries }}
    query_frontend:
      max_outstanding_per_tenant: {{ .Values.queryFrontend.config.max_outstanding_per_tenant }}
      max_retries: {{ .Values.queryFrontend.config.max_retries }}
      search:
        target_bytes_per_job: {{ .Values.queryFrontend.config.search.target_bytes_per_job }}
        concurrent_jobs: {{ .Values.queryFrontend.config.search.concurrent_jobs }}
      trace_by_id:
        query_shards: {{ .Values.queryFrontend.config.trace_by_id.query_shards }}

    ingester:
      lifecycler:
        ring:
          replication_factor: {{ .Values.ingester.config.replication_factor }}
          {{- if .Values.ingester.zoneAwareReplication.enabled }}
          zone_awareness_enabled: true
          {{- end }}
          kvstore:
            store: memberlist
        tokens_file_path: /var/tempo/tokens.json
      {{- if .Values.ingester.config.trace_idle_period }}
      trace_idle_period: {{ .Values.ingester.config.trace_idle_period }}
      {{- end }}
      {{- if .Values.ingester.config.flush_check_period }}
      flush_check_period: {{ .Values.ingester.config.flush_check_period }}
      {{- end }}
      {{- if .Values.ingester.config.max_block_bytes }}
      max_block_bytes: {{ .Values.ingester.config.max_block_bytes }}
      {{- end }}
      {{- if .Values.ingester.config.max_block_duration }}
      max_block_duration: {{ .Values.ingester.config.max_block_duration }}
      {{- end }}
      {{- if .Values.ingester.config.complete_block_timeout }}
      complete_block_timeout: {{ .Values.ingester.config.complete_block_timeout }}
      {{- end }}
      {{- if .Values.ingester.config.flush_all_on_shutdown }}
      flush_all_on_shutdown: {{ .Values.ingester.config.flush_all_on_shutdown }}
      {{- end }}
    memberlist:
      {{- with .Values.memberlist }}
        {{- toYaml . | nindent 2 }}
      {{- end }}
      join_members:
        - dns+{{ include "tempo.fullname" . }}-gossip-ring:{{ .Values.memberlist.bind_port }}
    overrides:
      {{- toYaml .Values.global_overrides | nindent 2 }}
    server:
      http_listen_port: {{ .Values.server.httpListenPort }}
      log_level: {{ .Values.server.logLevel }}
      log_format: {{ .Values.server.logFormat }}
      grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}
      grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}
      http_server_read_timeout: {{ .Values.server.http_server_read_timeout }}
      http_server_write_timeout: {{ .Values.server.http_server_write_timeout }}
    cache:
    {{- toYaml .Values.cache | nindent 2}}
    storage:
      trace:
        {{- if .Values.storage.trace.block.version }}
        block:
          version: {{.Values.storage.trace.block.version}}
          {{- if .Values.storage.trace.block.dedicated_columns}}
          parquet_dedicated_columns:
            {{ .Values.storage.trace.block.dedicated_columns | toYaml | nindent 8}}
          {{- end }}
        {{- end }}
        pool:
          max_workers: {{ .Values.storage.trace.pool.max_workers }}
          queue_depth: {{ .Values.storage.trace.pool.queue_depth }}
        backend: {{.Values.storage.trace.backend}}
        {{- if eq .Values.storage.trace.backend "s3"}}
        s3:
          {{- toYaml .Values.storage.trace.s3 | nindent 6}}
        {{- end }}
        {{- if eq .Values.storage.trace.backend "gcs"}}
        gcs:
          {{- toYaml .Values.storage.trace.gcs | nindent 6}}
        {{- end }}
        {{- if eq .Values.storage.trace.backend "azure"}}
        azure:
          {{- toYaml .Values.storage.trace.azure | nindent 6}}
        {{- end }}
        blocklist_poll: 5m
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal

  # Set Tempo server configuration
  # Refers to https://grafana.com/docs/tempo/latest/configuration/#server
  server:
    # --  HTTP server listen host
    httpListenPort: 3100
    # -- Log level. Can be set to trace, debug, info (default), warn, error, fatal, panic
    logLevel: info
    # -- Log format. Can be set to logfmt (default) or json.
    logFormat: logfmt
    # -- Max gRPC message size that can be received
    grpc_server_max_recv_msg_size: 4194304
    # -- Max gRPC message size that can be sent
    grpc_server_max_send_msg_size: 4194304
    # -- Read timeout for HTTP server
    http_server_read_timeout: 30s
    # -- Write timeout for HTTP server
    http_server_write_timeout: 30s

  # Use this block to configure caches available throughout the application.
  # Multiple caches can be created and assigned roles which determine how they are used by Tempo.
  # https://grafana.com/docs/tempo/latest/configuration/#cache
  cache:
    caches:
      - memcached:
          host: '{{ include "tempo.fullname" . }}-memcached'
          service: memcached-client
          consistent_hash: true
          timeout: 500ms
        roles:
          - parquet-footer
          - bloom
          - frontend-search

  # To configure a different storage backend instead of local storage:
  # storage:
  #   trace:
  #     backend: azure
  #     azure:
  #       container_name:
  #       storage_account_name:
  #       storage_account_key:
  storage:
    trace:
      # Settings for the block storage backend and buckets.
      block:
        # -- The supported block versions are specified here https://grafana.com/docs/tempo/latest/configuration/parquet/
        version: null
        # -- Lis with dedicated attribute columns (only for vParquet3 or later)
        dedicated_columns: []
      # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage
      backend: local
      # The worker pool is used primarily when finding traces by id, but is also used by other.
      pool:
        # -- Total number of workers pulling jobs from the queue
        max_workers: 400
        # -- Length of job queue. imporatant for querier as it queues a job for every block it has to search
        queue_depth: 20000
    # Settings for the Admin client storage backend and buckets. Only valid is enterprise.enabled is true.
    admin:
      # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/enterprise-traces/latest/configure/reference/#admin_client_config
      backend: filesystem

  # Global overrides
  global_overrides:
    per_tenant_override_config: /runtime-config/overrides.yaml

  # Per tenants overrides
  overrides: {}

  # memcached is for all of the Tempo pieces to coordinate with each other.
  # you can use your self memcacherd by set enable: false and host + service
  memcached:
    # -- Specified whether the memcached cachce should be enabled
    enabled: true
    image:
      # -- The Docker registry for the Memcached image. Overrides `global.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
      pullSecrets: []
      # -- Memcached Docker image repository
      repository: memcached
      # -- Memcached Docker image tag
      tag: 1.6.29-alpine
      # -- Memcached Docker image pull policy
      pullPolicy: IfNotPresent
    host: memcached
    # Number of replicas for memchached
    replicas: 1
    # -- Additional CLI args for memcached
    extraArgs: []
    # -- Environment variables to add to memcached pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to memcached pods
    extraEnvFrom: []
    # -- Labels for memcached pods
    podLabels: {}
    # -- Annotations for memcached pods
    podAnnotations: {}
    # -- Resource requests and limits for memcached
    resources: {}
    # -- topologySpread for memcached pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 6 }}
    # -- Affinity for memcached pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    service:
      # -- Annotations for memcached service
      annotations: {}

  memcachedExporter:
    # -- Specifies whether the Memcached Exporter should be enabled
    enabled: false
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    image:
      # -- The Docker registry for the Memcached Exporter image. Overrides `global.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
      pullSecrets: []
      # -- Memcached Exporter Docker image repository
      repository: prom/memcached-exporter
      # -- Memcached Exporter Docker image tag
      tag: v0.14.4
      # -- Memcached Exporter Docker image pull policy
      pullPolicy: IfNotPresent
      # -- Memcached Exporter resource requests and limits
    resources: {}

  metaMonitoring:
    # ServiceMonitor configuration
    serviceMonitor:
      # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
      enabled: false
      # -- Alternative namespace for ServiceMonitor resources
      namespace: null
      # -- Namespace selector for ServiceMonitor resources
      namespaceSelector: {}
      # -- ServiceMonitor annotations
      annotations: {}
      # -- Additional ServiceMonitor labels
      labels: {}
      # -- ServiceMonitor scrape interval
      interval: null
      # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)
      scrapeTimeout: null
      # -- ServiceMonitor relabel configs to apply to samples before scraping
      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      relabelings: []
      # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
      # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
      metricRelabelings: []
      # -- ServiceMonitor will use http by default, but you can pick https as well
      scheme: http
      # -- ServiceMonitor will use these tlsConfig settings to make the health check requests
      tlsConfig: null

    # metaMonitoringAgent configures the built in Grafana Agent that can scrape metrics and logs and send them to a local or remote destination
    grafanaAgent:
      # -- Controls whether to create PodLogs, MetricsInstance, LogsInstance, and GrafanaAgent CRs to scrape the
      # ServiceMonitors of the chart and ship metrics and logs to the remote endpoints below.
      # Note that you need to configure serviceMonitor in order to have some metrics available.
      enabled: false

      # -- Controls whether to install the Grafana Agent Operator and its CRDs.
      # Note that helm will not install CRDs if this flag is enabled during an upgrade.
      # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds
      installOperator: false

      logs:
        # -- Default destination for logs. The config here is translated to Promtail client
        # configuration to write logs to this Loki-compatible remote. Optional.
        remote:
          # -- Full URL for Loki push endpoint. Usually ends in /loki/api/v1/push
          url: ''

          auth:
            # -- Used to set X-Scope-OrgID header on requests. Usually not used in combination with username and password.
            tenantId: ''

            # -- Basic authentication username. Optional.
            username: ''

            # -- The value under key passwordSecretKey in this secret will be used as the basic authentication password. Required only if passwordSecretKey is set.
            passwordSecretName: ''
            # -- The value under this key in passwordSecretName will be used as the basic authentication password. Required only if passwordSecretName is set.
            passwordSecretKey: ''

        # -- Client configurations for the LogsInstance that will scrape Mimir pods. Follows the format of .remote.
        additionalClientConfigs: []

      metrics:
        # -- Default destination for metrics. The config here is translated to remote_write
        # configuration to push metrics to this Prometheus-compatible remote. Optional.
        # Note that you need to configure serviceMonitor in order to have some metrics available.
        remote:
          # -- Full URL for Prometheus remote-write. Usually ends in /push
          url: ''

          # -- Used to add HTTP headers to remote-write requests.
          headers: {}
          auth:
            # -- Basic authentication username. Optional.
            username: ''

            # -- The value under key passwordSecretKey in this secret will be used as the basic authentication password. Required only if passwordSecretKey is set.
            passwordSecretName: ''
            # -- The value under this key in passwordSecretName will be used as the basic authentication password. Required only if passwordSecretName is set.
            passwordSecretKey: ''

        # -- Additional remote-write for the MetricsInstance that will scrape Mimir pods. Follows the format of .remote.
        additionalRemoteWriteConfigs: []

        scrapeK8s:
          # -- When grafanaAgent.enabled and serviceMonitor.enabled, controls whether to create ServiceMonitors CRs
          # for cadvisor, kubelet, and kube-state-metrics. The scraped metrics are reduced to those pertaining to
          # Mimir pods only.
          enabled: true

          # -- Controls service discovery of kube-state-metrics.
          kubeStateMetrics:
            namespace: kube-system
            labelSelectors:
              app.kubernetes.io/name: kube-state-metrics

      # -- Sets the namespace of the resources. Leave empty or unset to use the same namespace as the Helm release.
      namespace: ''

      # -- Labels to add to all monitoring.grafana.com custom resources.
      # Does not affect the ServiceMonitors for kubernetes metrics; use serviceMonitor.labels for that.
      labels: {}

      # -- Annotations to add to all monitoring.grafana.com custom resources.
      # Does not affect the ServiceMonitors for kubernetes metrics; use serviceMonitor.annotations for that.
      annotations: {}

  # Rules for the Prometheus Operator
  prometheusRule:
    # -- If enabled, a PrometheusRule resource for Prometheus Operator is created
    enabled: false
    # -- Alternative namespace for the PrometheusRule resource
    namespace: null
    # -- PrometheusRule annotations
    annotations: {}
    # -- Additional PrometheusRule labels
    labels: {}
    # -- Contents of Prometheus rules file
    groups: []
    # - name: loki-rules
    #   rules:
    #     - record: job:loki_request_duration_seconds_bucket:sum_rate
    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)
    #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate
    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)
    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)

  minio:
    enabled: false
    mode: standalone
    rootUser: grafana-tempo
    rootPassword: supersecret
    buckets:
      # Default Tempo storage bucket.
      - name: tempo-traces
        policy: none
        purge: false
      # Bucket for traces storage if enterprise.enabled is true - requires license.
      - name: enterprise-traces
        policy: none
        purge: false
      # Admin client bucket if enterprise.enabled is true - requires license.
      - name: enterprise-traces-admin
        policy: none
        purge: false
    persistence:
      size: 5Gi
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
    # Changed the mc config path to '/tmp' from '/etc' as '/etc' is only writable by root and OpenShift will not permit this.
    configPathmc: '/tmp/minio/mc/'

  # Configuration for the gateway
  gateway:
    # -- Specifies whether the gateway should be enabled
    enabled: false
    # -- Number of replicas for the gateway
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    autoscaling:
      # -- Enable autoscaling for the gateway
      enabled: false
      # -- Minimum autoscaling replicas for the gateway
      minReplicas: 1
      # -- Maximum autoscaling replicas for the gateway
      maxReplicas: 3
      # -- Autoscaling behavior configuration for the gateway
      behavior: {}
      # -- Target CPU utilisation percentage for the gateway
      targetCPUUtilizationPercentage: 60
      # -- Target memory utilisation percentage for the gateway
      targetMemoryUtilizationPercentage:
    # -- Enable logging of 2xx and 3xx HTTP requests
    verboseLogging: true
    image:
      # -- The Docker registry for the gateway image. Overrides `global.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `global.image.pullSecrets`
      pullSecrets: []
      # -- The gateway image repository
      repository: nginxinc/nginx-unprivileged
      # -- The gateway image tag
      tag: 1.27-alpine
      # -- The gateway image pull policy
      pullPolicy: IfNotPresent
    # -- The name of the PriorityClass for gateway pods
    priorityClassName: null
    # -- Labels for gateway pods
    podLabels: {}
    # -- Annotations for gateway pods
    podAnnotations: {}
    # -- Additional CLI args for the gateway
    extraArgs: []
    # -- Environment variables to add to the gateway pods
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the gateway pods
    extraEnvFrom: []
    # -- Volumes to add to the gateway pods
    extraVolumes: []
    # -- Volume mounts to add to the gateway pods
    extraVolumeMounts: []
    # -- Resource requests and limits for the gateway
    resources: {}
    # -- Grace period to allow the gateway to shutdown before it is killed
    terminationGracePeriodSeconds: 30
    # -- topologySpread for gateway pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 6 }}
    # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Hard node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 10 }}
            topologyKey: kubernetes.io/hostname
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "gateway") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone
    # -- Pod Disruption Budget maxUnavailable
    maxUnavailable: 1
    # -- Node selector for gateway pods
    nodeSelector: {}
    # -- Tolerations for gateway pods
    tolerations: []
    # Gateway service configuration
    service:
      # -- Port of the gateway service
      port: 80
      # -- Type of the gateway service
      type: ClusterIP
      # -- ClusterIP of the gateway service
      clusterIP: null
      # -- Node port if service type is NodePort
      nodePort: null
      # -- Load balancer IPO address if service type is LoadBalancer
      loadBalancerIP: null
      # -- Annotations for the gateway service
      annotations: {}
      # -- Labels for gateway service
      labels: {}
      # -- Additional ports to be opneed on gateway service (e.g. for RPC connections)
      additionalPorts: []
    # Gateway ingress configuration
    ingress:
      # -- Specifies whether an ingress for the gateway should be created
      enabled: false
      # -- Labels for the gateway ingress
      labels: {}
      # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
      # ingressClassName: nginx
      # -- Annotations for the gateway ingress
      annotations: {}
      # -- Hosts configuration for the gateway ingress
      hosts:
        - host: gateway.tempo.example.com
          paths:
            - path: /
              # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
              # pathType: Prefix
      # -- TLS configuration for the gateway ingress
      tls:
        - secretName: tempo-gateway-tls
          hosts:
            - gateway.tempo.example.com
    # Basic auth configuration
    basicAuth:
      # -- Enables basic authentication for the gateway
      enabled: false
      # -- The basic auth username for the gateway
      username: null
      # -- The basic auth password for the gateway
      password: null
      # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.
      # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes
      # high CPU load.
      htpasswd: >-
        {{ htpasswd (required "'gateway.basicAuth.username' is required" .Values.gateway.basicAuth.username) (required "'gateway.basicAuth.password' is required" .Values.gateway.basicAuth.password) }}
      # -- Existing basic auth secret to use. Must contain '.htpasswd'
      existingSecret: null
    # Configures the readiness probe for the gateway
    readinessProbe:
      httpGet:
        path: /
        port: http-metrics
      initialDelaySeconds: 15
      timeoutSeconds: 1
    nginxConfig:
      # -- NGINX log format
      logFormat: |-
        main '$remote_addr - $remote_user [$time_local]  $status '
                '"$request" $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';
      # -- Allows appending custom configuration to the server block
      serverSnippet: ''
      # -- Allows appending custom configuration to the http block
      httpSnippet: ''
      # -- Allows overriding the DNS resolver address nginx will use
      resolver: ''
      # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating
      # @default -- See values.yaml
      file: |
        worker_processes  5;  ## Default: 1
        error_log  /dev/stderr;
        pid        /tmp/nginx.pid;
        worker_rlimit_nofile 8192;

        events {
          worker_connections  4096;  ## Default: 1024
        }

        http {
          client_body_temp_path /tmp/client_temp;
          proxy_temp_path       /tmp/proxy_temp_path;
          fastcgi_temp_path     /tmp/fastcgi_temp;
          uwsgi_temp_path       /tmp/uwsgi_temp;
          scgi_temp_path        /tmp/scgi_temp;

          proxy_http_version    1.1;

          default_type application/octet-stream;
          log_format   {{ .Values.gateway.nginxConfig.logFormat }}

          {{- if .Values.gateway.verboseLogging }}
          access_log   /dev/stderr  main;
          {{- else }}

          map $status $loggable {
            ~^[23]  0;
            default 1;
          }
          access_log   /dev/stderr  main  if=$loggable;
          {{- end }}

          sendfile     on;
          tcp_nopush   on;
          {{- if .Values.gateway.nginxConfig.resolver }}
          resolver {{ .Values.gateway.nginxConfig.resolver }};
          {{- else }}
          resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
          {{- end }}

          {{- with .Values.gateway.nginxConfig.httpSnippet }}
          {{ . | nindent 2 }}
          {{- end }}

          server {
            listen             8080;

            {{- if .Values.gateway.basicAuth.enabled }}
            auth_basic           "Tempo";
            auth_basic_user_file /etc/nginx/secrets/.htpasswd;
            {{- end }}

            location = / {
              return 200 'OK';
              auth_basic off;
            }

            location = /jaeger/api/traces {
              set $distributor {{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$distributor:14268/api/traces;
            }

            location = /zipkin/spans {
              set $distributor {{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$distributor:9411/spans;
            }

            location = /v1/traces {
              set $distributor {{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$distributor:4318/v1/traces;
            }

            location = /otlp/v1/traces {
              set $distributor {{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$distributor:4318/v1/traces;
            }

            location ^~ /api {
              set $query_frontend {{ include "tempo.resourceName" (dict "ctx" . "component" "query-frontend") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$query_frontend:3100$request_uri;
            }

            location = /flush {
              set $ingester {{ include "tempo.resourceName" (dict "ctx" . "component" "ingester") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$ingester:3100$request_uri;
            }

            location = /shutdown {
              set $ingester {{ include "tempo.resourceName" (dict "ctx" . "component" "ingester") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$ingester:3100$request_uri;
            }

            location = /distributor/ring {
              set $distributor {{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$distributor:3100$request_uri;
            }

            location = /ingester/ring {
              set $distributor {{ include "tempo.resourceName" (dict "ctx" . "component" "distributor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$distributor:3100$request_uri;
            }

            location = /compactor/ring {
              set $compactor {{ include "tempo.resourceName" (dict "ctx" . "component" "compactor") }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
              proxy_pass       http://$compactor:3100$request_uri;
            }

            {{- with .Values.gateway.nginxConfig.serverSnippet }}
            {{ . | nindent 4 }}
            {{- end }}
          }
        }

  ##############################################################################
  # The values in and after the `enterprise:` key configure the enterprise features
  enterprise:
    # Enable enterprise features. License must be provided, nginx gateway is not installed, instead
    # the enterprise gateway is used.
    enabled: false

    image:
      # -- Grafana Enterprise Metrics container image repository. Note: for Grafana Tempo use the value 'image.repository'
      repository: grafana/enterprise-traces
      # -- Grafana Enterprise Metrics container image tag. Note: for Grafana Tempo use the value 'image.tag'
      tag: v2.4.0
      # Note: pullPolicy and optional pullSecrets are set in toplevel 'image' section, not here

  # In order to use Grafana Enterprise Traces features, you will need to provide the contents of your Grafana Enterprise Traces
  # license, either by providing the contents of the license.jwt, or the name Kubernetes Secret that contains your license.jwt.
  # To set the license contents, use the flag `--set-file 'license.contents=./license.jwt'`
  # To use your own Kubernetes Secret, `--set license.external=true`.
  license:
    contents: 'NOTAVALIDLICENSE'
    external: false
    secretName: '{{ include "tempo.resourceName" (dict "ctx" . "component" "license") }}'

  # Settings for the initial admin(istrator) token generator job. Can only be enabled if
  # enterprise.enabled is true - requires license.
  tokengenJob:
    enable: true
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld
    extraArgs: {}
    env: []
    extraEnvFrom: []
    annotations: {}
    image:
      # -- The Docker registry for the tokengenJob image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the tokengenJob image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the tokengenJob image. Overrides `tempo.image.tag`
      tag: null
    initContainers: []
    # -- The SecurityContext for tokenjobgen containers
    containerSecurityContext:
      readOnlyRootFilesystem: true

  # Settings for the admin_api service providing authentication and authorization service.
  # Can only be enabled if enterprise.enabled is true - requires license.
  adminApi:
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld

    annotations: {}
    service:
      annotations: {}
      labels: {}

    image:
      # -- The Docker registry for the adminApi image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the adminApi image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the adminApi image. Overrides `tempo.image.tag`
      tag: null

    initContainers: []

    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1

    podLabels: {}
    podAnnotations: {}

    nodeSelector: {}
    # -- topologySpread for admin-api pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "admin-api") | nindent 6 }}
    # -- Affinity for admin-api pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Soft node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "admin-api") | nindent 12 }}
              topologyKey: kubernetes.io/hostname
          - weight: 75
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "admin-api") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone

    # Pod Disruption Budget
    podDisruptionBudget: {}

    securityContext: {}

    # -- The SecurityContext for admin_api containers
    containerSecurityContext:
      readOnlyRootFilesystem: true

    extraArgs: {}

    persistence:
      subPath:

    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45

    resources:
      requests:
        cpu: 10m
        memory: 32Mi

    terminationGracePeriodSeconds: 60

    tolerations: []
    extraContainers: []
    extraVolumes: []
    extraVolumeMounts: []
    env: []
    extraEnvFrom: []

  # Settings for the gateway service providing authentication and authorization via the admin_api.
  # Can only be enabled if enterprise.enabled is true - requires license.
  enterpriseGateway:
    # -- If you want to use your own proxy URLs, set this to false.
    useDefaultProxyURLs: true
    # -- Proxy URLs defined in this object will be used if useDefaultProxyURLs is set to false.
    proxy: {}
    replicas: 1
    # -- hostAliases to add
    hostAliases: []
    #  - ip: 1.2.3.4
    #    hostnames:
    #      - domain.tld

    image:
      # -- The Docker registry for the enterpriseGateway image. Overrides `tempo.image.registry`
      registry: null
      # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
      pullSecrets: []
      # -- Docker image repository for the enterpriseGateway image. Overrides `tempo.image.repository`
      repository: null
      # -- Docker image tag for the enterpriseGateway image. Overrides `tempo.image.tag`
      tag: null

    annotations: {}
    service:
      # -- Port of the enterprise gateway service; if left undefined, the service will listen on the same port as the pod
      port: null
      # -- Type of the enterprise gateway service
      type: ClusterIP
      # -- ClusterIP of the enterprise gateway service
      clusterIP: null
      # -- Load balancer IPO address if service type is LoadBalancer for enterprise gateway service
      loadBalancerIP: null
      # -- Annotations for the enterprise gateway service
      annotations: {}
      # -- Labels for enterprise gateway service
      labels: {}

    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 0
        maxUnavailable: 1

    podLabels: {}
    podAnnotations: {}

    # Pod Disruption Budget
    podDisruptionBudget: {}

    nodeSelector: {}
    # -- topologySpread for enterprise-gateway pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Defaults to allow skew no more then 1 node per AZ
    topologySpreadConstraints: |
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            {{- include "tempo.selectorLabels" (dict "ctx" . "component" "enterprise-gateway") | nindent 6 }}
    # -- Affinity for enterprise-gateway pods. Passed through `tpl` and, thus, to be configured as string
    # @default -- Soft node and soft zone anti-affinity
    affinity: |
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "enterprise-gateway") | nindent 12 }}
              topologyKey: kubernetes.io/hostname
          - weight: 75
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "enterprise-gateway") | nindent 12 }}
              topologyKey: topology.kubernetes.io/zone

    securityContext:
      {}

      # -- The SecurityContext for gateway containers
    containerSecurityContext:
      readOnlyRootFilesystem: true

    initContainers: []

    extraArgs: {}

    persistence:
      subPath:

    readinessProbe:
      httpGet:
        path: /ready
        port: http-metrics
      initialDelaySeconds: 45

    resources:
      requests:
        cpu: 10m
        memory: 32Mi

    terminationGracePeriodSeconds: 60

    tolerations: []
    extraContainers: []
    extraVolumes: []
    extraVolumeMounts: []
    env: []
    extraEnvFrom: []

    # Ingress configuration
    ingress:
      # -- Specifies whether an ingress for the gateway should be created
      enabled: false
      # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
      # ingressClassName: gateway
      # -- Annotations for the gateway ingress
      annotations: {}
      # -- Hosts configuration for the gateway ingress
      hosts:
        - host: gateway.gem.example.com
          paths:
            - path: /
              # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
              # pathType: Prefix
      # -- TLS configuration for the gateway ingress
      tls:
        - secretName: gem-gateway-tls
          hosts:
            - gateway.gem.example.com

  # -- Create extra manifests via values.
  extraObjects: []
    # - apiVersion: "kubernetes-client.io/v1"
    #   kind: ExternalSecret
    #   metadata:
    #     name: tempo-secrets
    #   spec:
    #     backendType: aws
    #     data:
    #       - key: secret-access-key
    #         name: awssm-secret



# Sloth
sloth:
  enabled: true
  labels: {}

  image:
    repository: ghcr.io/slok/sloth
    tag: v0.11.0

  # -- Container resources: requests and limits for CPU, Memory
  resources:
    limits:
      cpu: 50m
      memory: 150Mi
    requests:
      cpu: 5m
      memory: 75Mi

  imagePullSecrets: []
  #  - name: secret1
  #  - name: secret2

  sloth:
    resyncInterval: ""    # The controller resync interval duration (e.g 15m).
    workers: 0            # The number of concurrent controller workers (e.g 5).
    labelSelector: ""     # Sloth will handle only the ones that match the selector.
    namespace: ""         # The namespace where sloth will the CRs to process.
    extraLabels: {}       # Labels that will be added to all the generated SLO Rules.
    defaultSloPeriod: ""  # The slo period used by sloth (e.g. 30d).
    optimizedRules: true  # Reduce prom load for calculating period window burnrates.
    debug:
      enabled: false
    # Could be: default or json
    logger: default

  commonPlugins:
    enabled: true
    image:
      repository: k8s.gcr.io/git-sync/git-sync
      tag: v3.6.9
    gitRepo:
      url: https://github.com/slok/sloth-common-sli-plugins
      branch: main
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 5m
          memory: 50Mi

  metrics:
    enabled: true
    #scrapeInterval: 30s
    prometheusLabels: {}

  customSloConfig:
    enabled: false
    path: /windows
    data: {}
  #    apiVersion: sloth.slok.dev/v1
  #    kind: AlertWindows
  #    spec:
  #    ... See https://sloth.dev/usage/slo-period-windows/

  # add deployment pod tolerations
  # tolerations:
  #   - key: kubernetes.azure.com/scalesetpriority
  #     operator: Equal
  #     value: spot
  #     effect: NoSchedule

  securityContext:
    pod: null
    #   fsGroup: 100
    #   runAsGroup: 1000
    #   runAsNonRoot: true
    #   runAsUser: 100
    container: null
    #   allowPrivilegeEscalation: false

#Kiali-server
kiali-server:
  enabled: true
  # 'fullnameOverride' is deprecated. Use 'deployment.instance_name' instead.
  # This is only supported for backward compatibility and will be removed in a future version.
  # If 'fullnameOverride' is not "kiali" and 'deployment.instance_name' is "kiali",
  # then 'deployment.instance_name' will take the value of 'fullnameOverride' value.
  # Otherwise, 'fullnameOverride' is ignored and 'deployment.instance_name' is used.
  fullnameOverride: "kiali"

  # This is required for "openshift" auth strategy.
  # You have to know ahead of time what your Route URL will be because
  # right now the helm chart can't figure this out at runtime (it would
  # need to wait for the Kiali Route to be deployed and for OpenShift
  # to start it up). If someone knows how to update this helm chart to
  # do this, a PR would be welcome.
  kiali_route_url: ""

  #
  # Settings that mimic the Kiali CR which are placed in the ConfigMap.
  # Note that only those values used by the Helm Chart will be here.
  #

  additional_display_details:
  - annotation: kiali.io/api-spec
    icon_annotation: kiali.io/api-type
    title: API Documentation

  istio_namespace: "" # default is where Kiali is installed

  auth:
    openid: {}
    openshift: {}
    strategy: ""
  
  oidc_secret: ""

  clustering:
    autodetect_secrets:
      enabled: true
      label: "kiali.io/multiCluster=true"
    clusters: []

  deployment:
    additional_service_yaml: {}
    affinity:
      node: {}
      pod: {}
      pod_anti: {}
    # The Kiali server helm chart only supports cluster-wide access; setting cluster_wide_access to false is not supported.
    # For more control over what the Kial Service Account can see, use the Kiali Operator.
    cluster_wide_access: true
    configmap_annotations: {}
    custom_envs: []
    custom_secrets: []
    dns:
      config: {}
      policy: ""
    host_aliases: []
    hpa:
      api_version: "autoscaling/v2"
      spec: {}
    image_digest: "" # use "sha256" if image_version is a sha256 hash (do NOT prefix this value with a "@")
    image_name: quay.io/kiali/kiali
    image_pull_policy: "Always"
    image_pull_secrets: []
    image_version: ${HELM_IMAGE_TAG} # version like "v1.39" (see: https://quay.io/repository/kiali/kiali?tab=tags) or a digest hash
    ingress:
      additional_labels: {}
      class_name: "nginx"
      #enabled:
      override_yaml:
        metadata: {}
    instance_name: "kiali"
    logger:
      log_format: "text"
      log_level: "info"
      time_field_format: "2006-01-02T15:04:05Z07:00"
      sampler_rate: "1"
    node_selector: {}
    pod_annotations: {}
    pod_labels: {}
    priority_class_name: ""
    remote_cluster_resources_only: false
    # if deployment.hpa is defined, this replicas setting will be ignored
    replicas: 1
    resources:
      requests:
        cpu: "10m"
        memory: "64Mi"
      limits:
        memory: "1Gi"
    secret_name: "kiali"
    security_context: {}
    service_annotations: {}
    service_type: ""
    tolerations: []
    version_label: ${HELM_IMAGE_TAG} # v1.39 # v1.39.0 # see: https://quay.io/repository/kiali/kiali?tab=tags
    view_only_mode: false

  external_services:
    custom_dashboards:
      enabled: true
    istio:
      root_namespace: ""

  identity: {}
    #cert_file:
    #private_key_file:

  kiali_feature_flags:
    disabled_features: []
    validations:
      ignore: ["KIA1301"]

  login_token:
    signing_key: ""

  server:
    port: 20001
    #node_port:
    observability:
      metrics:
        enabled: true
        port: 9090
    web_root: ""

### Prom2teams
### This Helm chart is taken from here https://github.com/idealista/prom2teams, please check how to configure and use prom2teams.
prom2teams:
  enabled: false
  # Default values for prom2teams.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  image:
    repository: idealista/prom2teams
    tag:
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      memory: 105M

  service:
    type: ClusterIP
    port: 8089
    monitoring_port: 9090

  ## To use external secret enable the external secrets config below
  ##in prom2teams config just set the secret name from azure key vault which contains the MS teams webhook url

  externalSecrets: 
    enabled: false
    secretStoreRef:
        kind: ClusterSecretStore
        name: azure-backend
    externalSecretsRefreshInterval: 1h

  prom2teams:
    host: 0.0.0.0
    port: 8089
    monitoring_port: 9090
    connector: http://dummy-teams-url
    connectors: {}
    # group_alerts_by can be one of
    # ("name" | "description" | "instance" | "severity" | "status" | "summary" | "fingerprint" | "runbook_url")
    group_alerts_by:
    # loglevel can be one of (DEBUG | INFO | WARNING | ERROR | CRITICAL)
    loglevel: INFO
    templatepath: /opt/prom2teams/helmconfig/teams.j2
    config: /opt/prom2teams/helmconfig/config.ini
    extraEnv: {}

  # Security Context properties
  securityContext:
    # enabled is a flag to enable Security Context
    enabled: true
    # runAsUser is the user ID used to run the container
    runAsUser: 101
    # runAsGroup is the primary group ID used to run all processes within any container of the pod
    runAsGroup: 101
    # fsGroup is the group ID associated with the container
    fsGroup: 101
    # readOnlyRootFilesystem is a flag to enable readOnlyRootFilesystem for the Hazelcast security context
    readOnlyRootFilesystem: true

### Kuberhealthy
kuberhealthy:
  enabled: false
  # Default values for kuberhealthy.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  # can be used to globally override the namespace where to deploy all components. By default .Release.namespace is used.
  #namespace: ""

  checkReaper:
    logLevel: error
    maxKHJobAge: 15m # Maximum age of the khjob resource before being reaped. Valid time units: "ns", "us" (or "µs"), "ms", "s", "m", "h"
    maxCheckPodAge: 72h # Maximum age of khcheck/khjob pods before being reaped. Valid time units: "ns", "us" (or "µs"), "ms", "s", "m", "h"
    maxCompletedPodCount: 4 # Maximum number of khcheck/khjob pods in Completed state before being reaped. If not set or set to 0, no completed khjob/khcheck pod will remain.
    maxErrorPodCount: 4 # Maximum number of khcheck/khjob pods in Error state before being reaped. If not set or set to 0, no completed khjob/khcheck pod will remain.

  stateMetadata: {}

  prometheus:
    enabled: false
    name: "prometheus"

    grafanaDashboard:
      enabled: false
      label: grafana_dashboard
      labelValue: "1"
      additionalLabels: {}
        # env: "dev"
        # mycustomlabel: "value"
      annotations: {}
        # k8s-sidecar-target-directory: "kuberhealthy"
      # namespace: "" # Alternative namespace to use for grafana dashboard.

    serviceMonitor:
      enabled: false
      release: prometheus-operator
      additionalLabels: {}
        # env: "dev"
        # mycustomlabel: "value"
      endpoints:
        interval: 15s
        bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
        # -- ServiceMonitor relabel configs to apply to samples before scraping
        ## Ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
        relabelings: []
        # -- ServiceMonitor metric relabel configs to apply to samples before ingestion
        ## Ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
        metricRelabelings: []
      # -- ServiceMonitor will add labels from the service to the Prometheus metric
      ## Ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitorspec
      targetLabels: []
      namespace: kuberhealthy # prometheus operator must be allowed to scrape serviceMonitor metrics from kuberhealthy namespace or set to namespace of prometheus operator
      rules: # applies to all PrometheusRule specs
        additionalAnnotations: {}
        additionalLabels:
          severity: critical

    prometheusRule:
      enabled: false
      release: prometheus-operator
      additionalLabels: {}
        # env: "dev"
        # mycustomlabel: "value"
      namespace: kuberhealthy

  # imageRegistry can be used to globally override where check images are pulled from. Individual checks can be overridden below.
  # By default if no overrides are specified, all images are pulled from Docker Hub.  Do not include a trailing '/'.
  imageRegistry: ""

  # Global imagePullSecrets setting for all images.
  # imagePullSecrets:
  # - name: my-secret-name

  # imageURL allows you to override all other nonsense and just specify the full image URL you want to pull
  #imageURL: docker.io/kuberhealthy/kuberhealthy:v1.6.0

  image:
    registry: docker.io
    repository: kuberhealthy/kuberhealthy
    # Leave empty to use .Chart.AppVersion
    tag:

  resources:
    requests:
      cpu: 400m
      memory: 300Mi
    limits:
      cpu: 2
      memory: 1Gi

  ## Only minAvailable or maxUnavailable can be set at the same time.
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
    maxUnavailable:


  tolerations:
    # change to true to tolerate and deploy to masters
    master: false

  deployment:
    replicas: 2
    maxSurge: 0
    maxUnavailable: 1
    imagePullPolicy: IfNotPresent
    nodeSelector: {}
    podAnnotations: {}
    # tolerations:
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"
    env:
      CHECK_REAPER_RUN_INTERVAL: "30s" # Interval for how often check pods should get reaped. Default is 30s
      TARGET_NAMESPACE: "" # leave blank to target all namespaces, or supply a namespace to operate on only a single namespace
    command:
    - /app/kuberhealthy
    # args:
    # priorityClassName:
    affinity: {}
    ## Acceptable values for podAntiAffinity:
    ## soft: specifies preferences that the scheduler will try to enforce but will not guarantee (Default)
    ## hard: specifies rules that must be met for a pod to be scheduled onto a node
    podAntiAffinity: "soft"

  # When enabled equals to true, runAsUser and fsGroup will be
  # included to all khchecks as specified below.
  securityContext:
    enabled: true # if enabled is set to false, securityContext settings will not be applied at all in checker pod custom resources
    runAsNonRoot: true
    runAsUser: 999
    fsGroup: 999
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    #seccompProfile:
    #  type: RuntimeDefault

  # When enabled kuberhealthy will create a PodSecurityPolicy, Role and Binding to match the specified securityContext
  # In case you use your own images for tests, you might need to add other PSPs as well on your own
  podSecurityPolicy:
    enabled: false

  # Please remember that changing the service type to LoadBalancer
  # will expose Kuberhealthy to the internet, which could cause
  # error messages shown by Kuberhealthy to be exposed to the
  # public internet.  It is recommended to create the service
  # with ClusterIP, then to manually edit the service in order to
  # securely expose the port in an appropriate way for your
  # specific environment.
  service:
    externalPort: 80
    type: ClusterIP
    annotations: {}

  check:
    daemonset:
      # leave blank for the default service account name
      serviceAccountName:
      enabled: true
      runInterval: 15m
      timeout: 12m
      image:
        registry: docker.io
        repository: kuberhealthy/daemonset-check
        tag: v3.3.0
      extraEnvs: {}
      nodeSelector: {}
      tolerations:
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
    deployment:
      enabled: true
      runInterval: 10m
      timeout: 15m
      image:
        registry: docker.io
        repository: kuberhealthy/deployment-check
        tag: v1.9.0
      extraEnvs:
        CHECK_DEPLOYMENT_REPLICAS: "4"
        CHECK_DEPLOYMENT_ROLLING_UPDATE: "true"
        CHECK_SERVICE_ACCOUNT: "default"
      nodeSelector: {}
      tolerations:
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 25m
          memory: 15Mi
        limits:
          cpu: 40m
    dnsInternal:
      enabled: true
      runInterval: 2m
      timeout: 15m
      image:
        registry: docker.io
        repository: kuberhealthy/dns-resolution-check
        tag: v1.5.0
      extraEnvs:
        HOSTNAME: "kubernetes.default"
      nodeSelector: {}
      tolerations:
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
    dnsExternal:
      enabled: false
      runInterval: 2m
      timeout: 15m
      image:
        registry: docker.io
        repository: kuberhealthy/dns-resolution-check
        tag: v1.5.0
      extraEnvs:
        HOSTNAME: "google.com"
      nodeSelector: {}
      tolerations:
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
    podRestarts:
      enabled: false
      runInterval: 5m
      timeout: 10m
      image:
        registry: docker.io
        repository: kuberhealthy/pod-restarts-check
        tag: v2.5.0
      allNamespaces: false
      extraEnvs:
        MAX_FAILURES_ALLOWED: "10"
      nodeSelector: {}
      tolerations: []
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
    podStatus:
      enabled: false
      runInterval: 5m
      timeout: 15m
      image:
        registry: docker.io
        repository: kuberhealthy/pod-status-check
        tag: v1.3.0
      allNamespaces: false
      extraEnvs: {}
      nodeSelector: {}
      tolerations: []
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
    storage:
    # external check from registry
    # https://github.com/ChrisHirsch/kuberhealthy-storage-check
      enabled: false
      # empty string indicate default storage class
      # kubectl get storageclass
      # or put storage class names into list
      storageClass: [""]
      runInterval: 5m
      timeout: 10m
      image:
        registry: docker.io
        repository: chrishirsch/kuberhealthy-storage-check
        tag: v0.0.1
      extraEnvs:
        CHECK_STORAGE_IMAGE: bitnami/nginx:1.19
        CHECK_STORAGE_INIT_IMAGE: bitnami/nginx:1.19
      nodeSelector: {}
      tolerations: []
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
    networkConnection:
      enabled: false
      runInterval: 30m
      timeout: 10m
      image:
        registry: docker.io
        repository: kuberhealthy/network-connection-check
        tag: v0.2.0
      extraEnvs:
        CONNECTION_TARGET: "tcp://github.com:443"
      nodeSelector: {}
      tolerations: []
      #- key: "key"
      #  operator: "Equal"
      #  value: "value"
      #  effect: "NoSchedule"
      resources:
        requests:
          memory: 5Mi
          cpu: 10m

### Nopo11y-health-check
nopo11y_health_check:
  checks: []
  # - name: test
  #   image: ghcr.io/znsio/nopo11y/system_health_check:1.0
  #   imagePullPolicy: "IfNotPresent"
  #   runInterval: 1m
  #   timeout: 5m
  #   resources:
  #     requests:
  #     cpu: 10m
  #     memory: 50Mi
  #     limits:
  #     cpu: 20m
  #     memory: 100Mi
  #   customLabels: {}
  #      app: test
  #   ## You can provide the below environment variables to the health check
  #   ## NAMESPACE - default is default namespace
  #   ## HEALTHY_PODS_PERCENTAGE - default is 30%
  #   ## HEALTHY_POD_CPU_UTILIZATION_THRESHOLD - default is 80%
  #   ## HEALTHY_POD_MEMORY_UTILIZATION_THRESHOLD - default is 80%
  #   ## HEALTHY_PVC_FREE_SPACE - default is 200mb
  #   ## HEALTHY_NODE_CPU_UTILIZATION_THRESHOLD - default is 90%
  #   ## HEALTHY_NODE_CPU_AVAILABLE - default is 400m
  #   ## HEALTHY_NODE_MEMORY_UTILIZATION_THRESHLD - default is 90%
  #   ## HEALTHY_NODE_MEMORY_AVAILABLE - default is 1000mb
  #   ## HEALTHY_NODE_ROOT_AVAILABEL_SPACE - default is 200mb
  #   ## SLOTH_SLO_CRITICAL_ALERT_CHECK - default is true
  #   env: {}

# Kubernetes event exporter.
kubernetes-event-exporter:
  enabled: true
  # Copyright Broadcom, Inc. All Rights Reserved.
  # SPDX-License-Identifier: APACHE-2.0

  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ##
  global:
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []
    ## Compatibility adaptations for Kubernetes platforms
    ##
    compatibility:
      ## Compatibility adaptations for Openshift
      ##
      openshift:
        ## @param global.compatibility.openshift.adaptSecurityContext Adapt the securityContext sections of the deployment to make them compatible with Openshift restricted-v2 SCC: remove runAsUser, runAsGroup and fsGroup and let the platform use their allowed default IDs. Possible values: auto (apply if the detected running cluster is Openshift), force (perform the adaptation always), disabled (do not perform adaptation)
        ##
        adaptSecurityContext: auto
  ## @section Common parameters

  ## @param kubeVersion Override Kubernetes version
  ##
  kubeVersion: ""
  ## @param nameOverride String to partially override kubernetes-event-exporter.fullname include (will maintain the release name)
  ##
  nameOverride: ""
  ## @param fullnameOverride String to fully override kubernetes-event-exporter.fullname template
  ##
  fullnameOverride: ""
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations: {}
  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## Enable diagnostic mode in the deployment
  ##
  diagnosticMode:
    ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
    ##
    enabled: false
    ## @param diagnosticMode.command Command to override all containers in the deployment
    ##
    command:
      - sleep
    ## @param diagnosticMode.args Args to override all containers in the deployment
    ##
    args:
      - infinity
  ## @param extraDeploy Array of extra objects to deploy with the release (evaluated as a template)
  ##
  extraDeploy: []
  ## @section Kubernetes Event Exporter parameters

  ## @param replicaCount Desired number of pod replicas
  replicaCount: 1
  ##
  ## @param revisionHistoryLimit Desired number of old ReplicaSets to retain
  ## Defaults to 10, if set to 0 old ReplicaSets will be garbage-collected
  revisionHistoryLimit: 10
  ##
  ## @param containerPorts.http HTTP container port
  ##
  containerPorts:
    http: 2112
  ## @param extraContainerPorts Optionally specify extra list of additional port-mappings for the container
  ##
  extraContainerPorts: []
  image:
    ## @param image.registry [default: REGISTRY_NAME] Container image registry
    ## @param image.repository [default: REPOSITORY_NAME/kubernetes-event-exporter] Container image name
    ## @skip image.tag Container image tag
    ## @param image.digest Container image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
    ## @param image.pullPolicy Container image pull policy
    ## @param image.pullSecrets Specify docker-registry secret names as an array
    ##
    registry: docker.io
    repository: bitnami/kubernetes-event-exporter
    tag: 1.7.0-debian-12-r15
    digest: ""
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images
    ##
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## Example:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## @param automountServiceAccountToken Mount Service Account token in pod
  ##
  automountServiceAccountToken: true
  ## @param hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## Kubernetes event exporter configuration, rendered as a template
  ## ref: https://github.com/resmoio/kubernetes-event-exporter#configuration
  ## @param config.logLevel Verbosity of the logs (options: `fatal`, `error`, `warn`, `info` or `debug`)
  ## @param config.logFormat How the logs are formatted. Allowed values: `pretty` or `json`
  ## @param config.receivers [array] Array containing event receivers
  ## @param config.route.routes [array] Array containing event route configuration
  ##
  config:
    logLevel: debug
    logFormat: pretty
    receivers:
      - name: "dump"
        file:
          path: "/dev/stdout"
          ## Example:
          ## layout:
          ##   message: "{{ .Message }}"
          ##   reason: "{{ .Reason }}"
          ##   type: "{{ .Type }}"
          ##   count: "{{ .Count }}"
          ##   kind: "{{ .InvolvedObject.Kind }}"
          ##   name: "{{ .InvolvedObject.Name }}"
          ##   namespace: "{{ .Namespace }}"
          ##   component: "{{ .Source.Component }}"
          ##   host: "{{ .Source.Host }}"
          ##
          layout: {}
    route:
      routes:
        - match:
            - receiver: "dump"
  rbac:
    ## @param rbac.create Create the RBAC roles for API accessibility
    ##
    create: true
    ## @param rbac.rules [array] List of rules for the cluster role
    ##
    rules:
      - apiGroups: ["*"]
        resources: ["*"]
        verbs: ["get", "watch", "list"]
  ## Pods Service Account
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ## @param serviceAccount.create Specifies whether a ServiceAccount should be created
  ## @param serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
  ## @param serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ## @param serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}
  ## @param podAnnotations Pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param podLabels Pod labels
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## SecurityContext holds pod-level security attributes and common container settings.
  ## @param podSecurityContext.enabled Enable security context
  ## @param podSecurityContext.fsGroupChangePolicy Set filesystem group change policy
  ## @param podSecurityContext.sysctls Set kernel settings using the sysctl interface
  ## @param podSecurityContext.supplementalGroups Set filesystem extra groups
  ## @param podSecurityContext.fsGroup Group ID for the container
  ##
  podSecurityContext:
    enabled: true
    fsGroupChangePolicy: Always
    sysctls: []
    supplementalGroups: []
    fsGroup: 1001
  ## Pod disruption budget
  ## @param pdb.create Create a pdb
  ## @param pdb.minAvailable Minimum number / percentage of pods that should remain scheduled
  ## @param pdb.maxUnavailable Maximum number / percentage of pods that may be made unavailable
  ##
  pdb:
    create: true
    minAvailable: ""
    maxUnavailable: ""
  ## @param containerSecurityContext.enabled Enabled containers' Security Context
  ## @param containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param containerSecurityContext.runAsUser Set containers' Security Context runAsUser
  ## @param containerSecurityContext.runAsGroup Set containers' Security Context runAsGroup
  ## @param containerSecurityContext.runAsNonRoot Set container's Security Context runAsNonRoot
  ## @param containerSecurityContext.privileged Set container's Security Context privileged
  ## @param containerSecurityContext.readOnlyRootFilesystem Set container's Security Context readOnlyRootFilesystem
  ## @param containerSecurityContext.allowPrivilegeEscalation Set container's Security Context allowPrivilegeEscalation
  ## @param containerSecurityContext.capabilities.drop List of capabilities to be dropped
  ## @param containerSecurityContext.seccompProfile.type Set container's Security Context seccomp profile
  ##
  containerSecurityContext:
    enabled: true
    seLinuxOptions: {}
    runAsUser: 1001
    runAsGroup: 1001
    runAsNonRoot: true
    privileged: false
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    seccompProfile:
      type: "RuntimeDefault"
  ## @param command Override default container command (useful when using custom images)
  ##
  command: []
  ## @param args Override default container args (useful when using custom images)
  ##
  args: []
  ## @param lifecycleHooks Lifecycle for the container to automate configuration before or after startup
  ##
  lifecycleHooks: {}
  ## Container resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "nano"
  ## @param resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Network Policies
  ## Ref: https://kubernetes.io/docs/concepts/services-networking/network-policies/
  ##
  networkPolicy:
    ## @param networkPolicy.enabled Specifies whether a NetworkPolicy should be created
    ##
    enabled: true
    ## @param networkPolicy.kubeAPIServerPorts [array] List of possible endpoints to kube-apiserver (limit to your cluster settings to increase security)
    ##
    kubeAPIServerPorts: [443, 6443, 8443]
    ## @param networkPolicy.allowExternal Don't require server label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## server label will have network access to the ports server is listening
    ## on. When true, server will accept connections from any source
    ## (with the correct destination port).
    ##
    allowExternal: true
    ## @param networkPolicy.allowExternalEgress Allow the pod to access any range of port and all destinations.
    ##
    allowExternalEgress: true
    ## @param networkPolicy.extraIngress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraIngress:
    ##   - ports:
    ##       - port: 1234
    ##     from:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    extraIngress: []
    ## @param networkPolicy.extraEgress [array] Add extra ingress rules to the NetworkPolicy
    ## e.g:
    ## extraEgress:
    ##   - ports:
    ##       - port: 1234
    ##     to:
    ##       - podSelector:
    ##           - matchLabels:
    ##               - role: frontend
    ##       - podSelector:
    ##           - matchExpressions:
    ##               - key: role
    ##                 operator: In
    ##                 values:
    ##                   - frontend
    ##
    extraEgress: []
    ## @param networkPolicy.ingressNSMatchLabels [object] Labels to match to allow traffic from other namespaces
    ## @param networkPolicy.ingressNSPodMatchLabels [object] Pod labels to match to allow traffic from other namespaces
    ##
    ingressNSMatchLabels: {}
    ingressNSPodMatchLabels: {}
  ## Configure extra options for Kubernetes event exporter container's liveness, readiness and startup probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
  ## @param livenessProbe.enabled Enable livenessProbe on Kubernetes event exporter container
  ## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 5
    successThreshold: 1
  ## @param readinessProbe.enabled Enable readinessProbe on Kubernetes event exporter container
  ## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 1
    successThreshold: 1
  ## @param startupProbe.enabled Enable startupProbe on Kubernetes event exporter container
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 40
    periodSeconds: 10
    timeoutSeconds: 15
    failureThreshold: 15
    successThreshold: 1
  ## Configure extra custom startup, liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param customStartupProbe Configure startup probe for Kubernetes event exporter pod
  ##
  customStartupProbe: {}
  ## @param customLivenessProbe Configure liveness probe for Kubernetes event exporter pod
  ##
  customLivenessProbe: {}
  ## @param customReadinessProbe Configure readiness probe for Kubernetes event exporter pod
  ##
  customReadinessProbe: {}
  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector: {}
  ## @param priorityClassName Set Priority Class Name to allow priority control over other pods
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  ##
  priorityClassName: ""
  ## @param schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param topologySpreadConstraints Topology Spread Constraints for pod assignment
  ## https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ## The value is evaluated as a template
  ##
  topologySpreadConstraints: []
  ## @param tolerations Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: soft
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param updateStrategy.type Deployment strategy type.
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  ## e.g:
  ## updateStrategy:
  ##  type: RollingUpdate
  ##  rollingUpdate:
  ##    maxSurge: 25%
  ##    maxUnavailable: 25%
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param extraEnvVars Array containing extra env vars to be added to all containers
  ## For example:
  ## extraEnvVars:
  ##  - name: MY_ENV_VAR
  ##    value: env_var_value
  ##
  extraEnvVars: []
  ## @param extraEnvVarsCM ConfigMap containing extra env vars to be added to all containers
  ##
  extraEnvVarsCM: ""
  ## @param extraEnvVarsSecret Secret containing extra env vars to be added to all containers
  ##
  extraEnvVarsSecret: ""
  ## @param extraVolumeMounts Array to add extra mounts (normally used with extraVolumes)
  ##
  extraVolumeMounts: []
  ## @param extraVolumes Array to add extra volumes
  ##
  extraVolumes: []
  ## @param initContainers Attach additional init containers to pods
  ## For example:
  ## initContainers:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##
  initContainers: []
  ## @param sidecars Add additional sidecar containers to pods
  ## e.g:
  ## sidecars:
  ##   - name: your-image-name
  ##     image: your-image
  ##     imagePullPolicy: Always
  ##     ports:
  ##       - name: portname
  ##         containerPort: 1234
  ##
  sidecars: []
  ## Metrics configuration
  ##
  metrics:
    ## @param metrics.enabled Enable exposing  statistics
    ## ref: https://github.com/resmoio/kubernetes-event-exporter/blob/858089f2dc42243c0939a7f13a76fdd22e70be0f/main.go#L25
    ##
    enabled: false
    ##  metrics service parameters
    ##
    service:
      ## @param metrics.service.ports.http Metrics service HTTP port
      ##
      ports:
        http: 2112
      ## @param metrics.service.annotations [object] Annotations for enabling prometheus to access the metrics endpoints
      ##
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "{{ .Values.metrics.service.ports.http }}"
    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.serviceMonitor.port Metrics service HTTP port
      ##
      port: http
      ## @param metrics.serviceMonitor.endpoints [array] The endpoint configuration of the ServiceMonitor. Path is mandatory. Interval, timeout and labellings can be overwritten.
      ##
      endpoints:
        - path: /metrics
      ## @param metrics.serviceMonitor.path Metrics service HTTP path. Deprecated: Use @param metrics.serviceMonitor.endpoints instead
      ##
      path: ""
      ## @param metrics.serviceMonitor.namespace Namespace which Prometheus is running in
      ##
      namespace: ""
      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
      ##
      interval: 30s
      ## @param metrics.serviceMonitor.scrapeTimeout Specify the timeout after which the scrape is ended
      ## e.g:
      ##   scrapeTimeout: 30s
      ##
      scrapeTimeout: ""
      ## @param metrics.serviceMonitor.labels Additional labels that can be used so ServiceMonitor will be discovered by Prometheus
      ##
      labels: {}
      ## @param metrics.serviceMonitor.selector Prometheus instance selector labels
      ## ref: https://github.com/bitnami/charts/tree/main/bitnami/prometheus-operator#prometheus-configuration
      ##
      selector: {}
      ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
      ##
      relabelings: []
      ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
      ##
      metricRelabelings: []
      ## @param metrics.serviceMonitor.honorLabels honorLabels chooses the metric's labels on collisions with target labels
      ##
      honorLabels: false
      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""
    ## Prometheus Operator alert rules configuration
    ##
    prometheusRule:
      ## @param metrics.prometheusRule.enabled Create PrometheusRule Resource for scraping metrics using PrometheusOperator
      ##
      enabled: false
      ## @param metrics.prometheusRule.namespace Namespace which Prometheus is running in
      ##
      namespace: ""
      ## @param metrics.prometheusRule.labels Additional labels that can be used so PrometheusRule will be discovered by Prometheus
      ##
      labels: {}
      ## @param metrics.prometheusRule.groups Groups, containing the alert rules.
      ## Example:
      ##   groups:
      ##     - name: KubernetesEventExporter
      ##       rules:
      ##         - alert: KubernetesEventExporterTooManyWatchErrors
      ##           annotations:
      ##             message: "Kubernetes Event Exporter instance in namespace {{ `{{` }} $labels.namespace {{ `}}` }} has reported too many watch errors in 5 minutes."
      ##           expr: |
      ##             sum(watch_errors{namespace="{{ include "common.names.namespace" . }}"})
      ##           for: 5m
      ##           labels:
      ##             severity: critical
      groups: []
  ## @section Autoscaling
  ##
  autoscaling:
    vpa:
      ## @param autoscaling.vpa.enabled Enable VPA
      ##
      enabled: false
      ## @param autoscaling.vpa.annotations Annotations for VPA resource
      ##
      annotations: {}
      ## @param autoscaling.vpa.recommenders Recommender responsible for generating recommendation for the object.
      ## List should be empty (then the default recommender will generate the recommendation) or contain exactly one recommender.
      ## For example:
      ## recommenders:
      ## - name: custom-recommender-performance
      recommenders: []
      ## @param autoscaling.vpa.controlledResources VPA List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
      ##
      controlledResources: []
      ## @param autoscaling.vpa.maxAllowed VPA Max allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      maxAllowed: {}
      ## @param autoscaling.vpa.minAllowed VPA Min allowed resources for the pod
      ## cpu: 200m
      ## memory: 100Mi
      minAllowed: {}
      ## @section VPA update policy
      ##
      updatePolicy:
        ## @param autoscaling.vpa.updatePolicy.minReplicas Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
        minReplicas: 1
        ## @param autoscaling.vpa.updatePolicy.updateMode Autoscaling update policy Specifies whether recommended updates are applied when a Pod is started and whether recommended updates are applied during the life of a Pod
        ## Possible values are "Off", "Initial", "Recreate", and "Auto".
        ##
        updateMode: Auto


# Nopo11y Operator
nopo11y-operator:
  enabled: true
  # Default values for nopo11y-operator.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  replicaCount: 1

  image:
    repository: ghcr.io/znsio/nopo11y/nopo11y-operator
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v1"

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""

  env: {}
    # - name: LOG_LEVEL
    #   value: "INFO"
    # - name: API_GATEWAY
    #   value: "istio"
    # - name: GRAFANA_EXTERNAL_URL
    #   value: ""
    # - name: AVAILABILITY_SLO
    #   value: "99"
    # - name: LATENCY_SLO
    #   value: "99"
    # - name: LATENCY_MS
    #   value: "3000"
    # - name: ERROR_RATE_4XX
    #   value: "5"
    # - name: ERROR_RATE_5XX
    #   value: "1"
    # - name: NOPO11Y_STACK_NAMESPACE
    #   value: "observability"

  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  podAnnotations: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    type: ClusterIP
    port: 80

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 256Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

## Ingress for nopo11y-stack components 
nopo11y_ingress:
  ## enable or disable ingress for nopo11y-stack components, the default it is disabled
  enabled: false
  ## Ingress type either istio or nginx, the default is istio
  type: "istio"
  ## DNS or host name to access nopo11y-stack components with.
  host: ""
  ## istio gateway selector to select which istio gateway to use.
  ## if you set ingress type to istio, check the lables of your istio ingress gateway pod and add it as istioGatewaySelector
  istioGatewaySelector: {}
    # app: gateway
  ## if you set ingress type to istio then istio namespace is required, the default is istio-system
  istioNamespace: "istio-system"
  ## TLS section for ingress
  tls:
    enabled: false
    tlsKey: ""
    tlsCert: ""
